# -*- coding: utf-8 -*-
"""
Chunks å“è³ªæª¢æŸ¥å·¥å…·

æª¢æŸ¥é …ç›®ï¼š
1. åŸºæœ¬çµ±è¨ˆï¼šç¸½æ•¸ã€å¹³å‡é•·åº¦ã€é•·åº¦åˆ†å¸ƒ
2. æ–‡å­—å“è³ªï¼šèªè¨€æ··é›œã€ç‰¹æ®Šå­—ç¬¦ã€æˆªæ–·å•é¡Œ
3. èªæ„å®Œæ•´æ€§ï¼šå¥å­å®Œæ•´åº¦ã€æ®µè½é€£è²«æ€§
4. å…§å®¹é‡è¤‡ï¼šç›¸ä¼¼åº¦æª¢æŸ¥ã€é‡è¤‡å…§å®¹åµæ¸¬
5. é ˜åŸŸç‰¹æ€§ï¼šæŠ€è¡“æ–‡ä»¶ç‰¹å¾µã€å°ˆæ¥­è¡“èªåˆ†æ

ä½¿ç”¨æ–¹å¼ï¼š
python -m scripts.check_chunks_quality --input indices/chunks.jsonl
"""

import json
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Any, Tuple
import argparse

# å¯é¸ä¾è³´ï¼šå¦‚æœæœ‰çš„è©±æœƒæä¾›æ›´è©³ç´°åˆ†æ
try:
    import jieba
    import jieba.analyse
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

try:
    from difflib import SequenceMatcher
    DIFFLIB_AVAILABLE = True
except ImportError:
    DIFFLIB_AVAILABLE = False


class ChunksQualityChecker:
    """Chunkså“è³ªæª¢æŸ¥å™¨"""
    
    def __init__(self, chunks_file: str):
        self.chunks_file = Path(chunks_file)
        self.chunks: List[Dict[str, Any]] = []
        self.stats = defaultdict(int)
        self.quality_issues = defaultdict(list)
        
        # è¼‰å…¥chunks
        self._load_chunks()
    
    def _load_chunks(self):
        """è¼‰å…¥chunksæª”æ¡ˆ"""
        print(f"ğŸ“ è¼‰å…¥æª”æ¡ˆ: {self.chunks_file}")
        
        if not self.chunks_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {self.chunks_file}")
        
        try:
            with open(self.chunks_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        chunk = json.loads(line)
                        if 'text' in chunk:
                            self.chunks.append(chunk)
                        else:
                            self.quality_issues['malformed_json'].append(f"Line {line_num}: ç¼ºå°‘textæ¬„ä½")
                    except json.JSONDecodeError as e:
                        self.quality_issues['json_errors'].append(f"Line {line_num}: {e}")
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•—: {e}")
            sys.exit(1)
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.chunks)} å€‹ chunks")
    
    def analyze_basic_stats(self) -> Dict[str, Any]:
        """åŸºæœ¬çµ±è¨ˆåˆ†æ"""
        print("\nğŸ“Š === åŸºæœ¬çµ±è¨ˆåˆ†æ ===")
        
        lengths = [len(chunk['text']) for chunk in self.chunks]
        sources = [chunk.get('source', 'unknown') for chunk in self.chunks]
        pages = [(chunk.get('source', 'unknown'), chunk.get('page', 0)) for chunk in self.chunks]
        
        stats = {
            'total_chunks': len(self.chunks),
            'unique_sources': len(set(sources)),
            'total_pages': len(set(pages)),
            'avg_length': sum(lengths) / len(lengths) if lengths else 0,
            'min_length': min(lengths) if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'length_std': self._calculate_std(lengths),
        }
        
        # é•·åº¦åˆ†å¸ƒ
        length_ranges = {
            'very_short': sum(1 for l in lengths if l < 100),
            'short': sum(1 for l in lengths if 100 <= l < 500),
            'medium': sum(1 for l in lengths if 500 <= l < 1000),
            'long': sum(1 for l in lengths if 1000 <= l < 2000),
            'very_long': sum(1 for l in lengths if l >= 2000),
        }
        
        print(f"ğŸ“ˆ ç¸½chunksæ•¸: {stats['total_chunks']}")
        print(f"ğŸ“š ä¾†æºæ–‡ä»¶: {stats['unique_sources']} å€‹")
        print(f"ğŸ“„ ç¸½é é¢æ•¸: {stats['total_pages']}")
        print(f"ğŸ“ å¹³å‡é•·åº¦: {stats['avg_length']:.1f} å­—å…ƒ")
        print(f"ğŸ“ é•·åº¦ç¯„åœ: {stats['min_length']} ~ {stats['max_length']} å­—å…ƒ")
        print(f"ğŸ“Š æ¨™æº–å·®: {stats['length_std']:.1f}")
        
        print("\nğŸ“ é•·åº¦åˆ†å¸ƒ:")
        total = stats['total_chunks']
        for range_name, count in length_ranges.items():
            percentage = (count / total * 100) if total > 0 else 0
            print(f"  {range_name:12}: {count:4d} ({percentage:5.1f}%)")
        
        return {**stats, 'length_distribution': length_ranges}
    
    def analyze_text_quality(self) -> Dict[str, Any]:
        """æ–‡å­—å“è³ªåˆ†æ"""
        print("\nğŸ” === æ–‡å­—å“è³ªåˆ†æ ===")
        
        language_stats = {'chinese': 0, 'english': 0, 'mixed': 0, 'other': 0}
        encoding_issues = []
        truncation_issues = []
        special_char_stats = Counter()
        
        for i, chunk in enumerate(self.chunks):
            text = chunk['text']
            
            # èªè¨€æª¢æ¸¬
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            english_chars = len(re.findall(r'[a-zA-Z]', text))
            total_chars = len(text)
            
            if chinese_chars > total_chars * 0.5:
                language_stats['chinese'] += 1
            elif english_chars > total_chars * 0.3:
                if chinese_chars > total_chars * 0.1:
                    language_stats['mixed'] += 1
                else:
                    language_stats['english'] += 1
            else:
                language_stats['other'] += 1
            
            # ç‰¹æ®Šå­—ç¬¦çµ±è¨ˆ
            special_chars = re.findall(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\-\.\,\!\?\;\:]', text)
            special_char_stats.update(special_chars)
            
            # ç·¨ç¢¼å•é¡Œæª¢æ¸¬
            if ' ' in text or text.count('\\') > text.count('\\n') + text.count('\\"'):
                encoding_issues.append(i)
            
            # æˆªæ–·å•é¡Œæª¢æ¸¬
            if text.endswith('...') or (not text.endswith(('ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ')) and len(text) > 100):
                truncation_issues.append(i)
        
        print(f"ğŸŒ èªè¨€åˆ†å¸ƒ:")
        total = len(self.chunks)
        for lang, count in language_stats.items():
            percentage = (count / total * 100) if total > 0 else 0
            print(f"  {lang:10}: {count:4d} ({percentage:5.1f}%)")
        
        print(f"\nâš ï¸  å“è³ªå•é¡Œ:")
        print(f"  ç·¨ç¢¼å•é¡Œ: {len(encoding_issues)} å€‹")
        print(f"  æˆªæ–·å•é¡Œ: {len(truncation_issues)} å€‹")
        
        if special_char_stats:
            print(f"\nğŸ”£ ç‰¹æ®Šå­—ç¬¦ TOP 5:")
            for char, count in special_char_stats.most_common(5):
                print(f"  '{char}': {count} æ¬¡")
        
        return {
            'language_distribution': language_stats,
            'encoding_issues': encoding_issues,
            'truncation_issues': truncation_issues,
            'special_characters': dict(special_char_stats.most_common(10))
        }
    
    def analyze_semantic_integrity(self) -> Dict[str, Any]:
        """èªæ„å®Œæ•´æ€§åˆ†æ"""
        print("\nğŸ§  === èªæ„å®Œæ•´æ€§åˆ†æ ===")
        
        sentence_patterns = {
            'complete_sentences': 0,
            'incomplete_sentences': 0,
            'bullet_points': 0,
            'tables': 0,
            'code_blocks': 0
        }
        
        paragraph_issues = []
        context_breaks = []
        
        for i, chunk in enumerate(self.chunks):
            text = chunk['text'].strip()
            
            # å¥å­å®Œæ•´æ€§æª¢æŸ¥
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', text)
            complete_count = 0
            for sentence in sentences:
                if len(sentence.strip()) > 5:  # å¿½ç•¥å¤ªçŸ­çš„ç‰‡æ®µ
                    if sentence.strip().endswith(('ã€‚', '.', '!', '?', 'ï¼', 'ï¼Ÿ')):
                        complete_count += 1
                    else:
                        # æª¢æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨é …æˆ–è¡¨æ ¼
                        if re.match(r'^\s*[\d\-\*\â€¢]\s*', sentence) or '\t' in sentence:
                            sentence_patterns['bullet_points'] += 1
                        elif '|' in sentence and sentence.count('|') > 2:
                            sentence_patterns['tables'] += 1
                        elif any(keyword in sentence for keyword in ['def ', 'class ', 'import ', '```']):
                            sentence_patterns['code_blocks'] += 1
                        else:
                            sentence_patterns['incomplete_sentences'] += 1
            
            if complete_count > 0:
                sentence_patterns['complete_sentences'] += complete_count
            
            # æ®µè½é€£è²«æ€§æª¢æŸ¥
            paragraphs = text.split('\n\n')
            if len(paragraphs) > 1:
                for j in range(len(paragraphs) - 1):
                    # ç°¡å–®çš„ä¸Šä¸‹æ–‡é€£è²«æ€§æª¢æŸ¥
                    current = paragraphs[j].strip()
                    next_para = paragraphs[j + 1].strip()
                    if current and next_para and not self._has_topic_continuity(current, next_para):
                        context_breaks.append((i, j))
        
        print(f"ğŸ“ å¥å­æ¨¡å¼åˆ†æ:")
        for pattern, count in sentence_patterns.items():
            print(f"  {pattern:20}: {count}")
        
        print(f"\nğŸ”— é€£è²«æ€§å•é¡Œ: {len(context_breaks)} è™•")
        
        return {
            'sentence_patterns': sentence_patterns,
            'context_breaks': context_breaks
        }
    
    def analyze_content_overlap(self, similarity_threshold: float = 0.8) -> Dict[str, Any]:
        """å…§å®¹é‡è¤‡åˆ†æ"""
        print("\nğŸ”„ === å…§å®¹é‡è¤‡åˆ†æ ===")
        
        if not DIFFLIB_AVAILABLE:
            print("âš ï¸  æœªå®‰è£difflibï¼Œè·³éç›¸ä¼¼åº¦åˆ†æ")
            return {}
        
        duplicates = []
        high_similarity_pairs = []
        
        print(f"ğŸ” æª¢æŸ¥ {len(self.chunks)} chunks çš„ç›¸ä¼¼åº¦...")
        
        # åªæª¢æŸ¥å‰100å€‹chunksçš„ç›¸ä¼¼åº¦ï¼ˆé¿å…è¨ˆç®—é‡éå¤§ï¼‰
        sample_size = min(100, len(self.chunks))
        for i in range(sample_size):
            for j in range(i + 1, sample_size):
                text1 = self.chunks[i]['text']
                text2 = self.chunks[j]['text']
                
                similarity = SequenceMatcher(None, text1, text2).ratio()
                
                if similarity >= similarity_threshold:
                    if similarity > 0.95:
                        duplicates.append((i, j, similarity))
                    else:
                        high_similarity_pairs.append((i, j, similarity))
        
        print(f"ğŸ”´ å®Œå…¨é‡è¤‡: {len(duplicates)} å°")
        print(f"ğŸŸ¡ é«˜ç›¸ä¼¼åº¦: {len(high_similarity_pairs)} å°")
        
        return {
            'duplicates': duplicates,
            'high_similarity_pairs': high_similarity_pairs
        }
    
    def analyze_domain_characteristics(self) -> Dict[str, Any]:
        """é ˜åŸŸç‰¹æ€§åˆ†æ"""
        print("\nğŸ“ === é ˜åŸŸç‰¹æ€§åˆ†æ ===")
        
        technical_terms = Counter()
        citation_patterns = []
        figure_references = []
        table_references = []
        
        # å¸¸è¦‹çš„æŠ€è¡“/å­¸è¡“è©å½™æ¨¡å¼
        tech_patterns = [
            r'\b[A-Z]{2,}\b',  # ç¸®å¯«
            r'\b\d+\.\d+\b',   # ç‰ˆæœ¬è™Ÿæˆ–æ•¸å€¼
            r'\([^)]*\d{4}[^)]*\)',  # åŒ…å«å¹´ä»½çš„å¼•ç”¨
            r'Figure\s+\d+|åœ–\s*\d+',  # åœ–è¡¨å¼•ç”¨
            r'Table\s+\d+|è¡¨\s*\d+',   # è¡¨æ ¼å¼•ç”¨
            r'Section\s+\d+|ç¬¬.*ç« |ç¬¬.*ç¯€',  # ç« ç¯€å¼•ç”¨
        ]
        
        for i, chunk in enumerate(self.chunks):
            text = chunk['text']
            
            # æŠ€è¡“è¡“èªçµ±è¨ˆ
            for pattern in tech_patterns:
                matches = re.findall(pattern, text)
                technical_terms.update(matches)
            
            # å¼•ç”¨æ¨¡å¼
            citations = re.findall(r'\[[^\]]*\d+[^\]]*\]|\([^)]*\d{4}[^)]*\)', text)
            citation_patterns.extend(citations)
            
            # åœ–è¡¨å¼•ç”¨
            figures = re.findall(r'Figure\s+\d+[\.\d]*|åœ–\s*\d+[\.\d]*', text, re.IGNORECASE)
            figure_references.extend(figures)
            
            tables = re.findall(r'Table\s+\d+[\.\d]*|è¡¨\s*\d+[\.\d]*', text, re.IGNORECASE)
            table_references.extend(tables)
        
        print(f"ğŸ”¬ æŠ€è¡“ç‰¹å¾µ:")
        print(f"  æŠ€è¡“è¡“èª: {len(technical_terms)} ç¨®")
        print(f"  å¼•ç”¨æ¨¡å¼: {len(citation_patterns)} å€‹")
        print(f"  åœ–è¡¨å¼•ç”¨: {len(figure_references)} å€‹")
        print(f"  è¡¨æ ¼å¼•ç”¨: {len(table_references)} å€‹")
        
        if technical_terms:
            print(f"\nğŸ·ï¸  å¸¸è¦‹æŠ€è¡“è¡“èª TOP 10:")
            for term, count in technical_terms.most_common(10):
                print(f"  {term}: {count}")
        
        return {
            'technical_terms': dict(technical_terms.most_common(20)),
            'citation_count': len(citation_patterns),
            'figure_references': len(figure_references),
            'table_references': len(table_references)
        }
    
    def generate_recommendations(self, analysis_results: Dict[str, Any]) -> List[str]:
        """ç”¢ç”Ÿæ”¹å–„å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼çµ±è¨ˆçµæœçµ¦å»ºè­°
        basic_stats = analysis_results.get('basic_stats', {})
        text_quality = analysis_results.get('text_quality', {})
        semantic = analysis_results.get('semantic_integrity', {})
        
        if basic_stats.get('avg_length', 0) < 300:
            recommendations.append("âš ï¸  å¹³å‡chunké•·åº¦åçŸ­ï¼Œå»ºè­°å¢åŠ chunk_sizeåƒæ•¸")
        
        if basic_stats.get('avg_length', 0) > 2000:
            recommendations.append("âš ï¸  å¹³å‡chunké•·åº¦éé•·ï¼Œå»ºè­°æ¸›å°‘chunk_sizeåƒæ•¸")
        
        length_dist = basic_stats.get('length_distribution', {})
        if length_dist.get('very_short', 0) > len(self.chunks) * 0.1:
            recommendations.append("âš ï¸  éå¤šæ¥µçŸ­chunksï¼Œå»ºè­°èª¿æ•´æœ€å°é•·åº¦éæ¿¾")
        
        if text_quality.get('encoding_issues'):
            recommendations.append("âŒ ç™¼ç¾ç·¨ç¢¼å•é¡Œï¼Œè«‹æª¢æŸ¥åŸå§‹æ–‡ä»¶ç·¨ç¢¼")
        
        if text_quality.get('truncation_issues'):
            recommendations.append("âŒ ç™¼ç¾æˆªæ–·å•é¡Œï¼Œå»ºè­°èª¿æ•´overlapåƒæ•¸æˆ–åˆ†éš”ç¬¦è¨­å®š")
        
        lang_dist = text_quality.get('language_distribution', {})
        if lang_dist.get('mixed', 0) > len(self.chunks) * 0.3:
            recommendations.append("â„¹ï¸  ä¸­è‹±æ··é›œå…§å®¹è¼ƒå¤šï¼Œé©åˆå¤šèªè¨€embeddingæ¨¡å‹")
        
        if semantic.get('context_breaks'):
            recommendations.append("âš ï¸  ç™¼ç¾ä¸Šä¸‹æ–‡æ–·è£‚ï¼Œå»ºè­°å¢åŠ overlapæˆ–èª¿æ•´åˆ†å‰²ç­–ç•¥")
        
        return recommendations
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ é–‹å§‹å…¨é¢chunkså“è³ªåˆ†æ...\n")
        
        results = {}
        
        try:
            results['basic_stats'] = self.analyze_basic_stats()
            results['text_quality'] = self.analyze_text_quality()
            results['semantic_integrity'] = self.analyze_semantic_integrity()
            results['content_overlap'] = self.analyze_content_overlap()
            results['domain_characteristics'] = self.analyze_domain_characteristics()
            
            # ç”¢ç”Ÿå»ºè­°
            recommendations = self.generate_recommendations(results)
            
            print("\n" + "="*60)
            print("ğŸ“‹ === æ”¹å–„å»ºè­° ===")
            if recommendations:
                for i, rec in enumerate(recommendations, 1):
                    print(f"{i:2d}. {rec}")
            else:
                print("âœ… Chunkså“è³ªè‰¯å¥½ï¼Œç„¡æ˜é¡¯å•é¡Œ")
            
            print("\n" + "="*60)
            print("âœ… åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ åˆ†æéç¨‹å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
        
        return results
    
    def _calculate_std(self, values: List[float]) -> float:
        """è¨ˆç®—æ¨™æº–å·®"""
        if not values:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def _has_topic_continuity(self, text1: str, text2: str) -> bool:
        """ç°¡å–®çš„ä¸»é¡Œé€£è²«æ€§æª¢æŸ¥"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šæª¢æŸ¥æ˜¯å¦æœ‰å…±åŒé—œéµè©
        if JIEBA_AVAILABLE:
            words1 = set(jieba.lcut(text1))
            words2 = set(jieba.lcut(text2))
            common_words = words1 & words2
            return len(common_words) >= 2
        else:
            # åŸºç¤ç‰ˆæœ¬ï¼šæª¢æŸ¥è‹±æ–‡å–®è©é‡ç–Š
            words1 = set(re.findall(r'\b[a-zA-Z]{3,}\b', text1.lower()))
            words2 = set(re.findall(r'\b[a-zA-Z]{3,}\b', text2.lower()))
            common_words = words1 & words2
            return len(common_words) >= 1


def main():
    parser = argparse.ArgumentParser(description="Chunkså“è³ªæª¢æŸ¥å·¥å…·")
    parser.add_argument("--input", default="indices/chunks.jsonl", help="chunks.jsonlæª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", help="çµæœè¼¸å‡ºæª”æ¡ˆï¼ˆå¯é¸ï¼‰")
    
    args = parser.parse_args()
    
    try:
        checker = ChunksQualityChecker(args.input)
        results = checker.run_full_analysis()
        
        if args.output:
            output_path = Path(args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ çµæœå·²ä¿å­˜åˆ°: {output_path}")
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()