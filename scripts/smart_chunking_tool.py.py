# -*- coding: utf-8 -*-
"""
èªè¨€è‡ªé©æ‡‰æ™ºèƒ½åˆ†å¡Šå·¥å…·
ä¸»è¦æ”¹é€²ï¼š
1. è‡ªå‹•æª¢æ¸¬å…§å®¹èªè¨€ï¼ˆä¸­æ–‡/è‹±æ–‡/æ··åˆï¼‰
2. æ ¹æ“šèªè¨€å‹•æ…‹èª¿æ•´chunké•·åº¦
3. èªè¨€ç‰¹å®šçš„é‚Šç•Œæª¢æ¸¬
4. æ”¹é€²çš„å“è³ªè©•åˆ†ç³»çµ±
"""

import os
import re
import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import argparse

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("[WARN] PyMuPDFæœªå®‰è£ï¼Œç„¡æ³•è™•ç†PDFæª”æ¡ˆ")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("[WARN] LangChainæœªå®‰è£ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆåˆ†å¡ŠåŠŸèƒ½")

try:
    from tqdm.auto import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False


class LanguageAwareChunkBuilder:
    """èªè¨€æ„ŸçŸ¥çš„æ™ºèƒ½åˆ†å¡Šå™¨"""
    
    def __init__(self, 
                 base_chunk_size: int = 1200,
                 base_overlap: int = 150,
                 base_min_size: int = 100,
                 quality_threshold: float = 0.6):
        
        self.base_chunk_size = base_chunk_size
        self.base_overlap = base_overlap
        self.base_min_size = base_min_size
        self.quality_threshold = quality_threshold
        
        # èªè¨€ç‰¹å®šé…ç½®
        self.lang_config = {
            "chinese": {
                "size_multiplier": 1.3,    # ä¸­æ–‡éœ€è¦æ›´å¤šå­—ç¬¦
                "overlap_multiplier": 1.2,
                "min_size_multiplier": 1.5,
                "sentence_enders": ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦'],
                "clause_enders": ['ï¼›', 'ï¼š'],
                "phrase_enders": ['ï¼Œ', 'ã€']
            },
            "english": {
                "size_multiplier": 1.0,    # è‹±æ–‡ä¿æŒåŸé•·åº¦
                "overlap_multiplier": 1.0,
                "min_size_multiplier": 1.0,
                "sentence_enders": ['.', '!', '?'],
                "clause_enders": [';', ':'],
                "phrase_enders": [',']
            },
            "mixed": {
                "size_multiplier": 1.15,   # æ··åˆå…§å®¹å–ä¸­é–“å€¼
                "overlap_multiplier": 1.1,
                "min_size_multiplier": 1.25,
                "sentence_enders": ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'â€¦'],
                "clause_enders": ['ï¼›', 'ï¼š', ';', ':'],
                "phrase_enders": ['ï¼Œ', 'ã€', ',']
            }
        }
        
        print(f"[INIT] èªè¨€è‡ªé©æ‡‰åˆ†å¡Šå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"       åŸºç¤åƒæ•¸: chunk_size={base_chunk_size}, overlap={base_overlap}")
    
    def detect_content_language(self, text: str) -> Tuple[str, Dict[str, float]]:
        """
        æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€
        è¿”å›: (ä¸»è¦èªè¨€, èªè¨€çµ±è¨ˆ)
        """
        if not text.strip():
            return "unknown", {}
            
        # çµ±è¨ˆä¸åŒé¡å‹å­—ç¬¦
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_meaningful = chinese_chars + english_chars
        
        if total_meaningful == 0:
            return "unknown", {"chinese": 0, "english": 0, "other": 100}
        
        chinese_ratio = chinese_chars / total_meaningful
        english_ratio = english_chars / total_meaningful
        
        stats = {
            "chinese": chinese_ratio * 100,
            "english": english_ratio * 100,
            "other": max(0, 100 - (chinese_ratio + english_ratio) * 100)
        }
        
        # åˆ¤å®šä¸»è¦èªè¨€
        if chinese_ratio > 0.6:
            return "chinese", stats
        elif english_ratio > 0.6:
            return "english", stats
        elif chinese_ratio > 0.3 and english_ratio > 0.3:
            return "mixed", stats
        else:
            return "unknown", stats
    
    def get_language_specific_params(self, lang: str) -> Dict[str, int]:
        """æ ¹æ“šèªè¨€ç²å–åˆ†å¡Šåƒæ•¸"""
        if lang not in self.lang_config:
            lang = "mixed"  # é»˜èªä½¿ç”¨æ··åˆé…ç½®
            
        config = self.lang_config[lang]
        return {
            "chunk_size": int(self.base_chunk_size * config["size_multiplier"]),
            "overlap": int(self.base_overlap * config["overlap_multiplier"]),
            "min_size": int(self.base_min_size * config["min_size_multiplier"]),
            "sentence_enders": config["sentence_enders"],
            "clause_enders": config["clause_enders"],
            "phrase_enders": config["phrase_enders"]
        }
    
    def smart_chunk_with_language_awareness(self, text: str) -> List[Dict]:
        """
        èªè¨€æ„ŸçŸ¥çš„æ™ºèƒ½åˆ†å¡Š
        è¿”å›åŒ…å«èªè¨€ä¿¡æ¯çš„chunkå­—å…¸åˆ—è¡¨
        """
        if not text or len(text.strip()) < 10:
            return []
        
        # æª¢æ¸¬æ•´é«”èªè¨€
        main_lang, lang_stats = self.detect_content_language(text)
        lang_params = self.get_language_specific_params(main_lang)
        
        print(f"[LANG] æª¢æ¸¬åˆ°ä¸»è¦èªè¨€: {main_lang}")
        print(f"       èªè¨€åˆ†å¸ƒ: ä¸­æ–‡{lang_stats.get('chinese', 0):.1f}%, "
              f"è‹±æ–‡{lang_stats.get('english', 0):.1f}%")
        print(f"       èª¿æ•´åƒæ•¸: chunk_size={lang_params['chunk_size']}, "
              f"min_size={lang_params['min_size']}")
        
        # å‰µå»ºèªè¨€ç‰¹å®šçš„æ–‡æœ¬åˆ†å‰²å™¨
        if LANGCHAIN_AVAILABLE:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=lang_params["chunk_size"],
                chunk_overlap=lang_params["overlap"],
                separators=["\n\n", "\n"] + lang_params["sentence_enders"] + 
                           lang_params["clause_enders"] + [" ", ""]
            )
            initial_chunks = splitter.split_text(text)
        else:
            # ç°¡åŒ–ç‰ˆåˆ†å‰²
            initial_chunks = self._simple_split_text(text, lang_params)
        
        # è™•ç†æ¯å€‹chunkä¸¦æ·»åŠ èªè¨€ä¿¡æ¯
        processed_chunks = []
        for i, chunk_text in enumerate(initial_chunks):
            if len(chunk_text.strip()) < lang_params["min_size"]:
                continue
            
            # èª¿æ•´chunké‚Šç•Œ
            adjusted_chunk = self._adjust_chunk_boundary(chunk_text, lang_params)
            if not adjusted_chunk or len(adjusted_chunk) < lang_params["min_size"]:
                continue
            
            # æª¢æ¸¬é€™å€‹chunkçš„å…·é«”èªè¨€
            chunk_lang, chunk_lang_stats = self.detect_content_language(adjusted_chunk)
            
            # è¨ˆç®—å“è³ªåˆ†æ•¸
            quality_score = self._calculate_language_aware_quality(
                adjusted_chunk, chunk_lang, lang_params
            )
            
            chunk_info = {
                "text": adjusted_chunk,
                "chunk_id": i,
                "length": len(adjusted_chunk),
                "quality_score": quality_score,
                "main_language": chunk_lang,
                "language_stats": chunk_lang_stats,
                "global_language": main_lang,
                "language_params": lang_params,
                "_quality": "high" if quality_score >= 0.8 else 
                           "medium" if quality_score >= 0.6 else "low"
            }
            
            processed_chunks.append(chunk_info)
        
        return processed_chunks
    
    def _simple_split_text(self, text: str, lang_params: Dict) -> List[str]:
        """ç°¡åŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²ï¼ˆç•¶LangChainä¸å¯ç”¨æ™‚ï¼‰"""
        chunks = []
        chunk_size = lang_params["chunk_size"]
        overlap = lang_params["overlap"]
        
        words = text.split()
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word) + 1  # +1 for space
            
            if current_length + word_length > chunk_size and current_chunk:
                chunks.append(" ".join(current_chunk))
                # ä¿ç•™é‡ç–Š
                overlap_words = current_chunk[-overlap//10:] if len(current_chunk) > overlap//10 else []
                current_chunk = overlap_words + [word]
                current_length = sum(len(w) + 1 for w in current_chunk)
            else:
                current_chunk.append(word)
                current_length += word_length
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def _adjust_chunk_boundary(self, chunk: str, lang_params: Dict) -> str:
        """æ ¹æ“šèªè¨€ç‰¹æ€§èª¿æ•´chunké‚Šç•Œ"""
        if len(chunk) <= lang_params["chunk_size"] * 0.8:
            return chunk
        
        max_length = int(lang_params["chunk_size"] * 0.95)
        if len(chunk) <= max_length:
            return chunk
        
        # åœ¨æœ€å¤§é•·åº¦é™„è¿‘å°‹æ‰¾é©åˆçš„åˆ†å‰²é»
        search_start = max(max_length - 200, max_length // 2)
        search_end = min(max_length + 100, len(chunk))
        search_text = chunk[search_start:search_end]
        
        best_cut = max_length
        
        # æŒ‰å„ªå…ˆç´šå°‹æ‰¾åˆ†å‰²é»
        for enders in [lang_params["sentence_enders"], 
                      lang_params["clause_enders"], 
                      lang_params["phrase_enders"]]:
            for ender in enders:
                pos = search_text.rfind(ender)
                if pos > 0:
                    actual_pos = search_start + pos + 1
                    if actual_pos > len(chunk) * 0.5:
                        best_cut = actual_pos
                        break
            if best_cut != max_length:
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°åˆé©åˆ†å‰²é»ï¼Œåœ¨ç©ºæ ¼è™•åˆ†å‰²
        if best_cut == max_length:
            space_pos = chunk.rfind(' ', max_length - 100, max_length)
            if space_pos > max_length * 0.7:
                best_cut = space_pos
        
        return chunk[:best_cut].strip()
    
    def _calculate_language_aware_quality(self, chunk: str, lang: str, 
                                        lang_params: Dict) -> float:
        """èªè¨€æ„ŸçŸ¥çš„å“è³ªè©•åˆ†"""
        if not chunk:
            return 0.0
        
        score = 1.0
        length = len(chunk)
        
        # é•·åº¦è©•åˆ† - æ ¹æ“šèªè¨€èª¿æ•´
        min_size = lang_params["min_size"]
        ideal_size = lang_params["chunk_size"] * 0.7
        
        if length < min_size:
            score *= 0.3
        elif length < min_size * 2:
            score *= 0.7
        elif min_size * 2 <= length <= ideal_size:
            score *= 1.0  # æœ€ä½³é•·åº¦ç¯„åœ
        elif length > lang_params["chunk_size"] * 1.2:
            score *= 0.8  # å¤ªé•·æ‰£åˆ†
        
        # é‚Šç•Œå®Œæ•´æ€§è©•åˆ†
        endings = lang_params["sentence_enders"] + lang_params["clause_enders"]
        if chunk.rstrip().endswith(tuple(endings)):
            score *= 1.1  # è‰¯å¥½é‚Šç•ŒåŠ åˆ†
        
        # èªè¨€ç´”åº¦è©•åˆ†
        chunk_lang, chunk_stats = self.detect_content_language(chunk)
        if chunk_lang == lang:
            score *= 1.05  # èªè¨€ä¸€è‡´æ€§åŠ åˆ†
        elif chunk_lang == "mixed" and lang in ["chinese", "english"]:
            score *= 0.95  # æ··åˆå…§å®¹å°å¹…æ‰£åˆ†
        
        # å…§å®¹å¯†åº¦è©•åˆ†
        meaningful_chars = len(re.findall(r'[\w\u4e00-\u9fff]', chunk))
        if meaningful_chars / length < 0.3:
            score *= 0.5  # å¤ªå¤šç‰¹æ®Šå­—ç¬¦æˆ–ç©ºç™½
        
        # æŠ€è¡“å…§å®¹æª¢æ¸¬ï¼ˆæŠ€è¡“æ–‡æª”é€šå¸¸å“è³ªè¼ƒé«˜ï¼‰
        tech_indicators = len(re.findall(r'\b[A-Z]{2,}\b|\d+\.\d+|\([^)]*\d{4}[^)]*\)', chunk))
        if tech_indicators > 0:
            score *= min(1.2, 1 + tech_indicators * 0.02)
        
        return min(1.0, score)
    
    def process_text_file(self, file_path: Path) -> List[Dict]:
        """è™•ç†æ–‡æœ¬æª”æ¡ˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                text = f.read()
            
            chunks = self.smart_chunk_with_language_awareness(text)
            
            # æ·»åŠ æª”æ¡ˆä¿¡æ¯
            for chunk in chunks:
                chunk["source"] = file_path.name
                chunk["page"] = 0
            
            return chunks
            
        except Exception as e:
            print(f"[ERROR] è™•ç†æ–‡æœ¬æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
            return []
    
    def process_pdf_file(self, file_path: Path) -> List[Dict]:
        """è™•ç†PDFæª”æ¡ˆ"""
        if not PYMUPDF_AVAILABLE:
            print(f"[SKIP] PyMuPDFæœªå®‰è£ï¼Œè·³éPDFæª”æ¡ˆ: {file_path}")
            return []
        
        chunks = []
        
        try:
            doc = fitz.open(str(file_path))
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():
                    page_chunks = self.smart_chunk_with_language_awareness(text)
                    
                    # æ·»åŠ é é¢ä¿¡æ¯
                    for chunk in page_chunks:
                        chunk["source"] = file_path.name
                        chunk["page"] = page_num + 1
                    
                    chunks.extend(page_chunks)
            
            doc.close()
            return chunks
            
        except Exception as e:
            print(f"[ERROR] è™•ç†PDFæª”æ¡ˆ {file_path} å¤±æ•—: {e}")
            return []
    
    def process_directory(self, input_dir: Path, output_file: Path):
        """è™•ç†æ•´å€‹ç›®éŒ„"""
        if not input_dir.exists():
            print(f"[ERROR] è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
            return
        
        # æ”¯æ´çš„æª”æ¡ˆé¡å‹
        supported_extensions = {'.txt', '.md', '.pdf'}
        files = [f for f in input_dir.rglob('*') 
                if f.is_file() and f.suffix.lower() in supported_extensions]
        
        if not files:
            print(f"[WARN] åœ¨ç›®éŒ„ {input_dir} ä¸­æœªæ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ")
            return
        
        all_chunks = []
        stats = {
            "total_files": len(files),
            "processed_files": 0,
            "total_chunks": 0,
            "language_distribution": {"chinese": 0, "english": 0, "mixed": 0, "unknown": 0},
            "quality_distribution": {"high": 0, "medium": 0, "low": 0}
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"[INFO] é–‹å§‹è™•ç† {len(files)} å€‹æª”æ¡ˆ...")
        
        file_iter = tqdm(files, desc="è™•ç†æª”æ¡ˆ") if TQDM_AVAILABLE else files
        
        for file_path in file_iter:
            print(f"[PROCESS] è™•ç†æª”æ¡ˆ: {file_path.name}")
            
            if file_path.suffix.lower() == '.pdf':
                chunks = self.process_pdf_file(file_path)
            else:
                chunks = self.process_text_file(file_path)
            
            if chunks:
                all_chunks.extend(chunks)
                stats["processed_files"] += 1
                
                # æ›´æ–°çµ±è¨ˆ
                for chunk in chunks:
                    lang = chunk.get("main_language", "unknown")
                    quality = chunk.get("_quality", "low")
                    stats["language_distribution"][lang] += 1
                    stats["quality_distribution"][quality] += 1
        
        stats["total_chunks"] = len(all_chunks)
        
        # å¯«å…¥è¼¸å‡ºæª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            for chunk in all_chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
        
        # é¡¯ç¤ºçµ±è¨ˆçµæœ
        print(f"\nğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   è™•ç†æª”æ¡ˆ: {stats['processed_files']}/{stats['total_files']}")
        print(f"   ç”Ÿæˆchunks: {stats['total_chunks']} å€‹")
        print(f"   èªè¨€åˆ†å¸ƒ:")
        for lang, count in stats["language_distribution"].items():
            if count > 0:
                pct = count / stats["total_chunks"] * 100
                print(f"     {lang}: {count} ({pct:.1f}%)")
        print(f"   å“è³ªåˆ†å¸ƒ:")
        for quality, count in stats["quality_distribution"].items():
            if count > 0:
                pct = count / stats["total_chunks"] * 100
                print(f"     {quality}: {count} ({pct:.1f}%)")
        
        print(f"\nâœ… è¼¸å‡ºè‡³: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="èªè¨€è‡ªé©æ‡‰æ™ºèƒ½åˆ†å¡Šå·¥å…·")
    parser.add_argument("--input", default="datasets", help="è¼¸å…¥è³‡æ–™å¤¾")
    parser.add_argument("--output", default="indices/chunks_language_aware.jsonl", help="è¼¸å‡ºæª”æ¡ˆ")
    parser.add_argument("--base-chunk-size", type=int, default=1200, help="åŸºç¤chunkå¤§å°")
    parser.add_argument("--base-overlap", type=int, default=150, help="åŸºç¤é‡ç–Šå¤§å°")
    parser.add_argument("--base-min-size", type=int, default=100, help="åŸºç¤æœ€å°chunkå¤§å°")
    parser.add_argument("--quality-threshold", type=float, default=0.6, help="å“è³ªé–¾å€¼")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†å¡Šå™¨
    builder = LanguageAwareChunkBuilder(
        base_chunk_size=args.base_chunk_size,
        base_overlap=args.base_overlap,
        base_min_size=args.base_min_size,
        quality_threshold=args.quality_threshold
    )
    
    input_dir = Path(args.input)
    output_file = Path(args.output)
    
    try:
        print("ğŸš€ é–‹å§‹èªè¨€è‡ªé©æ‡‰åˆ†å¡Šè™•ç†...")
        builder.process_directory(input_dir, output_file)
        print("ğŸ‰ è™•ç†å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()