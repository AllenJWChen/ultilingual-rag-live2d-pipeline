# -*- coding: utf-8 -*-
"""
build_chunks_optimized.py - å„ªåŒ–ç‰ˆæœ¬çš„chunksåˆ‡åˆ†å·¥å…·

ä¿®æ­£ç‰ˆæœ¬ï¼Œè§£æ±ºèªæ³•éŒ¯èª¤ä¸¦ç°¡åŒ–éƒ¨åˆ†åŠŸèƒ½
"""

from __future__ import annotations
import os, re, sys, json, argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import fitz  # PyMuPDF
from langchain.text_splitter import RecursiveCharacterTextSplitter

# å¯é¸ä¾è³´æª¢æŸ¥
try:
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False
    print("[INFO] PaddleOCR not available. OCRåŠŸèƒ½å°‡è¢«ç¦ç”¨")

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("[INFO] jieba not available. ä¸­æ–‡åˆ†è©åŠŸèƒ½å°‡è¢«ç¦ç”¨")


class OptimizedChunkBuilder:
    """å„ªåŒ–ç‰ˆchunkså»ºæ§‹å™¨"""
    
    def __init__(self, 
                 chunk_size: int = 1200,
                 chunk_overlap: int = 150,
                 min_chunk_size: int = 100,
                 quality_threshold: float = 0.6):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.quality_threshold = quality_threshold
        
        # å„ªåŒ–çš„åˆ†éš”ç¬¦é †åº - é‡å°ä¸­è‹±æ··é›œæŠ€è¡“æ–‡ä»¶
        self.separators = [
            "\n\n\n",           # å¤šå€‹æ›è¡Œ
            "\n\n",             # æ®µè½åˆ†éš”
            "\n",               # å–®æ›è¡Œ
            "ã€‚",               # ä¸­æ–‡å¥è™Ÿ
            "ï¼",               # ä¸­æ–‡é©šå˜†è™Ÿ
            "ï¼Ÿ",               # ä¸­æ–‡å•è™Ÿ
            "ï¼›",               # ä¸­æ–‡åˆ†è™Ÿ
            ".",                # è‹±æ–‡å¥è™Ÿ
            "!",                # è‹±æ–‡é©šå˜†è™Ÿ  
            "?",                # è‹±æ–‡å•è™Ÿ
            ";",                # è‹±æ–‡åˆ†è™Ÿ
            "ï¼š",               # ä¸­æ–‡å†’è™Ÿ
            ":",                # è‹±æ–‡å†’è™Ÿ
            "ï¼Œ",               # ä¸­æ–‡é€—è™Ÿ
            ",",                # è‹±æ–‡é€—è™Ÿ
            "ï¼‰",               # ä¸­æ–‡å³æ‹¬è™Ÿ
            ")",                # è‹±æ–‡å³æ‹¬è™Ÿ
            "ã€‘",               # ä¸­æ–‡å³æ–¹æ‹¬è™Ÿ
            "]",                # è‹±æ–‡å³æ–¹æ‹¬è™Ÿ
            "ã€",               # ä¸­æ–‡é “è™Ÿ
            " ",                # ç©ºæ ¼
            "",                 # å­—ç¬¦ç´šåˆ†å‰²
        ]
        
        # åˆå§‹åŒ–æ–‡å­—åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=self.separators,
            keep_separator=True
        )
        
        # OCRå¼•æ“åˆå§‹åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.ocr = None
        if PADDLEOCR_AVAILABLE:
            try:
                self.ocr = PaddleOCR(use_angle_cls=True, lang="ch")
                print("[INFO] OCRå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"[WARN] OCRåˆå§‹åŒ–å¤±æ•—: {e}")
    
    def clean_ocr_text(self, text: str) -> str:
        """æ¸…ç†OCRè­˜åˆ¥å‡ºçš„æ–‡å­—"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # ä¿®å¾©å¸¸è¦‹OCRéŒ¯èª¤
        corrections = {
            # ä¸­æ–‡æ¨™é»ä¿®å¾©
            ' ï¼Œ ': 'ï¼Œ',
            ' ã€‚ ': 'ã€‚',
            ' ï¼ ': 'ï¼', 
            ' ï¼Ÿ ': 'ï¼Ÿ',
            ' ï¼› ': 'ï¼›',
            ' ï¼š ': 'ï¼š',
            
            # è‹±æ–‡æ¨™é»ä¿®å¾©
            ' , ': ', ',
            ' . ': '. ',
            ' ! ': '! ',
            ' ? ': '? ',
            ' ; ': '; ',
            ' : ': ': ',
            
            # æ‹¬è™Ÿä¿®å¾©
            '( ': 'ï¼ˆ',
            ' )': 'ï¼‰',
            '[ ': 'ã€',
            ' ]': 'ã€‘',
        }
        
        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)
        
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)
        
        return text.strip()
    
    def extract_page_text_enhanced(self, page, page_idx: int, tmp_dir: Path) -> Tuple[str, bool]:
        """å¢å¼·ç‰ˆé é¢æ–‡å­—æŠ½å–"""
        # å…ˆå˜—è©¦æ–‡å­—å±¤
        text = (page.get_text("text") or "").strip()
        used_ocr = False
        
        # å¦‚æœæ–‡å­—å±¤å“è³ªä¸ä½³ä¸”æœ‰OCRï¼Œå‰‡ä½¿ç”¨OCR
        if (len(text) < 50 or self._is_low_quality_text(text)) and self.ocr:
            print(f"[OCR] é é¢ {page_idx+1} æ–‡å­—å±¤å“è³ªä¸ä½³ï¼Œä½¿ç”¨OCR...")
            tmp_dir.mkdir(parents=True, exist_ok=True)
            img_path = tmp_dir / f"page_{page_idx+1}.png"
            
            try:
                # é«˜è§£æåº¦è½‰åœ–
                pix = page.get_pixmap(dpi=300)
                pix.save(img_path.as_posix())
                
                # OCRè­˜åˆ¥
                result = self.ocr.ocr(img_path.as_posix(), cls=True)
                if result and result[0]:
                    ocr_text = "\n".join([line[1][0] for line in result[0]])
                    ocr_text = self.clean_ocr_text(ocr_text)
                    if len(ocr_text) > len(text):
                        text = ocr_text
                        used_ocr = True
                        
            except Exception as e:
                print(f"[WARN] OCRå¤±æ•—: {e}")
            finally:
                # æ¸…ç†æš«å­˜æª”
                try:
                    img_path.unlink(missing_ok=True)
                except:
                    pass
        
        # æ¸…ç†æ–‡å­—
        text = self._clean_extracted_text(text)
        return text, used_ocr
    
    def _is_low_quality_text(self, text: str) -> bool:
        """åˆ¤æ–·æ–‡å­—å“è³ªæ˜¯å¦éä½"""
        if not text or len(text) < 20:
            return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«éå¤šç‰¹æ®Šå­—ç¬¦
        special_chars = len(re.findall(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\-\.]', text))
        if special_chars / len(text) > 0.3:
            return True
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ„ç¾©çš„æ–‡å­—å…§å®¹
        meaningful_chars = len(re.findall(r'[\w\u4e00-\u9fff]', text))
        if meaningful_chars / len(text) < 0.3:
            return True
        
        return False
    
    def _clean_extracted_text(self, text: str) -> str:
        """æ¸…ç†æŠ½å–çš„æ–‡å­—"""
        if not text:
            return ""
        
        # çµ±ä¸€æ›è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # ç§»é™¤éå¤šé€£çºŒæ›è¡Œï¼ˆä¿ç•™æ®µè½çµæ§‹ï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # æ¸…ç†é é¦–é å°¾å¸¸è¦‹é›œè¨Š - ä¿®æ­£å¾Œçš„ç‰ˆæœ¬
        noise_patterns = [
            r'^\d+$',                    # å–®ç¨çš„é ç¢¼
            r'^Page \d+$',               # "Page X"
            r'^ç¬¬\s*\d+\s*é $',          # "ç¬¬Xé "
            r'^\d+\s*/\s*\d+$',         # "1/10"
            r'^Copyright.*$',            # ç‰ˆæ¬Šä¿¡æ¯
            r'^Â©.*$',                   # ç‰ˆæ¬Šç¬¦è™Ÿ
        ]
        
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                cleaned_lines.append('')
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé›œè¨Š
            is_noise = False
            for pattern in noise_patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    is_noise = True
                    break
            
            if not is_noise:
                cleaned_lines.append(line)
        
        # é‡çµ„æ–‡å­—ä¸¦æ¸…ç†å¤šé¤˜ç©ºè¡Œ
        text = '\n'.join(cleaned_lines)
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def smart_chunk_with_boundaries(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å¡Š - è€ƒæ…®å¥å­é‚Šç•Œ"""
        if not text or len(text) < self.min_chunk_size:
            return []
        
        # ä½¿ç”¨langchainçš„åˆ†å‰²å™¨é€²è¡Œåˆæ­¥åˆ†å‰²
        initial_chunks = self.text_splitter.split_text(text)
        
        # å°æ¯å€‹chunké€²è¡Œé‚Šç•Œå„ªåŒ–å’Œå“è³ªæª¢æŸ¥
        optimized_chunks = []
        for chunk in initial_chunks:
            chunk = chunk.strip()
            if len(chunk) < self.min_chunk_size:
                continue
            
            # èª¿æ•´chunké‚Šç•Œåˆ°åˆé©çš„å¥å­çµæŸä½ç½®
            adjusted_chunk = self._adjust_chunk_boundary(chunk)
            if adjusted_chunk and len(adjusted_chunk) >= self.min_chunk_size:
                optimized_chunks.append(adjusted_chunk)
        
        return optimized_chunks
    
    def _adjust_chunk_boundary(self, chunk: str) -> str:
        """èª¿æ•´chunké‚Šç•Œåˆ°é©ç•¶çš„å¥å­çµæŸä½ç½®"""
        if len(chunk) <= self.chunk_size * 0.8:  # å¦‚æœchunkä¸å¤ªé•·ï¼Œç›´æ¥è¿”å›
            return chunk
        
        # å°‹æ‰¾æœ€ä½³æˆªæ–·é»
        max_length = int(self.chunk_size * 0.95)  # å…è¨±ç¨å¾®è¶…éç›®æ¨™é•·åº¦
        
        if len(chunk) <= max_length:
            return chunk
        
        # åœ¨æœ€å¤§é•·åº¦é™„è¿‘å°‹æ‰¾åˆé©çš„å¥å­é‚Šç•Œ
        search_start = max(max_length - 200, max_length // 2)
        search_end = min(max_length + 100, len(chunk))
        
        best_cut = max_length
        search_text = chunk[search_start:search_end]
        
        # æŒ‰å„ªå…ˆç´šå°‹æ‰¾åˆ†å‰²é»
        sentence_enders = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
        clause_enders = ['ï¼›', ';', 'ï¼š', ':']
        phrase_enders = ['ï¼Œ', ',', 'ã€']
        
        for enders in [sentence_enders, clause_enders, phrase_enders]:
            for ender in enders:
                pos = search_text.rfind(ender)
                if pos > 0:
                    actual_pos = search_start + pos + 1
                    if actual_pos > len(chunk) * 0.5:  # ç¢ºä¿ä¸æœƒåˆ‡å¾—å¤ªçŸ­
                        best_cut = actual_pos
                        break
            if best_cut != max_length:
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°åˆé©åˆ†å‰²é»ï¼Œåœ¨ç©ºæ ¼è™•åˆ†å‰²
        if best_cut == max_length:
            space_pos = chunk.rfind(' ', max_length - 100, max_length)
            if space_pos > max_length * 0.7:
                best_cut = space_pos
        
        return chunk[:best_cut].strip()
    
    def calculate_chunk_quality(self, chunk: str) -> float:
        """è¨ˆç®—chunkå“è³ªè©•åˆ† (0-1)"""
        if not chunk:
            return 0.0
        
        score = 1.0
        
        # é•·åº¦è©•åˆ†
        length = len(chunk)
        if length < self.min_chunk_size:
            score *= 0.3
        elif length < self.min_chunk_size * 2:
            score *= 0.7
        
        # å…§å®¹å®Œæ•´æ€§è©•åˆ†
        # æª¢æŸ¥æ˜¯å¦ä»¥å¥å­é‚Šç•ŒçµæŸ
        if not chunk.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼›', ';', 'ï¼š', ':')):
            score *= 0.8
        
        # æª¢æŸ¥ä¸­è‹±æ–‡æ¯”ä¾‹å¹³è¡¡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', chunk))
        english_chars = len(re.findall(r'[a-zA-Z]', chunk))
        total_meaningful = chinese_chars + english_chars
        
        if total_meaningful > 0:
            meaningful_ratio = total_meaningful / length
            if meaningful_ratio < 0.3:  # å¤ªå¤šç‰¹æ®Šå­—ç¬¦æˆ–ç©ºç™½
                score *= 0.5
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æˆ–åˆ—è¡¨çµæ§‹ï¼ˆé€™äº›é€šå¸¸å“è³ªè¼ƒå¥½ï¼‰
        if '|' in chunk and chunk.count('|') > 2:  # å¯èƒ½æ˜¯è¡¨æ ¼
            score *= 1.2
        elif re.search(r'^\s*[\d\-\*\â€¢]\s*', chunk, re.MULTILINE):  # å¯èƒ½æ˜¯åˆ—è¡¨
            score *= 1.1
        
        # æª¢æŸ¥æŠ€è¡“å…§å®¹å¯†åº¦
        tech_indicators = len(re.findall(r'\b[A-Z]{2,}\b|\d+\.\d+|\([^)]*\d{4}[^)]*\)', chunk))
        if tech_indicators > 0:
            score *= min(1.3, 1 + tech_indicators * 0.05)  # æŠ€è¡“å…§å®¹åŠ åˆ†
        
        return min(1.0, score)
    
    def build_chunks_from_pdf(self, pdf_path: Path, tmp_dir: Path) -> List[Dict]:
        """å¾PDFå»ºç«‹chunks"""
        chunks = []
        
        print(f"[INFO] è™•ç†PDF: {pdf_path.name}")
        
        try:
            doc = fitz.open(str(pdf_path))
            total_pages = len(doc)
            ocr_pages = 0
            
            for page_idx, page in enumerate(doc):
                print(f"[INFO] è™•ç†ç¬¬ {page_idx + 1}/{total_pages} é ", end="")
                
                # æŠ½å–é é¢æ–‡å­—
                text, used_ocr = self.extract_page_text_enhanced(page, page_idx, tmp_dir)
                if used_ocr:
                    ocr_pages += 1
                    print(" (OCR)", end="")
                
                print()
                
                if not text:
                    continue
                
                # æ™ºèƒ½åˆ†å¡Š
                page_chunks = self.smart_chunk_with_boundaries(text)
                
                # è©•ä¼°å“è³ªä¸¦éæ¿¾
                for chunk_text in page_chunks:
                    quality = self.calculate_chunk_quality(chunk_text)
                    
                    if quality >= self.quality_threshold:
                        chunk_data = {
                            "source": pdf_path.name,
                            "page": page_idx + 1,
                            "text": chunk_text,
                            "quality_score": round(quality, 3),
                            "length": len(chunk_text)
                        }
                        chunks.append(chunk_data)
                    else:
                        print(f"[WARN] ä½å“è³ªchunkå·²éæ¿¾ (å“è³ª: {quality:.3f})")
            
            print(f"[INFO] PDFè™•ç†å®Œæˆ: {len(chunks)} chunks, {ocr_pages}/{total_pages} é ä½¿ç”¨OCR")
            
        except Exception as e:
            print(f"[ERROR] è™•ç†PDFå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            
        return chunks
    
    def process_directory(self, input_dir: Path, output_file: Path, tmp_dir: Path):
        """è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰PDF"""
        pdf_files = list(input_dir.rglob("*.pdf")) + list(input_dir.rglob("*.PDF"))
        pdf_files = sorted(set(pdf_files))
        
        if not pdf_files:
            print(f"[WARN] åœ¨ {input_dir} ä¸­æ‰¾ä¸åˆ°PDFæª”æ¡ˆ")
            return
        
        print(f"[INFO] æ‰¾åˆ° {len(pdf_files)} å€‹PDFæª”æ¡ˆ")
        
        all_chunks = []
        stats = {
            'total_files': len(pdf_files),
            'total_chunks': 0,
            'total_pages': 0,
            'ocr_pages': 0,
            'avg_quality': 0,
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0}
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # è™•ç†æ¯å€‹PDF
        for pdf_path in pdf_files:
            print(f"\n{'='*60}")
            chunks = self.build_chunks_from_pdf(pdf_path, tmp_dir)
            all_chunks.extend(chunks)
            
            # çµ±è¨ˆæ›´æ–°
            stats['total_chunks'] += len(chunks)
            if chunks:
                qualities = [c['quality_score'] for c in chunks]
                
                for q in qualities:
                    if q >= 0.8:
                        stats['quality_distribution']['high'] += 1
                    elif q >= 0.6:
                        stats['quality_distribution']['medium'] += 1
                    else:
                        stats['quality_distribution']['low'] += 1
        
        # è¨ˆç®—å¹³å‡å“è³ª
        if all_chunks:
            all_qualities = [c['quality_score'] for c in all_chunks]
            stats['avg_quality'] = sum(all_qualities) / len(all_qualities)
        
        # è¼¸å‡ºçµæœ
        with open(output_file, 'w', encoding='utf-8') as f:
            for chunk in all_chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
        
        # é¡¯ç¤ºçµ±è¨ˆ
        print(f"\n{'='*60}")
        print("ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"  PDFæª”æ¡ˆ: {stats['total_files']}")
        print(f"  ç¸½chunks: {stats['total_chunks']}")
        print(f"  å¹³å‡å“è³ª: {stats['avg_quality']:.3f}")
        print(f"  å“è³ªåˆ†å¸ƒ:")
        print(f"    é«˜å“è³ª(â‰¥0.8): {stats['quality_distribution']['high']}")
        print(f"    ä¸­å“è³ª(â‰¥0.6): {stats['quality_distribution']['medium']}")  
        print(f"    ä½å“è³ª(<0.6): {stats['quality_distribution']['low']}")
        print(f"\nâœ… è¼¸å‡ºè‡³: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="å„ªåŒ–ç‰ˆchunkså»ºæ§‹å·¥å…·")
    parser.add_argument("--input", default="datasets", help="è¼¸å…¥è³‡æ–™å¤¾")
    parser.add_argument("--output", default="indices/chunks_optimized.jsonl", help="è¼¸å‡ºæª”æ¡ˆ")
    parser.add_argument("--chunk-size", type=int, default=1200, help="chunkå¤§å°")
    parser.add_argument("--overlap", type=int, default=150, help="é‡ç–Šå¤§å°")
    parser.add_argument("--min-size", type=int, default=100, help="æœ€å°chunkå¤§å°")
    parser.add_argument("--quality-threshold", type=float, default=0.6, help="å“è³ªé–¾å€¼")
    parser.add_argument("--enable-ocr", action="store_true", help="å•Ÿç”¨OCR")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥ä¾è³´
    if args.enable_ocr and not PADDLEOCR_AVAILABLE:
        print("[ERROR] è¦ä½¿ç”¨OCRè«‹å…ˆå®‰è£: pip install paddleocr")
        sys.exit(1)
    
    # åˆå§‹åŒ–å»ºæ§‹å™¨
    builder = OptimizedChunkBuilder(
        chunk_size=args.chunk_size,
        chunk_overlap=args.overlap,
        min_chunk_size=args.min_size,
        quality_threshold=args.quality_threshold
    )
    
    # å¦‚æœæ²’å•Ÿç”¨OCRï¼Œç§»é™¤OCRå¼•æ“
    if not args.enable_ocr:
        builder.ocr = None
    
    input_dir = Path(args.input)
    output_file = Path(args.output)
    tmp_dir = output_file.parent / "_tmp_ocr"
    
    try:
        print("ğŸš€ é–‹å§‹å„ªåŒ–ç‰ˆchunkså»ºæ§‹...")
        builder.process_directory(input_dir, output_file, tmp_dir)
        print("ğŸ‰ è™•ç†å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # æ¸…ç†æš«å­˜ç›®éŒ„
        if tmp_dir.exists():
            import shutil
            try:
                shutil.rmtree(tmp_dir)
            except:
                pass


if __name__ == "__main__":
    main()