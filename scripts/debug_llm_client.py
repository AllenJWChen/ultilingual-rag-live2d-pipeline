# -*- coding: utf-8 -*-
"""
LLMå®¢æˆ¶ç«¯èª¿è©¦å’Œä¿®å¾©ç‰ˆæœ¬
è§£æ±ºJSONè§£æéŒ¯èª¤å’ŒAPIèª¿ç”¨å•é¡Œ

ä¸»è¦ä¿®å¾©ï¼š
1. å¢å¼·çš„JSONè§£æå’Œæ¸…ç†
2. æ›´å¥½çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
3. è©³ç´°çš„èª¿è©¦ä¿¡æ¯
4. å¤šç¨®LLMæœå‹™æ”¯æŒ
"""

import json
import re
import time
import requests
from typing import List, Dict, Optional, Union
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedLLMClient:
    """
    å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯
    æ”¯æŒå¤šç¨®LLMæœå‹™ï¼Œå¢å¼·éŒ¯èª¤è™•ç†
    """
    
    def __init__(self, service="ollama", base_url="http://localhost:11434", 
                 model="llama3.1:latest", debug=True):
        self.service = service
        self.base_url = base_url
        self.model = model
        self.debug = debug
        
        # APIé…ç½®
        self.config = {
            "timeout": 60,
            "max_retries": 3,
            "retry_delay": 2
        }
        
        print(f"[LLM] åˆå§‹åŒ– {service} å®¢æˆ¶ç«¯")
        print(f"      æœå‹™å™¨: {base_url}")
        print(f"      æ¨¡å‹: {model}")
        print(f"      èª¿è©¦æ¨¡å¼: {debug}")
    
    def generate_keywords(self, text: str, n: int = 3, lang: str = "en", 
                         content_type: str = "general") -> List[str]:
        """
        ç”Ÿæˆé—œéµå­— - å¢å¼·ç‰ˆ
        """
        try:
            # æ§‹å»ºæç¤ºè©
            prompt = self._build_keyword_prompt(text, n, lang, content_type)
            
            # èª¿ç”¨LLM
            raw_response = self._call_llm_with_retries(prompt)
            
            # è§£æå’Œæ¸…ç†éŸ¿æ‡‰
            keywords = self._parse_keyword_response(raw_response, n, lang)
            
            return keywords
            
        except Exception as e:
            logger.error(f"é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
            return self._generate_fallback_keywords(lang, n)
    
    def _call_llm_with_retries(self, prompt: str) -> str:
        """
        å¸¶é‡è©¦æ©Ÿåˆ¶çš„LLMèª¿ç”¨
        """
        last_error = None
        
        for attempt in range(self.config["max_retries"]):
            try:
                if self.debug:
                    logger.info(f"[å˜—è©¦ {attempt + 1}/{self.config['max_retries']}] èª¿ç”¨LLM")
                
                if self.service == "ollama":
                    response = self._call_ollama(prompt)
                elif self.service == "openai":
                    response = self._call_openai(prompt)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„LLMæœå‹™: {self.service}")
                
                if self.debug:
                    logger.info(f"[æˆåŠŸ] LLMéŸ¿æ‡‰é•·åº¦: {len(response)} å­—ç¬¦")
                    logger.debug(f"[éŸ¿æ‡‰é è¦½] {response[:200]}...")
                
                return response
                
            except Exception as e:
                last_error = e
                logger.warning(f"[å˜—è©¦ {attempt + 1}] å¤±æ•—: {e}")
                
                if attempt < self.config["max_retries"] - 1:
                    delay = self.config["retry_delay"] * (attempt + 1)
                    logger.info(f"ç­‰å¾… {delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
        
        raise last_error
    
    def _call_ollama(self, prompt: str) -> str:
        """
        èª¿ç”¨Ollama API - ä¿®å¾©ç‰ˆæœ¬
        """
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,  # é—œéµï¼šä½¿ç”¨éæµå¼æ¨¡å¼
            "options": {
                "temperature": 0.3,
                "top_p": 0.9,
                "num_predict": 200,
            }
        }
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        if self.debug:
            logger.debug(f"[Ollama] è«‹æ±‚URL: {url}")
            logger.debug(f"[Ollama] è«‹æ±‚è¼‰è·: {json.dumps(payload, indent=2)}")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                headers=headers,
                timeout=self.config["timeout"]
            )
            
            if self.debug:
                logger.debug(f"[Ollama] éŸ¿æ‡‰ç‹€æ…‹: {response.status_code}")
                logger.debug(f"[Ollama] éŸ¿æ‡‰é ­: {dict(response.headers)}")
                logger.debug(f"[Ollama] åŸå§‹éŸ¿æ‡‰: {response.text[:500]}...")
            
            if response.status_code != 200:
                raise requests.RequestException(f"HTTP {response.status_code}: {response.text}")
            
            # è§£æéŸ¿æ‡‰
            try:
                response_data = response.json()
                return response_data.get("response", "").strip()
                
            except json.JSONDecodeError as e:
                logger.error(f"[Ollama] JSONè§£æå¤±æ•—: {e}")
                logger.error(f"[Ollama] åŸå§‹éŸ¿æ‡‰: {response.text}")
                
                # å˜—è©¦å¾åŸå§‹æ–‡æœ¬ä¸­æå–
                return self._extract_from_raw_response(response.text)
        
        except requests.RequestException as e:
            raise Exception(f"ç¶²çµ¡è«‹æ±‚å¤±æ•—: {e}")
    
    def _call_openai(self, prompt: str) -> str:
        """
        èª¿ç”¨OpenAI API
        """
        try:
            import openai
        except ImportError:
            raise ImportError("è«‹å®‰è£openai: pip install openai")
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=200
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            raise Exception(f"OpenAI APIèª¿ç”¨å¤±æ•—: {e}")
    
    def _extract_from_raw_response(self, raw_text: str) -> str:
        """
        å¾åŸå§‹éŸ¿æ‡‰ä¸­æå–æœ‰ç”¨å…§å®¹
        """
        if self.debug:
            logger.info("[å˜—è©¦] å¾åŸå§‹éŸ¿æ‡‰æå–å…§å®¹")
        
        # å˜—è©¦æ‰¾åˆ°JSONéƒ¨åˆ†
        json_patterns = [
            r'\[.*?\]',  # æ•¸çµ„æ ¼å¼
            r'\{.*?\}',  # å°è±¡æ ¼å¼
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, raw_text, re.DOTALL)
            if matches:
                if self.debug:
                    logger.info(f"[æ‰¾åˆ°] JSONæ¨¡å¼: {matches[0][:100]}...")
                return matches[0]
        
        # å¦‚æœæ‰¾ä¸åˆ°JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return raw_text.strip()
    
    def _parse_keyword_response(self, response: str, n: int, lang: str) -> List[str]:
        """
        è§£æé—œéµå­—éŸ¿æ‡‰ - å¢å¼·ç‰ˆ
        """
        if not response or not response.strip():
            logger.warning("LLMéŸ¿æ‡‰ç‚ºç©º")
            return self._generate_fallback_keywords(lang, n)
        
        if self.debug:
            logger.info(f"[è§£æ] éŸ¿æ‡‰å…§å®¹: {response[:200]}...")
        
        # æ–¹æ³•1: å˜—è©¦ç›´æ¥JSONè§£æ
        try:
            # æ¸…ç†éŸ¿æ‡‰
            cleaned_response = self._clean_json_response(response)
            keywords = json.loads(cleaned_response)
            
            if isinstance(keywords, list) and all(isinstance(kw, str) for kw in keywords):
                filtered_keywords = [kw.strip() for kw in keywords if kw.strip()]
                if self.debug:
                    logger.info(f"[æˆåŠŸ] JSONè§£æ: {filtered_keywords}")
                return filtered_keywords[:n]
                
        except json.JSONDecodeError as e:
            logger.warning(f"JSONè§£æå¤±æ•—: {e}")
        
        # æ–¹æ³•2: æ­£å‰‡è¡¨é”å¼æå–
        keywords = self._extract_keywords_with_regex(response)
        if keywords:
            if self.debug:
                logger.info(f"[æˆåŠŸ] æ­£å‰‡æå–: {keywords}")
            return keywords[:n]
        
        # æ–¹æ³•3: ç°¡å–®åˆ†å‰²
        keywords = self._extract_keywords_simple(response)
        if keywords:
            if self.debug:
                logger.info(f"[æˆåŠŸ] ç°¡å–®åˆ†å‰²: {keywords}")
            return keywords[:n]
        
        # æ–¹æ³•4: å¾Œå‚™æ–¹æ¡ˆ
        logger.warning("æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™é—œéµå­—")
        return self._generate_fallback_keywords(lang, n)
    
    def _clean_json_response(self, response: str) -> str:
        """
        æ¸…ç†JSONéŸ¿æ‡‰
        """
        # ç§»é™¤markdownä»£ç¢¼å¡Š
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # ç§»é™¤å¤šé¤˜çš„æ–‡å­—
        response = response.strip()
        
        # å°‹æ‰¾JSONéƒ¨åˆ†
        json_match = re.search(r'(\[.*?\]|\{.*?\})', response, re.DOTALL)
        if json_match:
            response = json_match.group(1)
        
        # ä¿®å¾©å¸¸è¦‹çš„JSONå•é¡Œ
        response = response.replace('\n', ' ')
        response = response.replace('\t', ' ')
        response = re.sub(r'\s+', ' ', response)
        
        return response.strip()
    
    def _extract_keywords_with_regex(self, text: str) -> List[str]:
        """
        ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–é—œéµå­—
        """
        keywords = []
        
        # æ¨¡å¼1: å¼•è™ŸåŒ…åœçš„è©
        quoted_words = re.findall(r'"([^"]*)"', text)
        keywords.extend([w.strip() for w in quoted_words if w.strip()])
        
        # æ¨¡å¼2: åˆ—è¡¨é …
        list_items = re.findall(r'[-*â€¢]\s*(.+)', text)
        keywords.extend([item.strip().rstrip(',').strip('"\'') for item in list_items])
        
        # æ¨¡å¼3: ç·¨è™Ÿåˆ—è¡¨
        numbered_items = re.findall(r'\d+[.)]\s*(.+)', text)
        keywords.extend([item.strip().rstrip(',').strip('"\'') for item in numbered_items])
        
        # éæ¿¾å’Œæ¸…ç†
        cleaned_keywords = []
        for kw in keywords:
            kw = kw.strip()
            if kw and len(kw) > 1 and len(kw) < 50:
                cleaned_keywords.append(kw)
        
        return cleaned_keywords
    
    def _extract_keywords_simple(self, text: str) -> List[str]:
        """
        ç°¡å–®åˆ†å‰²æå–é—œéµå­—
        """
        # æŒ‰å¸¸è¦‹åˆ†éš”ç¬¦åˆ†å‰²
        separators = [',', 'ï¼Œ', ';', 'ï¼›', '\n', '|']
        
        for sep in separators:
            if sep in text:
                parts = text.split(sep)
                keywords = []
                
                for part in parts:
                    cleaned = part.strip().strip('"\'()[]{}')
                    if cleaned and len(cleaned) > 1 and len(cleaned) < 50:
                        keywords.append(cleaned)
                
                if len(keywords) >= 2:  # è‡³å°‘è¦æœ‰2å€‹æœ‰æ•ˆé—œéµå­—
                    return keywords
        
        return []
    
    def _generate_fallback_keywords(self, lang: str, n: int = 3) -> List[str]:
        """
        ç”Ÿæˆå¾Œå‚™é—œéµå­—
        """
        if "zh" in lang.lower():
            return [f"é—œéµè©{i+1}" for i in range(n)]
        else:
            return [f"keyword{i+1}" for i in range(n)]
    
    def _build_keyword_prompt(self, text: str, n: int, lang: str, content_type: str) -> str:
        """
        æ§‹å»ºé—œéµå­—æç¤ºè©
        """
        # æ ¹æ“šèªè¨€é¸æ“‡æç¤ºè©
        if lang.lower() in ["zh", "chinese"]:
            prompt = f"""è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡å…§å®¹æå– {n} å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚

è¦æ±‚ï¼š
1. é—œéµå­—å¿…é ˆæº–ç¢ºåæ˜ å…§å®¹
2. ä½¿ç”¨ä¸­æ–‡è©å½™
3. è¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯JSONæ•¸çµ„ï¼Œä¾‹å¦‚ï¼š["é—œéµå­—1", "é—œéµå­—2", "é—œéµå­—3"]
4. åªè¼¸å‡ºJSONï¼Œä¸è¦å…¶ä»–è§£é‡‹æ–‡å­—

å…§å®¹ï¼š
{text[:800]}

é—œéµå­—ï¼š"""

        else:
            prompt = f"""Please extract {n} most important keywords from the following English content.

Requirements:
1. Keywords must accurately reflect the content
2. Use English terms
3. Output format must be JSON array, for example: ["keyword1", "keyword2", "keyword3"]
4. Only output JSON, no other explanatory text

Content:
{text[:800]}

Keywords:"""
        
        return prompt
    
    def test_connection(self) -> bool:
        """
        æ¸¬è©¦LLMé€£æ¥
        """
        try:
            logger.info(f"[æ¸¬è©¦] é€£æ¥åˆ° {self.service} æœå‹™")
            
            test_prompt = "è«‹è¼¸å‡ºJSONæ ¼å¼: [\"test\"]"
            response = self._call_llm_with_retries(test_prompt)
            
            logger.info(f"[æ¸¬è©¦æˆåŠŸ] éŸ¿æ‡‰: {response}")
            return True
            
        except Exception as e:
            logger.error(f"[æ¸¬è©¦å¤±æ•—] {e}")
            return False


# ================== ä¿®å¾©ç‰ˆçš„é—œéµå­—ç”Ÿæˆå‡½æ•¸ ==================

def generate_keywords_fixed(text: str, n: int = 3, lang: str = "en", 
                           content_type: Optional[str] = None, 
                           llm_client: Optional[EnhancedLLMClient] = None) -> List[str]:
    """
    ä¿®å¾©ç‰ˆé—œéµå­—ç”Ÿæˆå‡½æ•¸
    """
    if llm_client is None:
        # ä½¿ç”¨é»˜èªçš„å¢å¼·å®¢æˆ¶ç«¯
        llm_client = EnhancedLLMClient(debug=True)
    
    try:
        return llm_client.generate_keywords(text, n, lang, content_type or "general")
    except Exception as e:
        logger.error(f"é—œéµå­—ç”Ÿæˆå¾¹åº•å¤±æ•—: {e}")
        # æœ€çµ‚å¾Œå‚™æ–¹æ¡ˆ
        if "zh" in lang.lower():
            return [f"å¾Œå‚™é—œéµå­—{i+1}" for i in range(n)]
        else:
            return [f"fallback_kw_{i+1}" for i in range(n)]


# ================== è¨ºæ–·å’Œæ¸¬è©¦å·¥å…· ==================

def diagnose_llm_issue():
    """
    è¨ºæ–·LLMé€£æ¥å•é¡Œ
    """
    print("ğŸ” LLMé€£æ¥è¨ºæ–·å·¥å…·")
    print("=" * 40)
    
    # æ¸¬è©¦ä¸åŒé…ç½®
    configs = [
        {"service": "ollama", "base_url": "http://localhost:11434", "model": "llama3.1:latest"},
        {"service": "ollama", "base_url": "http://localhost:11434", "model": "llama3.2:latest"}, 
        {"service": "ollama", "base_url": "http://127.0.0.1:11434", "model": "llama3.1:latest"},
    ]
    
    for i, config in enumerate(configs, 1):
        print(f"\nğŸ“‹ æ¸¬è©¦é…ç½® {i}:")
        print(f"   æœå‹™: {config['service']}")
        print(f"   åœ°å€: {config['base_url']}")  
        print(f"   æ¨¡å‹: {config['model']}")
        
        try:
            client = EnhancedLLMClient(**config, debug=True)
            
            if client.test_connection():
                print("âœ… é€£æ¥æˆåŠŸï¼")
                
                # æ¸¬è©¦é—œéµå­—ç”Ÿæˆ
                test_text = "This is a test document about artificial intelligence and machine learning."
                keywords = client.generate_keywords(test_text, 3, "en")
                print(f"ğŸ”‘ æ¸¬è©¦é—œéµå­—: {keywords}")
                
                print(f"ğŸ‰ é…ç½® {i} å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
                return client
            else:
                print("âŒ é€£æ¥å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ é…ç½® {i} å‡ºéŒ¯: {e}")
    
    print("\nâš ï¸  æ‰€æœ‰é…ç½®éƒ½å¤±æ•—äº†")
    print("å»ºè­°æª¢æŸ¥ï¼š")
    print("1. Ollamaæ˜¯å¦æ­£åœ¨é‹è¡Œ: ollama serve")
    print("2. æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰: ollama pull llama3.1:latest")
    print("3. é˜²ç«ç‰†è¨­ç½®")
    print("4. ç«¯å£æ˜¯å¦è¢«ä½”ç”¨")
    
    return None


def quick_fix_demo():
    """
    å¿«é€Ÿä¿®å¾©æ¼”ç¤º
    """
    print("ğŸ› ï¸ å¿«é€Ÿä¿®å¾©æ¼”ç¤º")
    print("=" * 30)
    
    # è¨ºæ–·å•é¡Œ
    working_client = diagnose_llm_issue()
    
    if working_client:
        print("\nğŸ¯ ä½¿ç”¨å¯ç”¨çš„å®¢æˆ¶ç«¯æ¸¬è©¦:")
        
        test_cases = [
            ("å°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„åŠå°é«”ä»£å·¥å» ", "zh"),
            ("TSMC is the world's largest semiconductor foundry", "en")
        ]
        
        for text, lang in test_cases:
            print(f"\nğŸ“ æ¸¬è©¦æ–‡æœ¬ ({lang}): {text[:50]}...")
            try:
                keywords = working_client.generate_keywords(text, 3, lang)
                print(f"âœ… ç”Ÿæˆé—œéµå­—: {keywords}")
            except Exception as e:
                print(f"âŒ å¤±æ•—: {e}")
    
    else:
        print("\nâŒ ç„¡æ³•å»ºç«‹æ­£å¸¸é€£æ¥")
        print("å»ºè­°ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼æˆ–æª¢æŸ¥Ollamaè¨­ç½®")


if __name__ == "__main__":
    quick_fix_demo()