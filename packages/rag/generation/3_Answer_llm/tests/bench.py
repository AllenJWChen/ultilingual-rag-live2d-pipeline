#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
bench.py - Answer LLM çµ±ä¸€æ€§èƒ½æ¸¬è©¦è…³æœ¬
æ”¯æ´åŸºç¤ç‰ˆæœ¬å’Œé«˜æ€§èƒ½ä¸¦è¡Œç‰ˆæœ¬æ¸¬è©¦

ç”¨æ³•ï¼š
# åŸºç¤ç‰ˆæœ¬æ¸¬è©¦
python bench.py --mode basic --max-questions 50

# ä¸¦è¡Œç‰ˆæœ¬æ¸¬è©¦ (é è¨­)
python bench.py --workers 1,8,16,32 --max-questions 200

# å®Œæ•´å£“åŠ›æ¸¬è©¦
python bench.py --workers 8,16,24,32 --max-questions 1000
"""

import os
import sys
import csv
import json
import time
import threading
import subprocess as sp
import argparse
from pathlib import Path
from typing import List, Dict, Optional

# æ¸¬è©¦çµæœè¼¸å‡ºä½ç½®
TESTS_DIR = Path(__file__).parent
RESULTS_DIR = TESTS_DIR / "results"
RESULTS_DIR.mkdir(exist_ok=True)

# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = TESTS_DIR.parents[4]

def has_nvidia_smi() -> bool:
    return shutil.which("nvidia-smi") is not None

def gpu_monitor_thread(stop_event: threading.Event, samples: list):
    """GPU ç›£æ§ç·šç¨‹"""
    if not has_nvidia_smi(): 
        return
    
    query = "timestamp,index,name,utilization.gpu,memory.used,memory.total"
    args = ["nvidia-smi", f"--query-gpu={query}", "--format=csv,noheader,nounits"]
    
    while not stop_event.is_set():
        try:
            out = sp.check_output(args, stderr=sp.DEVNULL, text=True, timeout=2)
            
            util_max = 0.0
            mem_used_max = 0.0
            mem_total_max = 0.0
            
            for line in out.strip().splitlines():
                parts = [p.strip() for p in line.split(',')]
                if len(parts) >= 6:
                    try:
                        util = float(parts[3])
                        mem_used = float(parts[4]) 
                        mem_total = float(parts[5])
                        
                        util_max = max(util_max, util)
                        mem_used_max = max(mem_used_max, mem_used)
                        mem_total_max = max(mem_total_max, mem_total)
                    except ValueError:
                        continue
            
            if util_max > 0:
                samples.append({
                    'timestamp': time.time(),
                    'gpu_util': util_max,
                    'vram_used': mem_used_max,
                    'vram_total': mem_total_max
                })
                
        except (sp.TimeoutExpired, sp.CalledProcessError):
            pass
        except KeyboardInterrupt:
            break
            
        time.sleep(1)

def run_benchmark(mode: str, workers: int, max_questions: int, 
                 questions_file: str, index_dir: str) -> Dict:
    """åŸ·è¡Œå–®æ¬¡åŸºæº–æ¸¬è©¦
    
    Args:
        mode: 'basic' æˆ– 'parallel'
        workers: ä¸¦è¡Œå·¥ä½œæ•¸ (basic æ¨¡å¼å¿½ç•¥)
        max_questions: æœ€å¤§å•é¡Œæ•¸
        questions_file: å•é¡Œæª”æ¡ˆè·¯å¾‘
        index_dir: ç´¢å¼•ç›®éŒ„è·¯å¾‘
    """
    
    if mode == 'basic':
        print(f"\nğŸš€ Testing Answer LLM (Basic Mode) - {max_questions} questions")
        module_name = "packages.rag.generation.Answer_llm.core"
        output_file = RESULTS_DIR / f"answers_basic_{max_questions}.jsonl"
        log_file = RESULTS_DIR / f"bench_basic_{max_questions}.log"
        workers_display = 1
    else:
        print(f"\nğŸš€ Testing Answer LLM (Parallel Mode) - {workers} workers, {max_questions} questions")
        module_name = "packages.rag.generation.Answer_llm.core_parallel"
        output_file = RESULTS_DIR / f"answers_w{workers}.jsonl"
        log_file = RESULTS_DIR / f"bench_w{workers}.log"
        workers_display = workers
    
    # ç’°å¢ƒè®Šæ•¸è¨­å®š
    env = os.environ.copy()
    if mode == 'parallel':
        env["OLLAMA_NUM_PARALLEL"] = str(workers)
    
    # å»ºæ§‹å‘½ä»¤
    cmd = [
        sys.executable, "-m", module_name,
        "--questions", str(questions_file),
        "--index", str(index_dir),
        "--out", str(output_file),
        "--max", str(max_questions)
    ]
    
    if mode == 'parallel':
        cmd.extend(["--workers", str(workers)])
    
    print(f"ğŸ“ Command: {' '.join(cmd)}")
    print(f"ğŸ“ Output: {output_file}")
    print(f"ğŸ“‹ Log: {log_file}")
    
    # GPU ç›£æ§
    gpu_samples = []
    gpu_stop = threading.Event()
    gpu_thread = threading.Thread(target=gpu_monitor_thread, args=(gpu_stop, gpu_samples))
    gpu_thread.start()
    
    start_time = time.time()
    
    try:
        # åŸ·è¡Œå‘½ä»¤
        with open(log_file, 'w', encoding='utf-8') as log_f:
            proc = sp.Popen(cmd, stdout=log_f, stderr=sp.STDOUT,
                          text=True, env=env, cwd=PROJECT_ROOT)
            proc.wait()
            returncode = proc.returncode
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Benchmark interrupted by user")
        proc.terminate()
        returncode = -1
    finally:
        gpu_stop.set()
        gpu_thread.join()
    
    end_time = time.time()
    total_seconds = end_time - start_time
    
    # åˆ†æçµæœ
    success_count = 0
    fail_count = 0
    
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if data.get('status') == 'failed':
                                fail_count += 1
                            else:
                                success_count += 1
                        except:
                            pass
        except Exception as e:
            print(f"âŒ Error reading output file: {e}")
    
    total_processed = success_count + fail_count
    throughput = total_processed / total_seconds if total_seconds > 0 else 0
    
    # GPU çµ±è¨ˆ
    gpu_util_avg = 0.0
    gpu_util_max = 0
    vram_max_mb = 0
    
    if gpu_samples:
        gpu_utils = [s['gpu_util'] for s in gpu_samples]
        vram_usages = [s['vram_used'] for s in gpu_samples]
        
        gpu_util_avg = sum(gpu_utils) / len(gpu_utils)
        gpu_util_max = int(max(gpu_utils))
        vram_max_mb = int(max(vram_usages)) if vram_usages else 0
    
    result = {
        'mode': mode,
        'workers': workers_display,
        'seconds': round(total_seconds, 2),
        'questions': total_processed,
        'success': success_count,
        'failed': fail_count,
        'throughput': round(throughput, 2),
        'gpu_util_avg': round(gpu_util_avg, 1),
        'gpu_util_max': gpu_util_max,
        'vram_max_mb': vram_max_mb,
        'returncode': returncode
    }
    
    print(f"â±ï¸  Time: {total_seconds:.2f}s")
    print(f"ğŸ“Š Processed: {total_processed} questions ({success_count} success, {fail_count} failed)")
    print(f"ğŸš€ Throughput: {throughput:.2f} Q/s")
    if gpu_samples:
        print(f"ğŸ® GPU Util: {gpu_util_avg:.1f}% avg, {gpu_util_max}% max")
        print(f"ğŸ’¾ VRAM: {vram_max_mb} MB max")
    
    return result

def main():
    parser = argparse.ArgumentParser(description="Answer LLM Performance Benchmark")
    
    # æ¨¡å¼é¸æ“‡
    parser.add_argument("--mode", choices=['basic', 'parallel'], default='parallel',
                       help="Test mode: basic (single-threaded) or parallel (multi-threaded)")
    
    # ä¸¦è¡Œè¨­å®š
    parser.add_argument("--workers", default="1,8,16,32",
                       help="Comma-separated worker counts for parallel mode")
    
    # æ¸¬è©¦åƒæ•¸
    parser.add_argument("--max-questions", type=int, default=200,
                       help="Maximum questions per test")
    parser.add_argument("--questions", 
                       default=str(PROJECT_ROOT / "outputs" / "data" / "questions.jsonl"),
                       help="Questions file path")
    parser.add_argument("--index",
                       default=str(PROJECT_ROOT / "indices"),
                       help="Index directory path")
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ Answer LLM Performance Benchmark")
    print(f"ğŸ”§ Mode: {args.mode.upper()}")
    print(f"â“ Max questions: {args.max_questions}")
    print(f"ğŸ“„ Questions file: {args.questions}")
    print(f"ğŸ“ Index directory: {args.index}")
    print(f"ğŸ’¾ Results will be saved in: {RESULTS_DIR}")
    
    # æª¢æŸ¥æª”æ¡ˆå­˜åœ¨
    questions_file = Path(args.questions)
    index_dir = Path(args.index)
    
    if not questions_file.exists():
        print(f"âŒ Questions file not found: {questions_file}")
        sys.exit(1)
        
    if not index_dir.exists():
        print(f"âŒ Index directory not found: {index_dir}")
        sys.exit(1)
    
    results = []
    
    if args.mode == 'basic':
        # åŸºç¤ç‰ˆæœ¬æ¸¬è©¦
        try:
            result = run_benchmark(
                mode='basic',
                workers=1,  # basic æ¨¡å¼å›ºå®šç‚º 1
                max_questions=args.max_questions,
                questions_file=str(questions_file),
                index_dir=str(index_dir)
            )
            results.append(result)
            
        except Exception as e:
            print(f"âŒ Error in basic mode test: {e}")
    
    else:
        # ä¸¦è¡Œç‰ˆæœ¬æ¸¬è©¦
        try:
            worker_counts = [int(w.strip()) for w in args.workers.split(',')]
        except ValueError:
            print("âŒ Invalid workers format. Use comma-separated integers")
            sys.exit(1)
        
        print(f"ğŸ“Š Testing with workers: {worker_counts}")
        
        for workers in worker_counts:
            try:
                result = run_benchmark(
                    mode='parallel',
                    workers=workers,
                    max_questions=args.max_questions,
                    questions_file=str(questions_file),
                    index_dir=str(index_dir)
                )
                results.append(result)
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸  Benchmark stopped by user")
                break
            except Exception as e:
                print(f"âŒ Error with {workers} workers: {e}")
                continue
    
    # å„²å­˜çµæœ
    if results:
        timestamp = int(time.time())
        csv_file = RESULTS_DIR / f"benchmark_{args.mode}_{timestamp}.csv"
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\nğŸ“Š Results saved to: {csv_file}")
        
        # é¡¯ç¤ºå½™ç¸½è¡¨æ ¼
        print(f"\nğŸ“ˆ Performance Summary ({args.mode.upper()} mode):")
        print(f"{'Workers':<8} {'Time(s)':<8} {'Questions':<10} {'Q/s':<8} {'GPU%':<6} {'VRAM(MB)':<10}")
        print("-" * 60)
        
        for r in results:
            print(f"{r['workers']:<8} {r['seconds']:<8} {r['questions']:<10} "
                  f"{r['throughput']:<8} {r['gpu_util_max']:<6} {r['vram_max_mb']:<10}")
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        if len(results) > 1:
            best_throughput = max(results, key=lambda x: x['throughput'])
            print(f"\nğŸ† Best performance: {best_throughput['workers']} workers "
                  f"({best_throughput['throughput']:.2f} Q/s)")
    
    print(f"\nâœ… All results saved in: {RESULTS_DIR}")

if __name__ == "__main__":
    import shutil  # åŠ ä¸Šé€™å€‹ import
    main()