# ================================
# ä¿®å¾©ç‰ˆ clients.py (ç„¡èªæ³•éŒ¯èª¤)
# è·¯å¾‘: packages/rag/generation/1_Keyword_llm/clients.py
# ================================

# -*- coding: utf-8 -*-
"""
å®Œæ•´ä¿®å¾©ç‰ˆèªè¨€æ„ŸçŸ¥é—œéµå­—å®¢æˆ¶ç«¯æ¨¡çµ„ (clients.py)
ğŸ”§ ä¿®å¾©èªæ³•éŒ¯èª¤ä¸¦é©é… 1_Keyword_llm è·¯å¾‘çµæ§‹
"""

import json
import re
import time
import requests
from typing import List, Dict, Optional, Union

# ä¿®å¾© import è·¯å¾‘
try:
    from .prompts import (
        build_keywords_prompt,
        build_adaptive_keywords_prompt,
        build_contextual_keywords_prompt,
        build_quality_enhanced_keywords_prompt,
        build_domain_specific_keywords_prompt,
        build_fallback_keywords_prompt
    )
except ImportError:
    # å¦‚æœç›¸å°è·¯å¾‘å¤±æ•—ï¼Œå˜—è©¦çµ•å°è·¯å¾‘
    try:
        from packages.rag.generation.keyword_llm.prompts import (
            build_keywords_prompt,
            build_adaptive_keywords_prompt,
            build_contextual_keywords_prompt,
            build_quality_enhanced_keywords_prompt,
            build_domain_specific_keywords_prompt,
            build_fallback_keywords_prompt
        )
    except ImportError:
        print("Warning: ç„¡æ³•å°å…¥ prompts æ¨¡çµ„ï¼Œä½¿ç”¨åŸºç¤æç¤ºè©")
        def build_keywords_prompt(text: str, n: int = 3, lang: str = "en") -> str:
            if lang in ["zh", "chinese"]:
                return f"è«‹å¾ä»¥ä¸‹æ–‡æœ¬ä¸­æå–{n}å€‹é—œéµå­—ï¼Œè¼¸å‡ºJSONæ ¼å¼ï¼š{text}"
            elif lang in ["ja", "japanese"]:
                return f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{n}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š{text}"
            else:
                return f"Extract {n} keywords from the following text in JSON format: {text}"


# ========== èªè¨€æ„ŸçŸ¥é…ç½® ==========

LANGUAGE_AWARE_CONFIG = {
    "chinese": {
        "temperature": 0.3,
        "max_tokens": 200,
        "timeout": 30,
        "retry_count": 3
    },
    "english": {
        "temperature": 0.2,
        "max_tokens": 150,
        "timeout": 25,
        "retry_count": 3
    },
    "japanese": {
        "temperature": 0.25,
        "max_tokens": 180,
        "timeout": 30,
        "retry_count": 3
    },
    "mixed": {
        "temperature": 0.25,
        "max_tokens": 250,
        "timeout": 35,
        "retry_count": 3
    },
    "auto": {
        "temperature": 0.3,
        "max_tokens": 200,
        "timeout": 30,
        "retry_count": 3
    }
}


# ========== æ ¸å¿ƒç”Ÿæˆå‡½æ•¸ ==========

def generate_keywords(text: str, n: int = 3, lang: str = "en", 
                     content_type: Optional[str] = None,
                     quality_score: Optional[float] = None,
                     domain: Optional[str] = None,
                     context_keywords: Optional[List[str]] = None) -> List[str]:
    """ä¿®å¾©ç‰ˆèªè¨€æ„ŸçŸ¥é—œéµå­—ç”Ÿæˆå‡½æ•¸"""
    
    if not text or not text.strip():
        return _generate_fallback_keywords(lang, n)
    
    # é¸æ“‡æç¤ºè©
    prompt = _select_optimal_prompt(
        text=text, n=n, lang=lang, content_type=content_type,
        quality_score=quality_score, domain=domain, 
        context_keywords=context_keywords
    )
    
    # ç²å–é…ç½®
    config = LANGUAGE_AWARE_CONFIG.get(lang, LANGUAGE_AWARE_CONFIG["auto"])
    
    # èªè¨€ç‰¹å®šå„ªåŒ–
    if lang in ["en", "english"]:
        config = config.copy()
        config["temperature"] = 0.1
    elif lang in ["ja", "japanese"]:
        config = config.copy()
        config["temperature"] = 0.15
    
    # åŸ·è¡ŒLLMèª¿ç”¨
    try:
        keywords = _call_llm_with_retry(
            prompt=prompt, config=config, expected_count=n, lang=lang
        )
        
        keywords = _postprocess_keywords(keywords, lang, n)
        
        if _is_valid_keywords(keywords, lang):
            return keywords
        else:
            return _generate_fallback_keywords(lang, n, "invalid_extraction")
        
    except Exception as e:
        return _generate_fallback_keywords(lang, n, str(e))


def generate_keywords_batch(chunks: List[Dict], n: int = 3, lang: str = "en") -> List[Dict]:
    """æ‰¹é‡é—œéµå­—ç”Ÿæˆ"""
    
    if not chunks:
        return []
    
    print(f"æ‰¹é‡è™•ç† {len(chunks)} å€‹chunks...")
    
    results = []
    
    for i, chunk in enumerate(chunks):
        try:
            chunk_lang = _detect_chunk_language_preference(chunk, lang)
            
            keywords = generate_keywords(
                text=chunk.get("text", ""),
                n=n,
                lang=chunk_lang
            )
            
            processing_success = _is_valid_keywords(keywords, chunk_lang)
            
            result = {
                "chunk_id": i,
                "source": chunk.get("source", "unknown"),
                "page": chunk.get("page", 0),
                "keywords": keywords,
                "detected_language": chunk_lang,
                "processing_success": processing_success
            }
            
            results.append(result)
            
        except Exception as e:
            error_result = {
                "chunk_id": i,
                "source": chunk.get("source", "unknown"),
                "page": chunk.get("page", 0),
                "keywords": _generate_fallback_keywords(lang, n, str(e)),
                "detected_language": lang,
                "processing_success": False,
                "error": str(e)
            }
            
            results.append(error_result)
    
    success_count = sum(1 for r in results if r.get("processing_success", False))
    print(f"å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ ({success_count/len(results)*100:.1f}%)")
    
    return results


# ========== è¼”åŠ©å‡½æ•¸ ==========

def _select_optimal_prompt(text: str, n: int, lang: str, **kwargs) -> str:
    """é¸æ“‡æœ€å„ªæç¤ºè©"""
    return build_keywords_prompt(text, n, lang)


def _detect_chunk_language_preference(chunk: Dict, default_lang: str) -> str:
    """æª¢æ¸¬chunkèªè¨€åå¥½"""
    
    if "main_language" in chunk:
        main_lang = chunk["main_language"]
        lang_mapping = {
            "chinese": "zh", "english": "en", "japanese": "ja",
            "mixed": "mixed", "unknown": "auto"
        }
        return lang_mapping.get(main_lang, default_lang)
    
    text = chunk.get("text", "")
    if text:
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
        total_chars = chinese_chars + english_chars + japanese_chars
        
        if total_chars > 0:
            japanese_ratio = japanese_chars / total_chars
            chinese_ratio = chinese_chars / total_chars
            english_ratio = english_chars / total_chars
            
            if japanese_ratio > 0.3:
                return "ja"
            elif chinese_ratio > 0.6:
                return "zh"
            elif english_ratio > 0.6:
                return "en"
            else:
                return "mixed"
    
    return default_lang


def _call_llm_with_retry(prompt: str, config: Dict, expected_count: int, lang: str = "en") -> List[str]:
    """å¸¶é‡è©¦çš„LLMèª¿ç”¨"""
    
    max_retries = config.get("retry_count", 3)
    timeout = config.get("timeout", 30)
    
    for attempt in range(max_retries):
        try:
            if lang in ["en", "english"]:
                response = _call_ollama_api_english_optimized(prompt, config, timeout)
            elif lang in ["ja", "japanese"]:
                response = _call_ollama_api_japanese_optimized(prompt, config, timeout)
            else:
                response = _call_ollama_api_fixed(prompt, config, timeout)
            
            return response
            
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise e


def _call_ollama_api_fixed(prompt: str, config: Dict, timeout: int) -> List[str]:
    """ä¿®å¾©ç‰ˆOllama APIèª¿ç”¨"""
    
    try:
        url = "http://localhost:11434/api/generate"
        
        payload = {
            "model": "llama3.1:latest",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": config.get("temperature", 0.3),
                "num_predict": config.get("max_tokens", 200),
                "stop": ["\n\n", "---", "###"],
                "top_k": 40,
                "top_p": 0.9,
                "repeat_penalty": 1.1
            }
        }
        
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            try:
                result = response.json()
                text_response = result.get("response", "").strip()
                keywords = _parse_llm_response_enhanced(text_response)
                return keywords if keywords else []
                    
            except json.JSONDecodeError:
                return _emergency_keyword_extract(response.text)
        else:
            raise Exception(f"APIèª¿ç”¨å¤±æ•—: {response.status_code}")
            
    except requests.RequestException as e:
        raise Exception(f"ç¶²çµ¡è«‹æ±‚å¤±æ•—: {e}")


def _call_ollama_api_english_optimized(prompt: str, config: Dict, timeout: int) -> List[str]:
    """è‹±æ–‡å„ªåŒ–ç‰ˆAPIèª¿ç”¨"""
    
    try:
        url = "http://localhost:11434/api/generate"
        
        payload = {
            "model": "llama3.1:latest",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.05,
                "num_predict": 80,
                "stop": ["\n", "---", "Note:", "Remember:", "Content:", "Text:"],
                "top_k": 10,
                "top_p": 0.7,
                "repeat_penalty": 1.3,
                "seed": 42
            }
        }
        
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            result = response.json()
            text_response = result.get("response", "").strip()
            keywords = _parse_english_keywords_enhanced(text_response)
            return keywords if keywords else []
                
        else:
            raise Exception(f"è‹±æ–‡APIèª¿ç”¨å¤±æ•—: {response.status_code}")
            
    except Exception as e:
        raise Exception(f"è‹±æ–‡è«‹æ±‚å¤±æ•—: {e}")


def _call_ollama_api_japanese_optimized(prompt: str, config: Dict, timeout: int) -> List[str]:
    """æ—¥æ–‡å„ªåŒ–ç‰ˆAPIèª¿ç”¨"""
    
    try:
        url = "http://localhost:11434/api/generate"
        
        payload = {
            "model": "llama3.1:latest",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.15,
                "num_predict": 120,
                "stop": ["\n\n", "---", "æ³¨æ„:", "è¨˜ä½:", "å…§å®¹:", "ãƒ†ã‚­ã‚¹ãƒˆ:"],
                "top_k": 25,
                "top_p": 0.8,
                "repeat_penalty": 1.15,
                "seed": 123
            }
        }
        
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            result = response.json()
            text_response = result.get("response", "").strip()
            keywords = _parse_japanese_keywords_enhanced(text_response)
            return keywords if keywords else []
                
        else:
            raise Exception(f"æ—¥æ–‡APIèª¿ç”¨å¤±æ•—: {response.status_code}")
            
    except Exception as e:
        raise Exception(f"æ—¥æ–‡è«‹æ±‚å¤±æ•—: {e}")


def _parse_llm_response_enhanced(text_response: str) -> List[str]:
    """å¢å¼·ç‰ˆLLMéŸ¿æ‡‰è§£æ"""
    
    if not text_response:
        return []
    
    # è™•ç† "Here are the" é–‹é ­çš„éŸ¿æ‡‰
    if "Here are the" in text_response and "keywords" in text_response:
        pattern = r'Here are the.*?keywords[^:]*:?\s*(.*?)(?:\n\n|$)'
        match = re.search(pattern, text_response, re.DOTALL | re.IGNORECASE)
        if match:
            keywords_text = match.group(1).strip()
            keywords = _extract_keywords_from_text(keywords_text)
            if keywords:
                return keywords
    
    # JSONé™£åˆ—è§£æ
    json_patterns = [
        r'\[(.*?)\]',
        r'keywords?\s*[:\-]\s*\[(.*?)\]',
        r'å›ç­”\s*[ï¼š:]\s*\[(.*?)\]'
    ]
    
    for pattern in json_patterns:
        matches = re.findall(pattern, text_response, re.DOTALL | re.IGNORECASE)
        for match in matches:
            try:
                if isinstance(match, tuple):
                    keywords = [kw.strip().strip('"\'') for kw in match if kw.strip()]
                else:
                    json_str = f'[{match}]'
                    keywords = json.loads(json_str)
                    keywords = [str(kw).strip().strip('"\'') for kw in keywords if kw]
                
                if keywords:
                    return keywords[:6]
            except json.JSONDecodeError:
                continue
    
    # å¼•è™Ÿè©åŒ¹é…
    quoted_patterns = [r'"([^"]+)"', r'ã€Œ([^ã€]+)ã€', r'ã€([^ã€]+)ã€', r"'([^']+)'"]
    
    quoted_words = []
    for pattern in quoted_patterns:
        matches = re.findall(pattern, text_response)
        quoted_words.extend([w.strip() for w in matches if w.strip()])
    
    if quoted_words:
        clean_words = [w for w in quoted_words if 1 < len(w) < 50]
        if clean_words:
            return clean_words[:6]
    
    # è¡Œåˆ†å‰²è§£æ
    lines = [line.strip() for line in text_response.split('\n') if line.strip()]
    potential_keywords = []
    
    for line in lines:
        if any(skip in line.lower() for skip in ['here are', 'keywords', 'extracted', 'format', 'json']):
            continue
        
        cleaned = re.sub(r'^\d+[.)]\s*', '', line)
        cleaned = re.sub(r'^[-*â€¢]\s*', '', cleaned)
        cleaned = cleaned.strip('.,;:"\'')
        
        if cleaned and 2 <= len(cleaned) <= 40:
            potential_keywords.append(cleaned)
    
    return potential_keywords[:6] if potential_keywords else []


def _parse_english_keywords_enhanced(text_response: str) -> List[str]:
    """è‹±æ–‡é—œéµå­—å°ˆç”¨è§£æå™¨"""
    
    if not text_response:
        return []
    
    # è™•ç† "Here are the extracted keywords in JSON format:" éŸ¿æ‡‰
    if "JSON format" in text_response:
        json_start = text_response.lower().find("json format")
        if json_start != -1:
            json_part = text_response[json_start + 11:].strip()
            json_match = re.search(r'\[(.*?)\]', json_part, re.DOTALL)
            if json_match:
                try:
                    json_str = f'[{json_match.group(1)}]'
                    keywords = json.loads(json_str)
                    keywords = [str(kw).strip().strip('"\'') for kw in keywords if kw]
                    if keywords:
                        # ä¿®å¾©èªæ³•éŒ¯èª¤ï¼šæ­£ç¢ºçš„å­—ç¬¦ä¸²çµæŸ
                        valid_keywords = []
                        for kw in keywords:
                            if re.match(r'^[a-zA-Z][a-zA-Z\s\-_]{1,30}$', kw):
                                valid_keywords.append(kw.title())
                        return valid_keywords[:5]
                except json.JSONDecodeError:
                    pass
    
    # è™•ç† "Here are the 4 most important keywords" éŸ¿æ‡‰
    if "most important keywords" in text_response:
        important_start = text_response.lower().find("most important keywords")
        if important_start != -1:
            content = text_response[important_start + 23:].strip()
            numbered_pattern = r'\d+\.\s*"?([^"\n]+)"?'
            numbered_matches = re.findall(numbered_pattern, content)
            if numbered_matches:
                valid_keywords = []
                for kw in numbered_matches:
                    kw = kw.strip().strip('.,;:"\'')
                    if re.match(r'^[a-zA-Z][a-zA-Z\s\-_]{1,30}$', kw):
                        valid_keywords.append(kw.title())
                if valid_keywords:
                    return valid_keywords[:5]
    
    # ä½¿ç”¨é€šç”¨è§£æ
    return _parse_llm_response_enhanced(text_response)


def _parse_japanese_keywords_enhanced(text_response: str) -> List[str]:
    """æ—¥æ–‡é—œéµå­—å°ˆç”¨è§£æå™¨"""
    
    if not text_response:
        return []
    
    # æ—¥æ–‡ç‰¹å®šæ¨¡å¼
    japanese_indicators = ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "é‡è¦ãªèª", "ä¸»è¦èª", "ç”¨èª"]
    
    has_japanese_indicator = any(indicator in text_response for indicator in japanese_indicators)
    
    if has_japanese_indicator:
        json_patterns = [
            r'\[(.*?)\]',
            r'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\s*[ï¼š:]\s*\[(.*?)\]',
            r'ç”¨èª\s*[ï¼š:]\s*\[(.*?)\]'
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text_response, re.DOTALL)
            for match in matches:
                try:
                    json_str = f'[{match}]'
                    keywords = json.loads(json_str)
                    keywords = [str(kw).strip().strip('"\'') for kw in keywords if kw]
                    if keywords:
                        valid_japanese = []
                        for kw in keywords:
                            if re.search(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', kw):
                                valid_japanese.append(kw)
                        return valid_japanese[:6]
                except json.JSONDecodeError:
                    continue
    
    # æ—¥æ–‡å¼•è™Ÿæ¨¡å¼
    japanese_quotes = [
        r'ã€Œ([^ã€]+)ã€',
        r'ã€([^ã€]+)ã€',
        r'"([^"]*[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff][^"]*)"'
    ]
    
    japanese_words = []
    for pattern in japanese_quotes:
        matches = re.findall(pattern, text_response)
        japanese_words.extend([w.strip() for w in matches if w.strip()])
    
    if japanese_words:
        valid_words = []
        for word in japanese_words:
            if 1 < len(word) < 20 and re.search(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', word):
                valid_words.append(word)
        
        if valid_words:
            return valid_words[:6]
    
    # æå–åŒ…å«æ—¥æ–‡å­—ç¬¦çš„è©å½™
    japanese_terms = re.findall(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]{2,10}', text_response)
    if japanese_terms:
        unique_terms = list(dict.fromkeys(japanese_terms))[:6]
        return unique_terms
    
    return []


def _extract_keywords_from_text(keywords_text: str) -> List[str]:
    """å¾æè¿°æ€§æ–‡æœ¬ä¸­æå–é—œéµå­—"""
    
    # JSONè§£æ
    json_match = re.search(r'\[(.*?)\]', keywords_text, re.DOTALL)
    if json_match:
        try:
            json_str = f'[{json_match.group(1)}]'
            keywords = json.loads(json_str)
            return [str(kw).strip().strip('"\'') for kw in keywords if kw]
        except json.JSONDecodeError:
            pass
    
    # ç·¨è™Ÿåˆ—è¡¨è§£æ
    numbered_pattern = r'\d+\.\s*"?([^"\n]+)"?'
    numbered_matches = re.findall(numbered_pattern, keywords_text)
    if numbered_matches:
        return [kw.strip().strip('.,;:"\'') for kw in numbered_matches]
    
    # é€—è™Ÿåˆ†éš”è§£æ
    if ',' in keywords_text:
        parts = keywords_text.split(',')
        keywords = []
        for part in parts:
            cleaned = part.strip().strip('.,;:"\'')
            if cleaned and 1 < len(cleaned) < 50:
                keywords.append(cleaned)
        if keywords:
            return keywords
    
    return []


def _emergency_keyword_extract(raw_response: str) -> List[str]:
    """æ‡‰æ€¥é—œéµå­—æå–"""
    words = re.findall(r'[a-zA-Z\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]{2,}', raw_response)
    
    stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 
                'keywords', 'extract', 'content', 'text', 'following', 'format', 'json'}
    
    filtered_words = [w for w in words if w.lower() not in stopwords and len(w) > 2]
    unique_words = list(dict.fromkeys(filtered_words))
    
    return unique_words[:3] if unique_words else []


def _is_valid_keywords(keywords: List[str], lang: str) -> bool:
    """æª¢æŸ¥é—œéµå­—æ˜¯å¦æœ‰æ•ˆ"""
    
    if not keywords:
        return False
    
    # æª¢æŸ¥å¾Œå‚™é—œéµå­—æŒ‡æ¨™
    fallback_indicators = [
        'fallback', 'emergency', 'error', 'supplementary',
        'Error', 'Emergency', 'Fallback', 'invalid',
        'Key_Concept', 'Main_Topic', 'Core_Content',
        'é—œéµæ¦‚å¿µ', 'æ ¸å¿ƒå…§å®¹', 'é‡è¦ä¿¡æ¯', 'ä¸»è¦è©±é¡Œ',
        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'é‡è¦èª', 'ä¸»è¦æ¦‚å¿µ', 'æ ¸å¿ƒå†…å®¹',
        'emergency_kw', 'fallback_kw', 'supplementary_'
    ]
    
    # å¦‚æœæ‰€æœ‰é—œéµå­—éƒ½åŒ…å«å¾Œå‚™æŒ‡ç¤ºè©ï¼Œå‰‡èªç‚ºç„¡æ•ˆ
    fallback_count = 0
    for kw in keywords:
        if any(indicator in kw for indicator in fallback_indicators):
            fallback_count += 1
    
    # å¦‚æœè¶…éä¸€åŠæ˜¯å¾Œå‚™é—œéµå­—ï¼Œèªç‚ºç„¡æ•ˆ
    if fallback_count > len(keywords) * 0.5:
        return False
    
    # èªè¨€ç‰¹å®šé©—è­‰
    if lang in ["en", "english"]:
        valid_count = sum(1 for kw in keywords if re.search(r'[a-zA-Z]', kw))
        return valid_count > 0
    elif lang in ["zh", "chinese"]:
        valid_count = sum(1 for kw in keywords if re.search(r'[\u4e00-\u9fff]', kw))
        return valid_count > 0
    elif lang in ["ja", "japanese"]:
        valid_count = sum(1 for kw in keywords if re.search(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', kw))
        return valid_count > 0
    
    return True


def _postprocess_keywords(keywords: List[str], lang: str, expected_count: int) -> List[str]:
    """é—œéµå­—å¾Œè™•ç†å’Œé©—è­‰"""
    
    if not keywords:
        return _generate_fallback_keywords(lang, expected_count)
    
    # æ¸…ç†é—œéµå­—
    cleaned = []
    for kw in keywords:
        if isinstance(kw, str):
            kw = kw.strip().strip('"\'.,;')
            # ç§»é™¤æ•¸å­—é–‹é ­çš„ç·¨è™Ÿ
            kw = re.sub(r'^\d+[.)]\s*', '', kw)
            
            # èªè¨€ç‰¹å®šé©—è­‰
            if lang in ["en", "english"]:
                # ä¿®å¾©èªæ³•éŒ¯èª¤ï¼šæ­£ç¢ºçš„å­—ç¬¦ä¸²çµæŸ
                if re.match(r'^[a-zA-Z][a-zA-Z\s\-_]{1,40}$', kw):
                    cleaned.append(kw.title())
            elif lang in ["zh", "chinese"]:
                if 1 < len(kw) < 20:
                    cleaned.append(kw)
            elif lang in ["ja", "japanese"]:
                if 1 < len(kw) < 15 and re.search(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', kw):
                    cleaned.append(kw)
            else:
                if 1 < len(kw) < 50:
                    cleaned.append(kw)
    
    # å»é‡ä½†ä¿æŒé †åº
    unique_keywords = []
    seen = set()
    for kw in cleaned:
        kw_lower = kw.lower()
        if kw_lower not in seen:
            unique_keywords.append(kw)
            seen.add(kw_lower)
    
    result = unique_keywords[:expected_count * 2]
    return result if result else []


def _generate_fallback_keywords(lang: str, count: int = 3, error_context: str = "") -> List[str]:
    """ç”Ÿæˆèªè¨€ç‰¹å®šçš„å¾Œå‚™é—œéµå­—"""
    
    if lang in ["en", "english"]:
        base_words = ["Key_Concept", "Main_Topic", "Core_Content", "Important_Info", "Relevant_Term"]
    elif lang in ["zh", "chinese"]:
        base_words = ["é—œéµæ¦‚å¿µ", "æ ¸å¿ƒå…§å®¹", "é‡è¦ä¿¡æ¯", "ä¸»è¦è©±é¡Œ", "ç›¸é—œè¡“èª"]
    elif lang in ["ja", "japanese"]:
        base_words = ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "é‡è¦èª", "ä¸»è¦æ¦‚å¿µ", "æ ¸å¿ƒå†…å®¹", "é–¢é€£ç”¨èª"]
    elif lang == "mixed":
        base_words = ["æ ¸å¿ƒæ¦‚å¿µ", "Key_Concept", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "Main_Topic", "é‡è¦å†…å®¹"]
    else:
        base_words = ["Concept", "Topic", "Content", "Information", "Term"]
    
    # æ·»åŠ éŒ¯èª¤ä¸Šä¸‹æ–‡æ¨™è­˜
    if error_context:
        if lang in ["en", "english"]:
            error_suffix = "_Error"
        elif lang in ["ja", "japanese"]:
            error_suffix = "_ã‚¨ãƒ©ãƒ¼"
        else:
            error_suffix = "_éŒ¯èª¤"
        base_words = [f"{word}{error_suffix}" for word in base_words[:count]]
    
    # ç¢ºä¿æœ‰è¶³å¤ æ•¸é‡
    while len(base_words) < count:
        base_words.extend(base_words[:count - len(base_words)])
    
    return base_words[:count]


# ========== æ¸¬è©¦å‡½æ•¸ ==========

def test_syntax_fix():
    """æ¸¬è©¦èªæ³•ä¿®å¾©æ˜¯å¦æˆåŠŸ"""
    print("æ¸¬è©¦èªæ³•ä¿®å¾©...")
    
    try:
        # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
        keywords = generate_keywords("TSMC semiconductor technology", n=3, lang="en")
        print(f"è‹±æ–‡æ¸¬è©¦çµæœ: {keywords}")
        
        keywords = generate_keywords("å°ç©é›»åŠå°é«”æŠ€è¡“", n=3, lang="zh")
        print(f"ä¸­æ–‡æ¸¬è©¦çµæœ: {keywords}")
        
        keywords = generate_keywords("TSMCã¯åŠå°ä½“æŠ€è¡“", n=3, lang="ja")
        print(f"æ—¥æ–‡æ¸¬è©¦çµæœ: {keywords}")
        
        print("èªæ³•ä¿®å¾©æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        print(f"èªæ³•ä¿®å¾©æ¸¬è©¦å¤±æ•—: {e}")
        return False


if __name__ == "__main__":
    test_syntax_fix()
