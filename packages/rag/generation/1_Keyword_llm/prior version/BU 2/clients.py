# -*- coding: utf-8 -*-
"""
èªè¨€æ„ŸçŸ¥é—œéµå­—å®¢æˆ¶ç«¯æ¨¡çµ„ (clients.py)
åŸºæ–¼åŸæœ‰ clients.py æ¶æ§‹ï¼Œå¢åŠ èªè¨€æ„ŸçŸ¥åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. èˆ‡èªè¨€æ„ŸçŸ¥çš„ prompts.py é›†æˆ
2. æ”¯æ´æ ¹æ“šèªè¨€å’Œå…§å®¹é¡å‹é¸æ“‡ä¸åŒçš„ç”Ÿæˆç­–ç•¥
3. ä¿æŒèˆ‡ç¾æœ‰ core.py çš„å…¼å®¹æ€§
4. å¢å¼·éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
"""

import json
import re
import time
import requests
from typing import List, Dict, Optional, Union
from .prompts import (
    build_keywords_prompt,
    build_adaptive_keywords_prompt,
    build_contextual_keywords_prompt,
    build_quality_enhanced_keywords_prompt,
    build_domain_specific_keywords_prompt,
    build_fallback_keywords_prompt
)


# ========== èªè¨€æ„ŸçŸ¥é…ç½® ==========

LANGUAGE_AWARE_CONFIG = {
    "chinese": {
        "model_preference": "chinese_optimized",
        "temperature": 0.3,
        "max_tokens": 200,
        "timeout": 30,
        "retry_count": 3
    },
    "english": {
        "model_preference": "english_optimized", 
        "temperature": 0.2,
        "max_tokens": 150,
        "timeout": 25,
        "retry_count": 3
    },
    "mixed": {
        "model_preference": "multilingual",
        "temperature": 0.25,
        "max_tokens": 250,
        "timeout": 35,
        "retry_count": 3
    },
    "auto": {
        "model_preference": "general",
        "temperature": 0.3,
        "max_tokens": 200,
        "timeout": 30,
        "retry_count": 3
    }
}


# ========== æ ¸å¿ƒç”Ÿæˆå‡½æ•¸ ==========

def generate_keywords(text: str, n: int = 3, lang: str = "en", 
                     content_type: Optional[str] = None,
                     quality_score: Optional[float] = None,
                     domain: Optional[str] = None,
                     context_keywords: Optional[List[str]] = None) -> List[str]:
    """
    èªè¨€æ„ŸçŸ¥çš„é—œéµå­—ç”Ÿæˆå‡½æ•¸
    
    Args:
        text: è¼¸å…¥æ–‡æœ¬
        n: é—œéµå­—æ•¸é‡
        lang: èªè¨€ ("zh", "en", "mixed", "auto")
        content_type: å…§å®¹é¡å‹ ("technical", "business", "academic", etc.)
        quality_score: æ–‡æœ¬å“è³ªåˆ†æ•¸ (0-1)
        domain: å°ˆæ¥­é ˜åŸŸ ("medical", "legal", "technology", etc.)
        context_keywords: ä¸Šä¸‹æ–‡é—œéµå­—ï¼ˆä¾†è‡ªç›¸é„°chunksï¼‰
    """
    
    if not text or not text.strip():
        return _generate_fallback_keywords(lang, n)
    
    # é¸æ“‡æœ€é©åˆçš„æç¤ºè©ç”Ÿæˆç­–ç•¥
    prompt = _select_optimal_prompt(
        text=text,
        n=n,
        lang=lang,
        content_type=content_type,
        quality_score=quality_score,
        domain=domain,
        context_keywords=context_keywords
    )
    
    # ç²å–èªè¨€ç‰¹å®šçš„é…ç½®
    config = LANGUAGE_AWARE_CONFIG.get(lang, LANGUAGE_AWARE_CONFIG["auto"])
    
    # åŸ·è¡ŒLLMèª¿ç”¨
    try:
        keywords = _call_llm_with_retry(
            prompt=prompt,
            config=config,
            expected_count=n
        )
        
        # å¾Œè™•ç†å’Œé©—è­‰
        keywords = _postprocess_keywords(keywords, lang, n)
        
        return keywords
        
    except Exception as e:
        print(f"[ERROR] é—œéµå­—ç”Ÿæˆå¤±æ•—: {e}")
        return _generate_fallback_keywords(lang, n, str(e))


def generate_keywords_batch(chunks: List[Dict], n: int = 3, lang: str = "en") -> List[Dict]:
    """
    èªè¨€æ„ŸçŸ¥çš„æ‰¹é‡é—œéµå­—ç”Ÿæˆ
    
    Args:
        chunks: chunkå­—å…¸åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« 'text' å­—æ®µ
        n: æ¯å€‹chunkçš„é—œéµå­—æ•¸é‡
        lang: åŸºç¤èªè¨€è¨­å®š
    """
    
    if not chunks:
        return []
    
    print(f"[BATCH] é–‹å§‹æ‰¹é‡è™•ç† {len(chunks)} å€‹chunks")
    
    # åˆ†æchunksçš„èªè¨€åˆ†å¸ƒ
    language_analysis = _analyze_chunks_languages(chunks)
    print(f"[BATCH] èªè¨€åˆ†å¸ƒ: {language_analysis}")
    
    results = []
    
    for i, chunk in enumerate(chunks):
        try:
            # ç‚ºæ¯å€‹chunkç²å–èªè¨€æ„ŸçŸ¥åƒæ•¸
            chunk_lang = _detect_chunk_language_preference(chunk, lang)
            chunk_content_type = chunk.get("content_type", "auto")
            chunk_quality = chunk.get("quality_score", 0.5)
            
            # ç”Ÿæˆé—œéµå­—
            keywords = generate_keywords(
                text=chunk.get("text", ""),
                n=n,
                lang=chunk_lang,
                content_type=chunk_content_type,
                quality_score=chunk_quality
            )
            
            # æ§‹å»ºçµæœè¨˜éŒ„
            result = {
                "chunk_id": i,
                "source": chunk.get("source", "unknown"),
                "page": chunk.get("page", 0),
                "keywords": keywords,
                "detected_language": chunk_lang,
                "processing_success": True
            }
            
            results.append(result)
            
        except Exception as e:
            print(f"[BATCH ERROR] Chunk {i} è™•ç†å¤±æ•—: {e}")
            
            # å‰µå»ºéŒ¯èª¤è¨˜éŒ„
            error_result = {
                "chunk_id": i,
                "source": chunk.get("source", "unknown"),
                "page": chunk.get("page", 0),
                "keywords": _generate_fallback_keywords(lang, n, str(e)),
                "detected_language": lang,
                "processing_success": False,
                "error": str(e)
            }
            
            results.append(error_result)
    
    success_count = sum(1 for r in results if r.get("processing_success", False))
    print(f"[BATCH] å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")
    
    return results


# ========== è¼”åŠ©å‡½æ•¸ ==========

def _select_optimal_prompt(text: str, n: int, lang: str, 
                          content_type: Optional[str] = None,
                          quality_score: Optional[float] = None,
                          domain: Optional[str] = None,
                          context_keywords: Optional[List[str]] = None) -> str:
    """
    æ ¹æ“šåƒæ•¸é¸æ“‡æœ€å„ªçš„æç¤ºè©ç”Ÿæˆç­–ç•¥
    """
    
    # å„ªå…ˆç´šï¼šé ˜åŸŸç‰¹å®š > ä¸Šä¸‹æ–‡æ„ŸçŸ¥ > å“è³ªå¢å¼· > å…§å®¹é¡å‹è‡ªé©æ‡‰ > åŸºç¤
    
    if domain and domain != "general":
        return build_domain_specific_keywords_prompt(text, domain, n, lang)
    
    if context_keywords:
        return build_contextual_keywords_prompt(text, context_keywords, n, lang)
    
    if quality_score is not None:
        return build_quality_enhanced_keywords_prompt(text, quality_score, n, lang)
    
    if content_type and content_type != "auto":
        return build_adaptive_keywords_prompt(text, content_type, n, lang)
    
    # é»˜èªä½¿ç”¨åŸºç¤èªè¨€æ„ŸçŸ¥æç¤ºè©
    return build_keywords_prompt(text, n, lang)


def _detect_chunk_language_preference(chunk: Dict, default_lang: str) -> str:
    """
    æª¢æ¸¬chunkçš„èªè¨€åå¥½
    """
    # å¦‚æœchunkåŒ…å«èªè¨€ä¿¡æ¯ï¼ˆä¾†è‡ªèªè¨€æ„ŸçŸ¥åˆ†å¡Šå™¨ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨
    if "main_language" in chunk:
        main_lang = chunk["main_language"]
        lang_mapping = {
            "chinese": "zh",
            "english": "en", 
            "mixed": "mixed",
            "unknown": "auto"
        }
        return lang_mapping.get(main_lang, default_lang)
    
    # å¦‚æœæœ‰keyword_languageä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
    if "keyword_language" in chunk:
        return chunk["keyword_language"]
    
    # å¦‚æœæœ‰language_statsï¼Œæ ¹æ“šæ¯”ä¾‹æ±ºå®š
    if "language_stats" in chunk:
        stats = chunk["language_stats"]
        chinese_pct = stats.get("chinese", 0)
        english_pct = stats.get("english", 0)
        
        if chinese_pct > 60:
            return "zh"
        elif english_pct > 60:
            return "en"
        elif chinese_pct > 30 and english_pct > 30:
            return "mixed"
    
    # ç°¡å–®æ–‡æœ¬åˆ†æ
    text = chunk.get("text", "")
    if text:
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = chinese_chars + english_chars
        
        if total_chars > 0:
            chinese_ratio = chinese_chars / total_chars
            if chinese_ratio > 0.6:
                return "zh"
            elif chinese_ratio < 0.4:
                return "en"
            else:
                return "mixed"
    
    return default_lang


def _analyze_chunks_languages(chunks: List[Dict]) -> Dict[str, int]:
    """
    åˆ†æchunksçš„èªè¨€åˆ†å¸ƒ
    """
    language_dist = {}
    
    for chunk in chunks:
        lang = _detect_chunk_language_preference(chunk, "unknown")
        language_dist[lang] = language_dist.get(lang, 0) + 1
    
    return language_dist


def _call_llm_with_retry(prompt: str, config: Dict, expected_count: int) -> List[str]:
    """
    å¸¶é‡è©¦æ©Ÿåˆ¶çš„LLMèª¿ç”¨
    
    æ³¨æ„ï¼šé€™è£¡éœ€è¦æ ¹æ“šä½ å¯¦éš›ä½¿ç”¨çš„LLMæœå‹™èª¿æ•´
    """
    
    max_retries = config.get("retry_count", 3)
    timeout = config.get("timeout", 30)
    
    for attempt in range(max_retries):
        try:
            # é€™è£¡éœ€è¦æ ¹æ“šä½ å¯¦éš›çš„LLMæœå‹™é€²è¡Œèª¿æ•´
            # ä¾‹å¦‚ï¼šå¦‚æœä½¿ç”¨ Ollama
            response = _call_ollama_api(prompt, config, timeout)
            
            # æˆ–è€…å¦‚æœä½¿ç”¨ OpenAI
            # response = _call_openai_api(prompt, config, timeout)
            
            # æˆ–è€…å¦‚æœä½¿ç”¨å…¶ä»–æœå‹™
            # response = _call_custom_llm_api(prompt, config, timeout)
            
            return response
            
        except Exception as e:
            print(f"[LLM] å˜—è©¦ {attempt + 1}/{max_retries} å¤±æ•—: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
            else:
                raise e


def _call_ollama_api(prompt: str, config: Dict, timeout: int) -> List[str]:
    """
    ä¿®å¾©ç‰ˆ Ollama API èª¿ç”¨
    è§£æ±º "Extra data: line 2 column 1" éŒ¯èª¤
    """
    try:
        # Ollama API é…ç½®
        url = "http://localhost:11434/api/generate"
        
        payload = {
            "model": "llama3.1:latest",  # ç¢ºä¿æ¨¡å‹åç¨±æ­£ç¢º
            "prompt": prompt,
            "stream": False,             # ğŸ”‘ é—œéµä¿®å¾©ï¼šé—œé–‰æµå¼è¼¸å‡º
            "options": {
                "temperature": config.get("temperature", 0.3),
                "num_predict": 200,      # é™åˆ¶è¼¸å‡ºé•·åº¦
                "stop": ["\n\n"]         # åœæ­¢æ¨™è¨˜
            }
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        # ç™¼é€è«‹æ±‚
        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        
        if response.status_code == 200:
            try:
                # è§£æ JSON éŸ¿æ‡‰
                result = response.json()
                text_response = result.get("response", "").strip()
                
                print(f"[DEBUG] LLMåŸå§‹éŸ¿æ‡‰: {text_response[:100]}...")  # èª¿è©¦ä¿¡æ¯
                
                # ğŸ”§ å¼·åŒ–çš„é—œéµå­—è§£æ
                keywords = _parse_llm_response(text_response)
                
                if keywords and len(keywords) >= 1:
                    return keywords
                else:
                    print("[FALLBACK] è§£æå¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™é—œéµå­—")
                    return ["fallback_keyword_1", "fallback_keyword_2", "fallback_keyword_3"]
                    
            except json.JSONDecodeError as e:
                print(f"[ERROR] JSONè§£æå¤±æ•—: {e}")
                print(f"[ERROR] åŸå§‹éŸ¿æ‡‰å…§å®¹: {response.text}")
                # å˜—è©¦å¾åŸå§‹æ–‡æœ¬ä¸­æå–
                return _emergency_keyword_extract(response.text)
        else:
            print(f"[ERROR] HTTPéŒ¯èª¤: {response.status_code}")
            raise Exception(f"APIèª¿ç”¨å¤±æ•—: {response.status_code}")
            
    except requests.RequestException as e:
        print(f"[ERROR] è«‹æ±‚å¤±æ•—: {e}")
        raise Exception(f"ç¶²çµ¡è«‹æ±‚å¤±æ•—: {e}")


def _parse_llm_response(text_response: str) -> List[str]:
    """
    è§£æ LLM éŸ¿æ‡‰ï¼Œæå–é—œéµå­—
    è™•ç†å„ç¨®å¯èƒ½çš„éŸ¿æ‡‰æ ¼å¼
    """
    if not text_response:
        return []
    
    # æ–¹æ³•1: å°‹æ‰¾ JSON æ•¸çµ„
    import re
    json_pattern = r'\[(.*?)\]'
    json_matches = re.findall(json_pattern, text_response, re.DOTALL)
    
    for json_str in json_matches:
        try:
            # é‡æ§‹ JSON å­—ç¬¦ä¸²
            json_array = f'[{json_str}]'
            keywords = json.loads(json_array)
            
            if isinstance(keywords, list):
                valid_keywords = []
                for kw in keywords:
                    if isinstance(kw, str) and kw.strip():
                        valid_keywords.append(kw.strip().strip('"\''))
                
                if valid_keywords:
                    print(f"[SUCCESS] JSONè§£ææˆåŠŸ: {valid_keywords}")
                    return valid_keywords[:5]  # æœ€å¤šè¿”å›5å€‹
                    
        except json.JSONDecodeError:
            continue
    
    # æ–¹æ³•2: å°‹æ‰¾å¼•è™ŸåŒ…åœçš„è©
    quoted_pattern = r'"([^"]+)"'
    quoted_words = re.findall(quoted_pattern, text_response)
    
    if quoted_words:
        clean_words = [w.strip() for w in quoted_words if w.strip() and len(w.strip()) > 1]
        if clean_words:
            print(f"[SUCCESS] å¼•è™Ÿè§£ææˆåŠŸ: {clean_words}")
            return clean_words[:5]
    
    # æ–¹æ³•3: æŒ‰è¡Œåˆ†å‰²ä¸¦æ¸…ç†
    lines = text_response.split('\n')
    potential_keywords = []
    
    for line in lines:
        line = line.strip()
        if line and not line.startswith('[') and not line.startswith('{'):
            # ç§»é™¤æ•¸å­—ã€æ¨™é»ç­‰
            cleaned = re.sub(r'^\d+[.)]\s*', '', line)  # ç§»é™¤ç·¨è™Ÿ
            cleaned = cleaned.strip('.,;:"\'')
            
            if cleaned and 1 < len(cleaned) < 30:
                potential_keywords.append(cleaned)
    
    if potential_keywords:
        print(f"[SUCCESS] è¡Œåˆ†å‰²è§£ææˆåŠŸ: {potential_keywords}")
        return potential_keywords[:5]
    
    print("[FAILED] æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±æ•—äº†")
    return []


def _emergency_keyword_extract(raw_response: str) -> List[str]:
    """
    æ‡‰æ€¥é—œéµå­—æå– - ç•¶æ‰€æœ‰è§£æéƒ½å¤±æ•—æ™‚çš„æœ€å¾Œæ‰‹æ®µ
    """
    print("[EMERGENCY] ä½¿ç”¨æ‡‰æ€¥é—œéµå­—æå–")
    
    # å˜—è©¦æ‰¾åˆ°ä»»ä½•çœ‹èµ·ä¾†åƒé—œéµå­—çš„å…§å®¹
    import re
    
    # æå–æ‰€æœ‰å¯èƒ½çš„è©å½™
    words = re.findall(r'[a-zA-Z\u4e00-\u9fff]{2,}', raw_response)
    
    # éæ¿¾å¸¸è¦‹è©å’Œå¤ªçŸ­çš„è©
    stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}
    
    filtered_words = [w for w in words if w.lower() not in stopwords and len(w) > 2]
    
    if filtered_words:
        return filtered_words[:3]
    else:
        return ["emergency_kw1", "emergency_kw2", "emergency_kw3"]


# ğŸ§ª æ¸¬è©¦å‡½æ•¸ - ç”¨ä¾†é©—è­‰ä¿®å¾©æ˜¯å¦ç”Ÿæ•ˆ
def test_ollama_fix():
    """
    æ¸¬è©¦ä¿®å¾©å¾Œçš„ Ollama èª¿ç”¨
    """
    print("ğŸ§ª æ¸¬è©¦ Ollama ä¿®å¾©...")
    
    test_prompt = '''è«‹ç‚ºä»¥ä¸‹å…§å®¹æå–3å€‹é—œéµå­—ï¼Œè¼¸å‡ºJSONæ ¼å¼ï¼š["é—œéµå­—1", "é—œéµå­—2", "é—œéµå­—3"]

å…§å®¹ï¼šå°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„åŠå°é«”ä»£å·¥å» ã€‚

é—œéµå­—ï¼š'''
    
    config = {"temperature": 0.3}
    
    try:
        result = _call_ollama_api(test_prompt, config, 30)
        print(f"âœ… æ¸¬è©¦æˆåŠŸï¼çµæœ: {result}")
        return True
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False


# ä½¿ç”¨æ–¹æ³•ï¼š
if __name__ == "__main__":
    test_ollama_fix()


def _call_openai_api(prompt: str, config: Dict, timeout: int) -> List[str]:
    """
    èª¿ç”¨ OpenAI APIï¼ˆç¤ºä¾‹å¯¦ç¾ï¼‰
    éœ€è¦å®‰è£: pip install openai
    """
    try:
        import openai
        
        # è¨­ç½® OpenAI API keyï¼ˆéœ€è¦å¾ç’°å¢ƒè®Šé‡æˆ–é…ç½®ä¸­ç²å–ï¼‰
        openai.api_key = os.getenv("OPENAI_API_KEY")
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # æˆ–å…¶ä»–æ¨¡å‹
            messages=[{"role": "user", "content": prompt}],
            temperature=config.get("temperature", 0.3),
            max_tokens=config.get("max_tokens", 200),
            timeout=timeout
        )
        
        text_response = response.choices[0].message.content.strip()
        
        # è§£æéŸ¿æ‡‰
        try:
            keywords = json.loads(text_response)
            if isinstance(keywords, list):
                return [str(kw).strip() for kw in keywords if kw]
        except json.JSONDecodeError:
            return _parse_non_json_response(text_response)
            
    except Exception as e:
        raise Exception(f"OpenAI APIèª¿ç”¨å¤±æ•—: {e}")


def _parse_non_json_response(text: str) -> List[str]:
    """
    è§£æéJSONæ ¼å¼çš„éŸ¿æ‡‰
    """
    # å˜—è©¦å„ç¨®æ ¼å¼çš„è§£æ
    keywords = []
    
    # æ–¹æ³•1: æŸ¥æ‰¾å¼•è™ŸåŒ…åœçš„è©
    quoted_words = re.findall(r'"([^"]*)"', text)
    if quoted_words:
        keywords.extend(quoted_words)
    
    # æ–¹æ³•2: æŸ¥æ‰¾åˆ—è¡¨æ ¼å¼
    list_items = re.findall(r'[-*â€¢]\s*(.+)', text)
    if list_items:
        keywords.extend([item.strip().strip('"\'') for item in list_items])
    
    # æ–¹æ³•3: æŒ‰é€—è™Ÿåˆ†å‰²
    if not keywords:
        comma_separated = [item.strip().strip('"\'') for item in text.split(',')]
        keywords.extend(comma_separated)
    
    # æ¸…ç†å’Œéæ¿¾
    cleaned_keywords = []
    for kw in keywords:
        kw = kw.strip()
        if kw and len(kw) > 1 and len(kw) < 50:
            cleaned_keywords.append(kw)
    
    return cleaned_keywords[:6]  # æœ€å¤šè¿”å›6å€‹


def _postprocess_keywords(keywords: List[str], lang: str, expected_count: int) -> List[str]:
    """
    é—œéµå­—å¾Œè™•ç†å’Œé©—è­‰
    """
    if not keywords:
        return _generate_fallback_keywords(lang, expected_count)
    
    # æ¸…ç†é—œéµå­—
    cleaned = []
    for kw in keywords:
        if isinstance(kw, str):
            kw = kw.strip().strip('"\'.,;')
            # ç§»é™¤æ•¸å­—é–‹é ­çš„ç·¨è™Ÿ
            kw = re.sub(r'^\d+[.)]\s*', '', kw)
            # ç§»é™¤éçŸ­æˆ–éé•·çš„é—œéµå­—
            if 1 < len(kw) < 50:
                cleaned.append(kw)
    
    # å»é‡ä½†ä¿æŒé †åº
    unique_keywords = []
    seen = set()
    for kw in cleaned:
        kw_lower = kw.lower()
        if kw_lower not in seen:
            unique_keywords.append(kw)
            seen.add(kw_lower)
    
    # ç¢ºä¿æ•¸é‡è¶³å¤ 
    if len(unique_keywords) < expected_count:
        fallback = _generate_fallback_keywords(lang, expected_count - len(unique_keywords))
        unique_keywords.extend(fallback)
    
    return unique_keywords[:expected_count * 2]  # å…è¨±ç¨å¾®è¶…éæœŸæœ›æ•¸é‡


def _generate_fallback_keywords(lang: str, count: int = 3, error_context: str = "") -> List[str]:
    """
    ç”Ÿæˆèªè¨€ç‰¹å®šçš„å¾Œå‚™é—œéµå­—
    """
    if lang in ["zh", "chinese"]:
        base_words = ["é—œéµæ¦‚å¿µ", "æ ¸å¿ƒå…§å®¹", "é‡è¦ä¿¡æ¯", "ä¸»è¦è©±é¡Œ", "ç›¸é—œè¡“èª"]
    elif lang in ["en", "english"]:
        base_words = ["key_concept", "core_content", "main_topic", "important_info", "relevant_term"]
    elif lang == "mixed":
        base_words = ["æ ¸å¿ƒæ¦‚å¿µ", "key_concept", "é‡è¦å…§å®¹", "main_topic", "ç›¸é—œä¿¡æ¯"]
    else:
        base_words = ["concept", "topic", "content", "information", "term"]
    
    # æ·»åŠ éŒ¯èª¤ä¸Šä¸‹æ–‡æ¨™è­˜ï¼ˆå¦‚æœæœ‰ï¼‰
    if error_context:
        error_suffix = "_error" if lang == "en" else "_éŒ¯èª¤"
        base_words = [f"{word}{error_suffix}" for word in base_words[:count]]
    
    # ç¢ºä¿æœ‰è¶³å¤ æ•¸é‡
    while len(base_words) < count:
        base_words.extend(base_words[:count - len(base_words)])
    
    return base_words[:count]


# ========== æ¸¬è©¦å’Œèª¿è©¦å‡½æ•¸ ==========

def test_language_detection():
    """
    æ¸¬è©¦èªè¨€æª¢æ¸¬åŠŸèƒ½
    """
    test_chunks = [
        {"text": "é€™æ˜¯ä¸€å€‹ä¸­æ–‡æ¸¬è©¦æ–‡æª”ï¼ŒåŒ…å«æŠ€è¡“å…§å®¹ã€‚"},
        {"text": "This is an English test document with technical content."},
        {"text": "é€™æ˜¯mixedå…§å®¹ï¼ŒåŒ…å«Chineseå’ŒEnglish textã€‚"},
        {"text": "TSMCå°ç©é›»ä½¿ç”¨advanced manufacturing technologyå…ˆé€²è£½é€ æŠ€è¡“ã€‚"}
    ]
    
    print("=== èªè¨€æª¢æ¸¬æ¸¬è©¦ ===")
    for i, chunk in enumerate(test_chunks):
        detected = _detect_chunk_language_preference(chunk, "auto")
        print(f"Chunk {i+1}: '{chunk['text'][:30]}...' -> {detected}")


def test_keyword_generation():
    """
    æ¸¬è©¦é—œéµå­—ç”ŸæˆåŠŸèƒ½
    """
    test_cases = [
        {
            "text": "å°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„åŠå°é«”ä»£å·¥å» ï¼Œæ¡ç”¨å…ˆé€²çš„7å¥ˆç±³è£½ç¨‹æŠ€è¡“ã€‚",
            "lang": "zh",
            "expected": ["å°ç©é›»", "åŠå°é«”", "7å¥ˆç±³"]
        },
        {
            "text": "TSMC is the world's largest semiconductor foundry using advanced 7nm technology.",
            "lang": "en", 
            "expected": ["TSMC", "semiconductor", "7nm"]
        }
    ]
    
    print("\n=== é—œéµå­—ç”Ÿæˆæ¸¬è©¦ ===")
    for i, case in enumerate(test_cases):
        try:
            keywords = generate_keywords(
                text=case["text"],
                n=3,
                lang=case["lang"]
            )
            print(f"Test {i+1} ({case['lang']}): {keywords}")
        except Exception as e:
            print(f"Test {i+1} å¤±æ•—: {e}")


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    test_language_detection()
    test_keyword_generation()