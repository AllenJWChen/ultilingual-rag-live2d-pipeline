# -*- coding: utf-8 -*-
"""
èªè¨€æ„ŸçŸ¥é—œéµå­—æç¤ºè©æ¨¡çµ„ (prompts.py)
åŸºæ–¼åŸæœ‰ prompts.py æ¶æ§‹ï¼Œå¢åŠ èªè¨€æ„ŸçŸ¥åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. æ ¹æ“šæª¢æ¸¬åˆ°çš„èªè¨€é¸æ“‡åˆé©çš„æç¤ºè©
2. æ”¯æ´ä¸­æ–‡ã€è‹±æ–‡ã€æ··åˆèªè¨€çš„é—œéµå­—ç”Ÿæˆ
3. é‡å°ä¸åŒå…§å®¹é¡å‹å„ªåŒ–æç¤ºç­–ç•¥
4. ä¿æŒèˆ‡ç¾æœ‰ clients.py çš„å…¼å®¹æ€§
"""

import re
from typing import List, Optional


def build_keywords_prompt(text: str, n: int = 3, lang: str = "en") -> str:
    """
    æ”¹é€²ç‰ˆé—œéµå­—ç”Ÿæˆæç¤ºè© - æ”¯æ´èªè¨€æ„ŸçŸ¥
    
    Args:
        text: è¦åˆ†æçš„æ–‡å­—
        n: éœ€è¦çš„é—œéµå­—æ•¸é‡
        lang: ç›®æ¨™èªè¨€ ("zh", "en", "mixed", "auto")
    """
    
    # æ ¹æ“šlangåƒæ•¸é¸æ“‡åˆé©çš„æç¤ºè©
    if lang.lower() in ["zh", "chinese"]:
        return build_chinese_keywords_prompt(text, n)
    elif lang.lower() in ["en", "english"]:
        return build_english_keywords_prompt(text, n)
    elif lang.lower() in ["mixed", "bilingual"]:
        return build_mixed_keywords_prompt(text, n)
    else:
        # autoæ¨¡å¼ï¼šæ ¹æ“šæ–‡æœ¬å…§å®¹è‡ªå‹•é¸æ“‡
        return build_auto_keywords_prompt(text, n)


def build_chinese_keywords_prompt(text: str, n: int = 3) -> str:
    """
    ä¸­æ–‡é—œéµå­—ç”Ÿæˆæç¤ºè©
    """
    return f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¸­æ–‡é—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡å…§å®¹æå– {n} å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚

âš ï¸ åš´æ ¼è¦æ±‚ï¼š
1. é—œéµå­—å¿…é ˆ**ç›´æ¥å‡ºç¾**åœ¨æ–‡æœ¬ä¸­æˆ–èˆ‡å…§å®¹**å¯†åˆ‡ç›¸é—œ**
2. ä½¿ç”¨æº–ç¢ºçš„ä¸­æ–‡è©å½™ï¼ˆ1-4å€‹å­—ç‚ºä½³ï¼‰
3. å„ªå…ˆç´šï¼šå°ˆæœ‰åè© > æŠ€è¡“è¡“èª > æ ¸å¿ƒæ¦‚å¿µ
4. **çµ•å°ç¦æ­¢**ï¼šä¸ç›¸é—œçš„è©å½™ï¼ˆä¾‹å¦‚ï¼šå…§å®¹è¬›Darwinï¼Œä¸è¦ç”Ÿæˆ"åŠå°é«”"ï¼‰
5. **çµ•å°ç¦æ­¢**ï¼šéæ–¼æ³›ç”¨çš„è©ï¼ˆå¦‚ï¼šæŠ€è¡“ã€ç™¼å±•ã€å¸‚å ´ã€ç³»çµ±ã€ç”¢å“ï¼‰
6. **çµ•å°ç¦æ­¢**ï¼šå¹´ä»½ã€æ—¥æœŸæˆ–æ™‚é–“ç›¸é—œè©å½™
7. è¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯JSONæ•¸çµ„ï¼š["è©1", "è©2", "è©3"]

âœ… è‰¯å¥½ç¤ºä¾‹ï¼š
- å…§å®¹æåŠ"å°ç©é›»" â†’ é—œéµå­—åŒ…å«["å°ç©é›»"]
- å…§å®¹æåŠ"ç‰©ç¨®æ¼”åŒ–" â†’ é—œéµå­—åŒ…å«["æ¼”åŒ–", "é”çˆ¾æ–‡"]
- å…§å®¹æåŠ"åŠå°é«”ç”¢æ¥­" â†’ é—œéµå­—åŒ…å«["åŠå°é«”"]

âŒ éŒ¯èª¤ç¤ºä¾‹ï¼š
- å…§å®¹è¬›é”çˆ¾æ–‡ â†’ éŒ¯èª¤ç”Ÿæˆ["åŠå°é«”"]ï¼ˆå®Œå…¨ç„¡é—œï¼‰
- å…§å®¹è¬›ç”Ÿç‰©å­¸ â†’ éŒ¯èª¤ç”Ÿæˆ["äººå·¥æ™ºæ…§", "æŠ€è¡“"]ï¼ˆä¸ç›¸é—œï¼‰

æ–‡æœ¬å…§å®¹ï¼š
{text}

è«‹ä»”ç´°é–±è®€ä¸Šè¿°å…§å®¹ï¼Œæå–**ç¢ºå¯¦å‡ºç¾**åœ¨æ–‡æœ¬ä¸­çš„é—œéµå­—ï¼š"""


def build_english_keywords_prompt(text: str, n: int = 3) -> str:
    """
    è‹±æ–‡é—œéµå­—ç”Ÿæˆæç¤ºè©
    """
    return f"""You are a professional English keyword extraction expert. Please extract {n} most important keywords from the following English content.

âš ï¸ STRICT REQUIREMENTS:
1. Keywords MUST be **directly present** or **clearly related** to the actual content
2. Use precise English terms (1-4 words max)
3. Priority: Specific proper nouns > Technical terms > Topic concepts  
4. **ABSOLUTELY FORBIDDEN**: Irrelevant terms (e.g., if content is about Darwin, DON'T generate "semiconductor")
5. **ABSOLUTELY FORBIDDEN**: Generic words (technology, development, market, system, product)
6. **ABSOLUTELY FORBIDDEN**: Years, dates, or temporal references
7. Output ONLY JSON format: ["term1", "term2", "term3"]

âœ… GOOD Examples:
- Content mentions "TSMC" â†’ keywords include ["TSMC"]  
- Content mentions "species evolution" â†’ keywords include ["evolution", "Darwin"]
- Content mentions "semiconductor industry" â†’ keywords include ["semiconductor"]

âŒ BAD Examples:
- Content about Darwin â†’ WRONG to generate ["semiconductor"] (completely unrelated)
- Content about biology â†’ WRONG to generate ["AI", "technology"] (not relevant)

Text content:
{text}

Carefully read the content above and extract keywords that **actually appear** in the text:"""


def build_mixed_keywords_prompt(text: str, n: int = 3) -> str:
    """
    ä¸­è‹±æ··åˆå…§å®¹é—œéµå­—ç”Ÿæˆæç¤ºè©
    """
    return f"""ä½ æ˜¯å°ˆæ¥­çš„é›™èªé—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ä¸­è‹±æ–‡æ··åˆå…§å®¹æå– {n} å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚
You are a professional bilingual keyword extraction expert. Please extract {n} most important keywords from the following mixed Chinese-English content.

âš ï¸ åš´æ ¼è¦æ±‚ / STRICT REQUIREMENTS:
1. é—œéµå­—å¿…é ˆ**ç›´æ¥å‡ºç¾**åœ¨æ–‡æœ¬ä¸­æˆ–èˆ‡å…§å®¹**å¯†åˆ‡ç›¸é—œ** / Keywords MUST be **directly present** or **clearly related** to actual content
2. å¯ä»¥æ˜¯ä¸­æ–‡è©å½™æˆ–è‹±æ–‡è©å½™ï¼Œé¸æ“‡æœ€æº–ç¢ºçš„èªè¨€ / Use Chinese or English terms, whichever is more precise
3. å„ªå…ˆç´šï¼šå°ˆæœ‰åè© > æŠ€è¡“è¡“èª > æ ¸å¿ƒæ¦‚å¿µ / Priority: Proper nouns > Technical terms > Core concepts
4. **çµ•å°ç¦æ­¢**ä¸ç›¸é—œè©å½™ / **ABSOLUTELY FORBIDDEN**: Irrelevant terms
5. **çµ•å°ç¦æ­¢**éæ–¼æ³›ç”¨çš„è© / **ABSOLUTELY FORBIDDEN**: Generic words
6. è¼¸å‡ºæ ¼å¼ï¼šJSONæ•¸çµ„ / Output format: JSON array ["è©1/term1", "è©2/term2", "è©3/term3"]

âœ… è‰¯å¥½ç¤ºä¾‹ / GOOD Examples:
- å…§å®¹åŒ…å«"TSMCå°ç©é›»" â†’ ["TSMC", "å°ç©é›»", "åŠå°é«”"]
- Content contains "äººå·¥æ™ºæ…§AI" â†’ ["äººå·¥æ™ºæ…§", "AI", "machine learning"]

âŒ éŒ¯èª¤ç¤ºä¾‹ / BAD Examples:  
- å…§å®¹è¬›åŠå°é«”ï¼Œç”Ÿæˆ["ç”Ÿç‰©å­¸"] / Content about semiconductors, generating ["biology"]

æ–‡æœ¬å…§å®¹ / Text content:
{text}

è«‹æå–**ç¢ºå¯¦ç›¸é—œ**çš„é—œéµå­— / Extract **actually relevant** keywords:"""


def build_auto_keywords_prompt(text: str, n: int = 3) -> str:
    """
    è‡ªå‹•æª¢æ¸¬èªè¨€ä¸¦ç”Ÿæˆé—œéµå­—çš„æç¤ºè©
    """
    # æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_chars = chinese_chars + english_chars
    
    if total_chars == 0:
        # ç„¡æ³•æª¢æ¸¬ï¼Œä½¿ç”¨è‹±æ–‡æç¤ºè©
        return build_english_keywords_prompt(text, n)
    
    chinese_ratio = chinese_chars / total_chars
    english_ratio = english_chars / total_chars
    
    if chinese_ratio > 0.6:
        return build_chinese_keywords_prompt(text, n)
    elif english_ratio > 0.6:
        return build_english_keywords_prompt(text, n)
    else:
        return build_mixed_keywords_prompt(text, n)


def build_contextual_keywords_prompt(text: str, context_keywords: List[str] = None, 
                                   n: int = 3, lang: str = "en") -> str:
    """
    ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—œéµå­—ç”Ÿæˆ - è€ƒæ…®å·²æœ‰é—œéµå­—çš„æƒ…æ³ä¸‹ç”Ÿæˆäº’è£œé—œéµå­—
    
    Args:
        text: è¦åˆ†æçš„æ–‡å­—
        context_keywords: å·²æœ‰çš„é—œéµå­—åˆ—è¡¨ï¼ˆä¾†è‡ªç›¸é„°chunksæˆ–ç›¸åŒæ–‡ä»¶ï¼‰
        n: éœ€è¦çš„é—œéµå­—æ•¸é‡
        lang: èªè¨€
    """
    
    context_info = ""
    if context_keywords:
        context_list = ", ".join(context_keywords[:10])  # é¿å…promptéé•·
        if lang.lower() in ["zh", "chinese"]:
            context_info = f"\nå·²çŸ¥ç›¸é—œé—œéµå­—ï¼š{context_list}\nè«‹ç”Ÿæˆ**äº’è£œä¸”ä¸é‡è¤‡**çš„é—œéµå­—ã€‚\n"
        else:
            context_info = f"\nExisting related keywords: {context_list}\nGenerate **complementary and non-duplicate** keywords.\n"
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # åœ¨è¦æ±‚éƒ¨åˆ†æ’å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯
    if lang.lower() in ["zh", "chinese"]:
        context_section = f"{context_info}ç‰¹åˆ¥æ³¨æ„ï¼šèˆ‡å·²çŸ¥é—œéµå­—å½¢æˆä¸»é¡Œå®Œæ•´æ€§ï¼Œä½†ä¸è¦é‡è¤‡ã€‚"
    else:
        context_section = f"{context_info}Focus on: Terms that complement existing keywords but provide new information."
    
    return base_prompt.replace("è¦æ±‚ï¼š", f"è¦æ±‚ï¼š\n{context_section}\nåŸè¦æ±‚ï¼š") if "è¦æ±‚ï¼š" in base_prompt else \
           base_prompt.replace("Requirements:", f"Requirements:\n{context_section}\nOriginal requirements:")


def build_adaptive_keywords_prompt(text: str, content_type: str = "auto", 
                                 n: int = 3, lang: str = "en") -> str:
    """
    è‡ªé©æ‡‰é—œéµå­—ç”Ÿæˆ - æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´ç­–ç•¥
    
    Args:
        content_type: "technical", "business", "academic", "auto"
    """
    
    # è‡ªå‹•æª¢æ¸¬å…§å®¹é¡å‹
    if content_type == "auto":
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in ["patent", "algorithm", "specification", "protocol", "display", "semiconductor"]):
            content_type = "technical"
        elif any(indicator in text_lower for indicator in ["market", "revenue", "business", "strategy", "company", "investment"]):
            content_type = "business"  
        elif any(indicator in text_lower for indicator in ["research", "study", "analysis", "methodology", "experiment", "è«–æ–‡", "ç ”ç©¶"]):
            content_type = "academic"
        else:
            content_type = "general"
    
    # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´æç¤ºè©
    specialized_instructions = {
        "technical": {
            "zh": "å°ˆæ³¨æ–¼æŠ€è¡“è¦æ ¼ã€ç”¢å“å‹è™Ÿã€è£½ç¨‹åƒæ•¸ã€å”è­°æ¨™æº–ã€å°ˆåˆ©æŠ€è¡“",
            "en": "Focus on technical specs, product models, process parameters, protocol standards, patent technologies"
        },
        "business": {
            "zh": "å°ˆæ³¨æ–¼å…¬å¸åç¨±ã€å•†æ¥­æ¨¡å¼ã€å¸‚å ´ç­–ç•¥ã€ç”¢æ¥­è¶¨å‹¢ã€æŠ•è³‡ç›¸é—œ",
            "en": "Focus on company names, business models, market strategies, industry trends, investment aspects"
        },
        "academic": {
            "zh": "å°ˆæ³¨æ–¼ç ”ç©¶æ–¹æ³•ã€ç†è«–æ¦‚å¿µã€å¯¦é©—çµæœã€å­¸è¡“è¡“èªã€ä½œè€…å§“å",
            "en": "Focus on research methods, theoretical concepts, experimental results, academic terms, author names"
        },
        "general": {
            "zh": "å°ˆæ³¨æ–¼æ ¸å¿ƒä¸»é¡Œã€é—œéµæ¦‚å¿µã€é‡è¦å¯¦é«”ã€ä¸»è¦äººç‰©",
            "en": "Focus on core topics, key concepts, important entities, main figures"
        }
    }
    
    lang_key = "zh" if lang.lower() in ["zh", "chinese"] else "en"
    additional_instruction = specialized_instructions[content_type][lang_key]
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # æ’å…¥å°ˆé–€åŒ–æŒ‡ä»¤
    if lang.lower() in ["zh", "chinese"]:
        specialized_section = f"å…§å®¹é¡å‹ï¼š{content_type.upper()}\nç‰¹åŒ–è¦æ±‚ï¼š{additional_instruction}\n\n"
        return base_prompt.replace("ä½ æ˜¯ä¸€å€‹", f"{specialized_section}ä½ æ˜¯ä¸€å€‹")
    else:
        specialized_section = f"Content type: {content_type.upper()}\nSpecialized focus: {additional_instruction}\n\n"
        return base_prompt.replace("You are a", f"{specialized_section}You are a")


def build_quality_enhanced_keywords_prompt(text: str, quality_score: float = 0.5, 
                                         n: int = 3, lang: str = "en") -> str:
    """
    åŸºæ–¼å“è³ªåˆ†æ•¸çš„å¢å¼·æç¤ºè©
    
    Args:
        quality_score: chunkå“è³ªåˆ†æ•¸ (0-1)
    """
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    if quality_score > 0.8:
        # é«˜å“è³ªchunkï¼Œè¦æ±‚æ›´å¤šé—œéµå­—å’Œæ›´é«˜ç²¾åº¦
        if lang.lower() in ["zh", "chinese"]:
            enhancement = "\nâ­ é«˜å“è³ªå…§å®¹æª¢æ¸¬ï¼šè«‹æå–æ›´å¤šå°ˆæ¥­è¡“èªå’Œæ ¸å¿ƒæ¦‚å¿µï¼Œç¢ºä¿é—œéµå­—çš„å°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§ã€‚"
        else:
            enhancement = "\nâ­ High-quality content detected: Extract more professional terms and core concepts, ensure keyword precision and accuracy."
        
        # å¢åŠ é—œéµå­—æ•¸é‡
        n_enhanced = min(n + 1, 6)
        base_prompt = base_prompt.replace(f"{n} å€‹", f"{n_enhanced} å€‹").replace(f"extract {n}", f"extract {n_enhanced}")
        
    elif quality_score < 0.6:
        # ä½å“è³ªchunkï¼Œå°ˆæ³¨æ–¼åŸºæœ¬æ¦‚å¿µ
        if lang.lower() in ["zh", "chinese"]:
            enhancement = "\nâš ï¸ å…§å®¹å“è³ªè¼ƒä½ï¼šè«‹å°ˆæ³¨æ–¼æœ€åŸºæœ¬å’Œæœ€æ˜ç¢ºçš„æ¦‚å¿µï¼Œé¿å…è¤‡é›œæˆ–æ¨¡ç³Šçš„è¡“èªã€‚"
        else:
            enhancement = "\nâš ï¸ Lower quality content: Focus on most basic and clear concepts, avoid complex or ambiguous terms."
    else:
        enhancement = ""
    
    return base_prompt + enhancement


def build_domain_specific_keywords_prompt(text: str, domain: str = "general", 
                                        n: int = 3, lang: str = "en") -> str:
    """
    é ˜åŸŸç‰¹å®šçš„é—œéµå­—æç¤ºè©
    
    Args:
        domain: "medical", "legal", "technology", "finance", "academic", "general"
    """
    
    domain_instructions = {
        "medical": {
            "zh": "é†«å­¸é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼ç–¾ç—…åç¨±ã€è—¥ç‰©åç¨±ã€è§£å‰–è¡“èªã€æ²»ç™‚æ–¹æ³•ã€é†«å­¸æ¦‚å¿µ",
            "en": "Medical domain: Focus on disease names, drug names, anatomical terms, treatment methods, medical concepts"
        },
        "legal": {
            "zh": "æ³•å¾‹é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼æ³•å¾‹æ¢æ–‡ã€æ¡ˆä¾‹åç¨±ã€æ³•å¾‹æ¦‚å¿µã€ç¨‹åºè¡“èªã€æ³•é™¢åˆ¤æ±º",
            "en": "Legal domain: Focus on legal provisions, case names, legal concepts, procedural terms, court decisions"
        },
        "technology": {
            "zh": "ç§‘æŠ€é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼æŠ€è¡“åç¨±ã€ç”¢å“å‹è™Ÿã€æŠ€è¡“æ¨™æº–ã€å…¬å¸åç¨±ã€å‰µæ–°æ¦‚å¿µ",
            "en": "Technology domain: Focus on technical names, product models, technical standards, company names, innovation concepts"
        },
        "finance": {
            "zh": "é‡‘èé ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼é‡‘èå·¥å…·ã€å¸‚å ´è¡“èªã€å…¬å¸åç¨±ã€æŠ•è³‡æ¦‚å¿µã€ç¶“æ¿ŸæŒ‡æ¨™",
            "en": "Finance domain: Focus on financial instruments, market terms, company names, investment concepts, economic indicators"
        },
        "academic": {
            "zh": "å­¸è¡“é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼ç ”ç©¶ä¸»é¡Œã€ä½œè€…å§“åã€ç†è«–åç¨±ã€æ–¹æ³•è«–ã€å¯¦é©—çµæœ",
            "en": "Academic domain: Focus on research topics, author names, theory names, methodologies, experimental results"
        },
        "general": {
            "zh": "é€šç”¨é ˜åŸŸï¼šå°ˆæ³¨æ–¼æ ¸å¿ƒæ¦‚å¿µã€é‡è¦å¯¦é«”ã€é—œéµè¡“èª",
            "en": "General domain: Focus on core concepts, important entities, key terms"
        }
    }
    
    lang_key = "zh" if lang.lower() in ["zh", "chinese"] else "en"
    domain_instruction = domain_instructions.get(domain, domain_instructions["general"])[lang_key]
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # æ’å…¥é ˜åŸŸç‰¹å®šæŒ‡ä»¤
    if lang.lower() in ["zh", "chinese"]:
        domain_section = f"ğŸ¯ {domain_instruction}\n\n"
        return base_prompt.replace("ä½ æ˜¯ä¸€å€‹", f"{domain_section}ä½ æ˜¯ä¸€å€‹")
    else:
        domain_section = f"ğŸ¯ {domain_instruction}\n\n"
        return base_prompt.replace("You are a", f"{domain_section}You are a")


# ========== æ‰¹é‡è™•ç†ç›¸é—œæç¤ºè© ==========

def build_batch_keywords_prompt(chunks: List[dict], n: int = 3, lang: str = "en") -> str:
    """
    æ‰¹é‡é—œéµå­—ç”Ÿæˆçš„æç¤ºè©
    å°ˆç‚ºæ‰¹é‡è™•ç†è¨­è¨ˆï¼Œè€ƒæ…®chunksé–“çš„é—œè¯æ€§
    """
    
    if lang.lower() in ["zh", "chinese"]:
        prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ‰¹é‡é—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ {len(chunks)} å€‹æ–‡æœ¬ç‰‡æ®µåˆ†åˆ¥ç”Ÿæˆ {n} å€‹é—œéµå­—ã€‚

âš ï¸ æ‰¹é‡è™•ç†è¦æ±‚ï¼š
1. æ¯å€‹ç‰‡æ®µçš„é—œéµå­—å¿…é ˆ**ç¨ç«‹ä¸”ç›¸é—œ**
2. è€ƒæ…®ç‰‡æ®µé–“çš„ä¸»é¡Œé€£è²«æ€§ï¼Œä½†é¿å…é‡è¤‡
3. ä½¿ç”¨æº–ç¢ºçš„ä¸­æ–‡è©å½™
4. è¼¸å‡ºæ ¼å¼ï¼š[["ç‰‡æ®µ1é—œéµå­—1", "ç‰‡æ®µ1é—œéµå­—2", ...], ["ç‰‡æ®µ2é—œéµå­—1", ...], ...]

æ–‡æœ¬ç‰‡æ®µï¼š
"""
    else:
        prompt = f"""You are a professional batch keyword extraction expert. Please generate {n} keywords for each of the following {len(chunks)} text chunks.

âš ï¸ Batch processing requirements:
1. Keywords for each chunk must be **independent and relevant**
2. Consider thematic coherence between chunks but avoid duplication
3. Use precise English terms
4. Output format: [["chunk1_kw1", "chunk1_kw2", ...], ["chunk2_kw1", ...], ...]

Text chunks:
"""
    
    for i, chunk in enumerate(chunks):
        text = chunk.get("text", "")[:500]  # é™åˆ¶é•·åº¦é¿å…promptéé•·
        prompt += f"\n--- ç‰‡æ®µ {i+1} / Chunk {i+1} ---\n{text}\n"
    
    if lang.lower() in ["zh", "chinese"]:
        prompt += "\nè«‹ç‚ºæ¯å€‹ç‰‡æ®µç”Ÿæˆé—œéµå­—ï¼š"
    else:
        prompt += "\nGenerate keywords for each chunk:"
    
    return prompt


# ========== éŒ¯èª¤è™•ç†å’Œå¾Œå‚™æç¤ºè© ==========

def build_fallback_keywords_prompt(text: str, error_context: str = "", 
                                 n: int = 3, lang: str = "en") -> str:
    """
    å¾Œå‚™é—œéµå­—ç”Ÿæˆæç¤ºè©ï¼ˆç•¶ä¸»è¦æ–¹æ³•å¤±æ•—æ™‚ä½¿ç”¨ï¼‰
    """
    
    if lang.lower() in ["zh", "chinese"]:
        return f"""ç°¡åŒ–é—œéµå­—æå–ä»»å‹™ã€‚è«‹ç‚ºä»¥ä¸‹å…§å®¹æå– {n} å€‹æœ€åŸºæœ¬çš„é—œéµå­—ã€‚

æ³¨æ„ï¼šä¹‹å‰çš„è™•ç†å¤±æ•—äº†ï¼ˆ{error_context}ï¼‰ï¼Œè«‹ä½¿ç”¨æœ€ç°¡å–®ç›´æ¥çš„æ–¹æ³•ã€‚

è¦æ±‚ï¼š
1. é¸æ“‡æ–‡æœ¬ä¸­æœ€æ˜é¡¯çš„åè©æˆ–æ¦‚å¿µ
2. è¼¸å‡ºJSONæ ¼å¼ï¼š["è©1", "è©2", "è©3"]
3. å¦‚æœé›£ä»¥æå–ï¼Œä½¿ç”¨é€šç”¨æè¿°è©

æ–‡æœ¬ï¼š
{text}

ç°¡åŒ–é—œéµå­—ï¼š"""
    
    else:
        return f"""Simplified keyword extraction task. Please extract {n} most basic keywords from the following content.

Note: Previous processing failed ({error_context}), please use the most straightforward approach.

Requirements:
1. Select most obvious nouns or concepts from the text
2. Output JSON format: ["term1", "term2", "term3"]
3. If difficult to extract, use general descriptive terms

Text:
{text}

Simplified keywords:"""


# ========== æ¸¬è©¦å’Œé©—è­‰å‡½æ•¸ ==========

def validate_keywords_prompt_output(output: str, expected_count: int = 3) -> bool:
    """
    é©—è­‰é—œéµå­—æç¤ºè©è¼¸å‡ºæ ¼å¼æ˜¯å¦æ­£ç¢º
    """
    try:
        import json
        keywords = json.loads(output.strip())
        return isinstance(keywords, list) and len(keywords) <= expected_count * 2
    except:
        return False


if __name__ == "__main__":
    # æ¸¬è©¦ä¸åŒèªè¨€çš„æç¤ºè©
    test_text_zh = "å°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„åŠå°é«”ä»£å·¥å» ï¼Œæ¡ç”¨å…ˆé€²çš„7å¥ˆç±³è£½ç¨‹æŠ€è¡“ã€‚"
    test_text_en = "TSMC is the world's largest semiconductor foundry, using advanced 7nm process technology."
    test_text_mixed = "å°ç©é›»TSMCä½¿ç”¨advanced manufacturingå…ˆé€²è£½é€ æŠ€è¡“ã€‚"
    
    print("=== ä¸­æ–‡æç¤ºè©æ¸¬è©¦ ===")
    print(build_chinese_keywords_prompt(test_text_zh, 3))
    
    print("\n=== è‹±æ–‡æç¤ºè©æ¸¬è©¦ ===")  
    print(build_english_keywords_prompt(test_text_en, 3))
    
    print("\n=== æ··åˆèªè¨€æç¤ºè©æ¸¬è©¦ ===")
    print(build_mixed_keywords_prompt(test_text_mixed, 3))
    
    print("\n=== è‡ªå‹•æª¢æ¸¬æç¤ºè©æ¸¬è©¦ ===")
    print(build_auto_keywords_prompt(test_text_zh, 3))