# -*- coding: utf-8 -*-
"""
èªè¨€æ„ŸçŸ¥ Keyword LLM runner - åŸºæ–¼ç¾æœ‰æ¶æ§‹çš„å¢å¼·ç‰ˆ
ä¿®æ­£ç‰ˆæœ¬ï¼šéµå¾ªåŸæœ‰çš„ core.py, clients.py, prompts.py ä¸‰å±¤æ¶æ§‹

ä¸»è¦æ”¹é€²ï¼š
1. é›†æˆèªè¨€æª¢æ¸¬åˆ°åŸæœ‰ core.py æµç¨‹ä¸­
2. æ ¹æ“šchunkèªè¨€ä¿¡æ¯æ™ºèƒ½èª¿æ•´é—œéµå­—ç”Ÿæˆç­–ç•¥
3. ä¿æŒèˆ‡ç¾æœ‰clients.pyå’Œprompts.pyçš„å…¼å®¹æ€§
4. å¢å¼·çµ±è¨ˆå’Œå“è³ªç›£æ§åŠŸèƒ½
"""

import os
import json
import re
import time
import argparse
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from tqdm.auto import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# ä½¿ç”¨ç¾æœ‰çš„clientæ¨¡çµ„
from .clients import generate_keywords, generate_keywords_batch


# ========== èªè¨€æª¢æ¸¬å’Œç­–ç•¥é¸æ“‡ ==========

def detect_chunk_language(text: str) -> Tuple[str, Dict[str, float]]:
    """
    æª¢æ¸¬chunkçš„ä¸»è¦èªè¨€
    è¿”å›: (ä¸»è¦èªè¨€, èªè¨€çµ±è¨ˆ)
    """
    if not text.strip():
        return "unknown", {"chinese": 0, "english": 0, "other": 100}
    
    # çµ±è¨ˆä¸åŒé¡å‹å­—ç¬¦
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_meaningful = chinese_chars + english_chars
    
    if total_meaningful == 0:
        return "unknown", {"chinese": 0, "english": 0, "other": 100}
    
    chinese_ratio = chinese_chars / total_meaningful
    english_ratio = english_chars / total_meaningful
    
    stats = {
        "chinese": chinese_ratio * 100,
        "english": english_ratio * 100,
        "other": max(0, 100 - (chinese_ratio + english_ratio) * 100)
    }
    
    # åˆ¤å®šä¸»è¦èªè¨€
    if chinese_ratio > 0.6:
        return "chinese", stats
    elif english_ratio > 0.6:
        return "english", stats
    elif chinese_ratio > 0.3 and english_ratio > 0.3:
        return "mixed", stats
    else:
        return "unknown", stats


def get_language_aware_params(chunk: Dict) -> Dict:
    """
    æ ¹æ“šchunkèªè¨€ä¿¡æ¯ç²å–å„ªåŒ–åƒæ•¸
    """
    # å¦‚æœchunkå·²ç¶“åŒ…å«èªè¨€ä¿¡æ¯ï¼ˆä¾†è‡ªèªè¨€æ„ŸçŸ¥åˆ†å¡Šå™¨ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    if "main_language" in chunk:
        main_lang = chunk["main_language"]
        lang_stats = chunk.get("language_stats", {})
    else:
        # å¦å‰‡é€²è¡Œèªè¨€æª¢æ¸¬
        main_lang, lang_stats = detect_chunk_language(chunk.get("text", ""))
    
    # èªè¨€ç‰¹å®šçš„é—œéµå­—ç”Ÿæˆç­–ç•¥
    language_strategies = {
        "chinese": {
            "keyword_lang": "zh",
            "max_chars": 1000,      # ä¸­æ–‡chunkè¼ƒé•·ï¼Œå¯ç™¼é€æ›´å¤šå…§å®¹
            "n_keywords": 4,        # ä¸­æ–‡æ¦‚å¿µå¯†åº¦é«˜ï¼Œç”Ÿæˆæ›´å¤šé—œéµå­—
            "strategy": "chinese_focused"
        },
        "english": {
            "keyword_lang": "en", 
            "max_chars": 1400,      # è‹±æ–‡ä¿æŒåŸæœ‰é•·åº¦
            "n_keywords": 3,        # è‹±æ–‡æ¨™æº–æ•¸é‡
            "strategy": "english_focused"
        },
        "mixed": {
            "keyword_lang": "mixed",  # ä½¿ç”¨æ··åˆèªè¨€ç­–ç•¥
            "max_chars": 1200,       # æ··åˆå…§å®¹å–ä¸­é–“å€¼
            "n_keywords": 5,         # æ··åˆå…§å®¹ç”Ÿæˆæ›´å¤šä»¥è¦†è“‹é›™èª
            "strategy": "bilingual"
        },
        "unknown": {
            "keyword_lang": "auto",
            "max_chars": 1200,
            "n_keywords": 3,
            "strategy": "adaptive"
        }
    }
    
    # å¦‚æœæ˜¯mixedæˆ–unknownï¼Œé€²ä¸€æ­¥åˆ†æ
    if main_lang in ["mixed", "unknown"]:
        chinese_pct = lang_stats.get("chinese", 0)
        english_pct = lang_stats.get("english", 0)
        
        if chinese_pct > 50:
            strategy = language_strategies["chinese"].copy()
        elif english_pct > 50:
            strategy = language_strategies["english"].copy()
        else:
            strategy = language_strategies["mixed"].copy()
    else:
        strategy = language_strategies.get(main_lang, language_strategies["unknown"]).copy()
    
    # æ ¹æ“šchunkå“è³ªèª¿æ•´åƒæ•¸
    quality_score = chunk.get("quality_score", 0.5)
    if quality_score > 0.8:
        strategy["n_keywords"] += 1  # é«˜å“è³ªchunkç”Ÿæˆæ›´å¤šé—œéµå­—
    elif quality_score < 0.6:
        strategy["max_chars"] = int(strategy["max_chars"] * 0.8)  # ä½å“è³ªchunkæ¸›å°‘è¼¸å…¥
    
    # æ·»åŠ èªè¨€ä¿¡æ¯åˆ°ç­–ç•¥ä¸­
    strategy.update({
        "detected_language": main_lang,
        "language_stats": lang_stats,
        "quality_score": quality_score
    })
    
    return strategy


# ========== æ”¹é€²ç‰ˆè¼‰å…¥å‡½æ•¸ ==========

def _load_language_aware_chunks(path: str) -> List[Dict]:
    """è¼‰å…¥æ”¯æ´èªè¨€ä¿¡æ¯çš„chunks"""
    items: List[Dict] = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line_no, line in enumerate(f, 1):
            s = line.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception as e:
                print(f"[WARN] Line {line_no}: JSONè§£æå¤±æ•— - {e}")
                continue
                
            text = (obj.get("text") or "").strip()
            if not text:
                continue
                
            # æ§‹å»ºchunkæ•¸æ“šï¼Œä¿ç•™æ‰€æœ‰åŸæœ‰å­—æ®µ
            chunk_data = {
                "text": text,
                "source": obj.get("source", obj.get("file", "unknown")),
                "page": int(obj.get("page", 0) or 0),
            }
            
            # ä¿ç•™èªè¨€æ„ŸçŸ¥åˆ†å¡Šå™¨çš„ä¿¡æ¯
            for key in ["quality_score", "length", "_quality", "main_language", 
                       "language_stats", "global_language", "language_params"]:
                if key in obj:
                    chunk_data[key] = obj[key]
                    
            items.append(chunk_data)
    
    return items


def _load_from_txt(path: str) -> List[Dict]:
    """ä¿æŒåŸæœ‰çš„txtæ ¼å¼æ”¯æ´ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
    tmp: List[Dict] = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            # JSONL try
            if s.startswith("{") and s.endswith("}"):
                try:
                    obj = json.loads(s)
                    text = (obj.get("text") or "").strip()
                    if not text:
                        continue
                    tmp.append({
                        "text": text,
                        "source": obj.get("source", obj.get("file", "unknown")),
                        "page": int(obj.get("page", 0) or 0),
                    })
                except:
                    pass
            else:
                # plain text, assume unknown source
                if s:
                    tmp.append({"text": s, "source": "unknown", "page": 0})
    return tmp


def load_chunks(index_dir: str, input_file: str = None) -> List[Dict]:
    """
    æ”¹é€²ç‰ˆchunkè¼‰å…¥ - å„ªå…ˆæ”¯æ´èªè¨€æ„ŸçŸ¥chunks
    """
    if input_file:
        # å¦‚æœæŒ‡å®šäº†å…·é«”æ–‡ä»¶
        file_path = input_file if os.path.isabs(input_file) else os.path.join(index_dir, input_file)
        if os.path.exists(file_path):
            if file_path.endswith('.jsonl'):
                return _load_language_aware_chunks(file_path)
            else:
                return _load_from_txt(file_path)
    
    # è‡ªå‹•æŸ¥æ‰¾ï¼Œå„ªå…ˆä½¿ç”¨èªè¨€æ„ŸçŸ¥ç‰ˆæœ¬
    language_aware_path = os.path.join(index_dir, "chunks_language_aware.jsonl")
    optimized_path = os.path.join(index_dir, "chunks_optimized.jsonl")
    txt_path = os.path.join(index_dir, "chunks.txt")
    
    # æŒ‰å„ªå…ˆç´šå˜—è©¦
    for path_to_try, load_func in [
        (language_aware_path, _load_language_aware_chunks),
        (optimized_path, _load_language_aware_chunks),
        (txt_path, _load_from_txt)
    ]:
        if os.path.exists(path_to_try):
            print(f"[INFO] ä½¿ç”¨ {path_to_try}")
            chunks = load_func(path_to_try)
            if chunks:
                return chunks
    
    return []


# ========== æ”¹é€²ç‰ˆå·¥ä½œå‡½æ•¸ ==========

def _trim(s: str, max_chars: int) -> str:
    """æ™ºèƒ½æ–‡æœ¬æˆªå–"""
    if max_chars <= 0 or len(s) <= max_chars:
        return s
    
    # å˜—è©¦åœ¨å¥å­é‚Šç•Œæˆªå–
    truncated = s[:max_chars]
    sentence_ends = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
    best_cut = max_chars
    
    for i in range(len(truncated) - 1, max(0, max_chars - 200), -1):
        if truncated[i] in sentence_ends:
            best_cut = i + 1
            break
    
    return s[:best_cut].strip()


def _detect_content_type(text: str) -> str:
    """
    è‡ªå‹•æª¢æ¸¬å…§å®¹é¡å‹ä»¥æ”¹é€²é—œéµå­—ç”Ÿæˆ
    """
    text_lower = text.lower()
    
    # æŠ€è¡“æ–‡ä»¶æŒ‡æ¨™
    tech_indicators = ["display", "led", "oled", "cpu", "gpu", "algorithm", "protocol", 
                      "specification", "patent", "manufacturing"]
    # å•†æ¥­æ–‡ä»¶æŒ‡æ¨™  
    business_indicators = ["market", "revenue", "business", "strategy", "competition", 
                          "growth", "investment", "company"]
    # å­¸è¡“æ–‡ä»¶æŒ‡æ¨™
    academic_indicators = ["research", "study", "analysis", "methodology", "experiment", 
                          "conclusion", "hypothesis", "theory"]
    
    tech_score = sum(1 for indicator in tech_indicators if indicator in text_lower)
    business_score = sum(1 for indicator in business_indicators if indicator in text_lower)
    academic_score = sum(1 for indicator in academic_indicators if indicator in text_lower)
    
    max_score = max(tech_score, business_score, academic_score)
    if max_score == 0:
        return "general"
    elif tech_score == max_score:
        return "technical"
    elif business_score == max_score:
        return "business"
    else:
        return "academic"


def _language_aware_job(idx: int, rec: Dict, base_kw_lang: str, max_chars: int,
                       retries: int = 2, backoff: float = 1.2) -> Tuple[int, Dict]:
    """
    èªè¨€æ„ŸçŸ¥çš„é—œéµå­—ç”Ÿæˆä»»å‹™
    """
    full_text = (rec.get("text") or "").strip()
    if not full_text:
        return idx, _make_error_record(idx, rec, "Empty text", base_kw_lang)
    
    # ç²å–èªè¨€æ„ŸçŸ¥åƒæ•¸
    lang_params = get_language_aware_params(rec)
    
    # ä½¿ç”¨èªè¨€ç‰¹å®šçš„åƒæ•¸
    actual_kw_lang = lang_params["keyword_lang"]
    actual_max_chars = lang_params["max_chars"]
    actual_n_keywords = lang_params["n_keywords"]
    
    text_for_llm = _trim(full_text, actual_max_chars)
    content_type = _detect_content_type(full_text)
    
    source = rec.get("source", rec.get("file", "unknown"))
    try:
        page = int(rec.get("page", 0) or 0)
    except Exception:
        page = 0
    
    last_err = None
    for attempt in range(retries + 1):
        try:
            # èª¿ç”¨ç¾æœ‰çš„generate_keywordså‡½æ•¸ï¼Œä½†å‚³å…¥èªè¨€æ„ŸçŸ¥åƒæ•¸
            kws = generate_keywords(
                text_for_llm, 
                n=actual_n_keywords, 
                lang=actual_kw_lang, 
                content_type=content_type
            )
            
            # é©—è­‰é—œéµå­—
            kws = [k for k in kws if isinstance(k, str) and k.strip()]
            if not kws:
                kws = _generate_fallback_keywords(actual_kw_lang, 3)
            elif len(kws) < 3:
                kws.extend(_generate_fallback_keywords(actual_kw_lang, 3 - len(kws)))
            
            return idx, _make_success_record(idx, rec, kws, content_type, lang_params)
            
        except Exception as e:
            last_err = e
            if attempt < retries:
                time.sleep((attempt + 1) * backoff)
            else:
                fallback_kws = _generate_fallback_keywords(actual_kw_lang, 3)
                return idx, _make_error_record(idx, rec, str(last_err), actual_kw_lang, fallback_kws)

    return idx, _make_error_record(idx, rec, "unknown_error", base_kw_lang)


def _generate_fallback_keywords(lang: str, count: int = 3) -> List[str]:
    """ç”Ÿæˆèªè¨€ç‰¹å®šçš„å¾Œå‚™é—œéµå­—"""
    if "zh" in lang.lower():
        return [f"ä¸­æ–‡é—œéµå­—{i+1}" for i in range(count)]
    elif "en" in lang.lower():
        return [f"english_keyword_{i+1}" for i in range(count)]
    elif "mixed" in lang.lower():
        return [f"æ··åˆé—œéµå­—{i+1}" if i % 2 == 0 else f"mixed_keyword_{i+1}" 
                for i in range(count)]
    else:
        return [f"fallback_kw_{i+1}" for i in range(count)]


def _make_success_record(idx: int, chunk: Dict, keywords: List[str], 
                        content_type: str, lang_params: Dict) -> Dict:
    """å‰µå»ºæˆåŠŸè¨˜éŒ„ï¼ŒåŒ…å«èªè¨€ä¿¡æ¯"""
    full_text = chunk.get("text", "")
    
    record = {
        "chunk_id": idx,
        "source": chunk.get("source", "unknown"),
        "page": chunk.get("page", 0),
        "text": full_text,
        "keywords": keywords,
        "preview": full_text[:150],
        "content_type": content_type,
        "text_length": len(full_text),
        "keywords_count": len(keywords),
        "has_error": False,
        
        # èªè¨€æ„ŸçŸ¥ä¿¡æ¯
        "detected_language": lang_params["detected_language"],
        "language_stats": lang_params["language_stats"],
        "keyword_language": lang_params["keyword_lang"],
        "generation_strategy": lang_params["strategy"],
        "language_aware_processing": True
    }
    
    # ä¿ç•™åŸæœ‰çš„å“è³ªå’Œèªè¨€ä¿¡æ¯
    for key in ["quality_score", "_quality", "main_language", "global_language"]:
        if key in chunk:
            record[key] = chunk[key]
    
    return record


def _make_error_record(idx: int, chunk: Dict, error: str, 
                      kw_lang: str, fallback_kws: List[str] = None) -> Dict:
    """å‰µå»ºéŒ¯èª¤è¨˜éŒ„"""
    if fallback_kws is None:
        fallback_kws = _generate_fallback_keywords(kw_lang, 3)
    
    lang_params = get_language_aware_params(chunk)
    record = _make_success_record(idx, chunk, fallback_kws, "unknown", lang_params)
    record.update({
        "has_error": True,
        "error": error,
        "generation_strategy": "fallback"
    })
    
    return record


# ========== æ‰¹é‡è™•ç†å‡½æ•¸ ==========

def _batch_process_language_aware(chunks: List[Dict], kw_lang: str, max_chars: int) -> List[Dict]:
    """
    èªè¨€æ„ŸçŸ¥çš„æ‰¹é‡è™•ç†
    """
    print(f"[INFO] ä½¿ç”¨èªè¨€æ„ŸçŸ¥æ‰¹é‡è™•ç†æ¨¡å¼")
    
    # æº–å‚™æ•¸æ“šä¸¦é€²è¡Œèªè¨€æ„ŸçŸ¥é è™•ç†
    processed_chunks = []
    language_distribution = {}
    
    if TQDM_AVAILABLE:
        chunk_iter = tqdm(enumerate(chunks), total=len(chunks), desc="èªè¨€åˆ†æ", unit="chunk")
    else:
        chunk_iter = enumerate(chunks)
        print(f"[INFO] åˆ†æ {len(chunks)} å€‹chunksçš„èªè¨€ä¿¡æ¯...")
    
    for i, chunk in chunk_iter:
        # ç²å–èªè¨€æ„ŸçŸ¥åƒæ•¸
        lang_params = get_language_aware_params(chunk)
        
        # çµ±è¨ˆèªè¨€åˆ†å¸ƒ
        detected_lang = lang_params["detected_language"]
        language_distribution[detected_lang] = language_distribution.get(detected_lang, 0) + 1
        
        # æº–å‚™è™•ç†æ•¸æ“š
        chunk_copy = chunk.copy()
        chunk_copy["text"] = _trim(chunk["text"], lang_params["max_chars"])
        chunk_copy["_lang_params"] = lang_params  # æš«å­˜èªè¨€åƒæ•¸
        processed_chunks.append(chunk_copy)
    
    print(f"[LANG] èªè¨€åˆ†å¸ƒçµ±è¨ˆ: {language_distribution}")
    
    # èª¿ç”¨ç¾æœ‰çš„æ‰¹é‡ç”Ÿæˆå‡½æ•¸
    print(f"[INFO] é–‹å§‹æ‰¹é‡é—œéµå­—ç”Ÿæˆ...")
    results = generate_keywords_batch(processed_chunks, n=3, lang=kw_lang)
    
    # è½‰æ›ç‚ºè¼¸å‡ºæ ¼å¼ï¼Œæ·»åŠ èªè¨€æ„ŸçŸ¥ä¿¡æ¯
    output_records = []
    
    if TQDM_AVAILABLE:
        result_iter = tqdm(enumerate(results), total=len(results), 
                          desc="è™•ç†çµæœ", unit="chunk")
    else:
        result_iter = enumerate(results)
        print(f"[INFO] è™•ç† {len(results)} å€‹çµæœ...")
        
    for i, result in result_iter:
        original_chunk = chunks[i]
        lang_params = processed_chunks[i]["_lang_params"]
        
        source = result.get("source", "unknown")
        try:
            page = int(result.get("page", 0) or 0)
        except Exception:
            page = 0
            
        full_text = original_chunk["text"]
        keywords = result.get("keywords", _generate_fallback_keywords(kw_lang, 3))
        
        out_record = {
            "chunk_id": i,
            "source": source,
            "page": page,
            "text": full_text,
            "keywords": keywords,
            "preview": full_text[:150],
            "content_type": _detect_content_type(full_text),
            "text_length": len(full_text),
            "keywords_count": len(keywords),
            "has_error": False,
            
            # èªè¨€æ„ŸçŸ¥ä¿¡æ¯
            "detected_language": lang_params["detected_language"],
            "language_stats": lang_params["language_stats"],
            "keyword_language": lang_params["keyword_lang"],
            "generation_strategy": lang_params["strategy"],
            "language_aware_processing": True
        }
        
        # ä¿ç•™åŸæœ‰ä¿¡æ¯
        for key in ["quality_score", "_quality", "main_language"]:
            if key in original_chunk:
                out_record[key] = original_chunk[key]
                
        output_records.append(out_record)
    
    print(f"[INFO] èªè¨€æ„ŸçŸ¥æ‰¹é‡è™•ç†å®Œæˆï¼Œç”Ÿæˆ {len(output_records)} æ¢è¨˜éŒ„")
    return output_records


# ========== ä¸»è™•ç†å‡½æ•¸ ==========

def main():
    parser = argparse.ArgumentParser(description="èªè¨€æ„ŸçŸ¥é—œéµå­—ç”Ÿæˆå·¥å…· (core.py)")
    parser.add_argument("--index", default="indices", help="ç´¢å¼•ç›®éŒ„")
    parser.add_argument("--input-file", help="æŒ‡å®šç‰¹å®šçš„chunksæ–‡ä»¶ (å¯é¸)")
    parser.add_argument("--out", default="outputs/data/keywords_language_aware.jsonl", help="è¼¸å‡ºæ–‡ä»¶")
    parser.add_argument("--langs", default="auto", help="åŸºç¤èªè¨€æç¤º (auto=è‡ªå‹•æª¢æ¸¬)")
    parser.add_argument("--max-chunks", type=int, default=0, help="é™åˆ¶è™•ç†chunksæ•¸é‡ (0=å…¨éƒ¨)")
    parser.add_argument("--workers", type=int, default=32, help="ä¸¦è¡Œworkeræ•¸é‡ (é è¨­32ï¼Œæœ€ä½³æ•ˆèƒ½)")
    parser.add_argument("--max-chars", type=int, default=1400, help="åŸºç¤æ–‡æœ¬æˆªæ–·é•·åº¦")
    parser.add_argument("--batch-mode", action="store_true", help="ä½¿ç”¨æ‰¹é‡è™•ç†æ¨¡å¼")
    parser.add_argument("--fast", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼š32 workers + æœ€ä½³åŒ–è¨­å®š")
    parser.add_argument("--language-aware", action="store_true", default=True, help="å•Ÿç”¨èªè¨€æ„ŸçŸ¥è™•ç† (é è¨­)")
    
    args = parser.parse_args()

    # å¿«é€Ÿæ¨¡å¼è¨­å®š
    if args.fast:
        args.workers = 32
        args.batch_mode = False
        print(f"[FAST] å•Ÿç”¨å¿«é€Ÿæ¨¡å¼ï¼š32 workersï¼Œæœ€ä½³åŒ–èªè¨€æ„ŸçŸ¥è™•ç†")

    # èªè¨€è¨­å®š
    kw_lang = args.langs if args.langs != "auto" else "auto"

    os.makedirs(os.path.dirname(args.out), exist_ok=True)

    # è¼‰å…¥chunks
    chunks = load_chunks(args.index, args.input_file)
    if args.max_chunks and args.max_chunks > 0:
        chunks = chunks[:args.max_chunks]
    
    print(f"[INFO] å¾ '{args.index}' è¼‰å…¥äº† {len(chunks)} å€‹chunks")
    if args.input_file:
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šæ–‡ä»¶: {args.input_file}")
    
    # é¡¯ç¤ºchunksçµ±è¨ˆ
    language_aware_count = len([c for c in chunks if "main_language" in c])
    if language_aware_count > 0:
        print(f"[INFO] åŒ…å«èªè¨€ä¿¡æ¯çš„chunks: {language_aware_count}/{len(chunks)}")
        
        # çµ±è¨ˆèªè¨€åˆ†å¸ƒ
        lang_dist = {}
        for c in chunks:
            if "main_language" in c:
                lang = c["main_language"]
                lang_dist[lang] = lang_dist.get(lang, 0) + 1
        print(f"[LANG] é å­˜èªè¨€åˆ†å¸ƒ: {lang_dist}")
    
    if not chunks:
        print("[ERROR] æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„chunks")
        return

    # è™•ç†chunks
    start_time = time.time()
    
    if args.batch_mode:
        # æ‰¹é‡è™•ç†æ¨¡å¼
        processed_records = _batch_process_language_aware(chunks, kw_lang, args.max_chars)
        
        # ç›´æ¥å¯«å…¥æ–‡ä»¶
        with open(args.out, "w", encoding="utf-8") as fout:
            for record in processed_records:
                fout.write(json.dumps(record, ensure_ascii=False) + "\n")
    else:
        # ä¸¦è¡Œè™•ç†æ¨¡å¼
        results = []
        
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            futures = [
                executor.submit(_language_aware_job, i, chunk, kw_lang, args.max_chars)
                for i, chunk in enumerate(chunks)
            ]
            
            with open(args.out, "w", encoding="utf-8") as fout:
                processed_count = 0
                error_count = 0
                
                if TQDM_AVAILABLE:
                    pbar = tqdm(total=len(chunks), desc="ç”Ÿæˆé—œéµå­—", unit="chunk")
                
                for fut in as_completed(futures):
                    try:
                        _, outrec = fut.result()
                        results.append(outrec)
                        
                        if outrec.get("has_error", False):
                            error_count += 1
                        
                        processed_count += 1
                        
                        if TQDM_AVAILABLE:
                            pbar.set_postfix({
                                "errors": error_count,
                                "success": f"{((processed_count-error_count)/processed_count*100):.1f}%"
                            })
                            pbar.update(1)
                        else:
                            if processed_count % 50 == 0:
                                success_rate = (processed_count - error_count) / processed_count * 100
                                print(f"[PROGRESS] {processed_count}/{len(chunks)} ({success_rate:.1f}% æˆåŠŸ)")
                    
                    except Exception as e:
                        error_count += 1
                        print(f"[ERROR] è™•ç†å¤±æ•—: {e}")
                
                if TQDM_AVAILABLE:
                    pbar.close()
                
                # æŒ‰chunk_idé †åºæ’åºä¸¦å¯«å…¥
                results.sort(key=lambda x: x.get('chunk_id', 0))
                for record in results:
                    fout.write(json.dumps(record, ensure_ascii=False) + "\n")

    elapsed = time.time() - start_time
    chunks_per_sec = len(chunks) / elapsed if elapsed > 0 else 0
    print(f"[PERF] è™•ç†å®Œæˆï¼šè€—æ™‚: {elapsed:.2f}s, é€Ÿåº¦: {chunks_per_sec:.1f} chunks/s")

    print(f"[OK] å·²å¯«å…¥ {len(chunks)} æ¢å®Œæ•´è¨˜éŒ„åˆ° {args.out}")
    
    # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
    _generate_language_aware_statistics(args.out)


def _generate_language_aware_statistics(output_file: str):
    """ç”Ÿæˆèªè¨€æ„ŸçŸ¥çµ±è¨ˆå ±å‘Š"""
    stats = {
        "total_chunks": 0,
        "language_distribution": {},
        "keyword_language_distribution": {},
        "generation_strategy_distribution": {},
        "quality_distribution": {"high": 0, "medium": 0, "low": 0},
        "content_type_distribution": {},
        "error_stats": {"total_errors": 0, "error_rate": 0},
        "keyword_stats": {"total_keywords": 0, "avg_keywords_per_chunk": 0},
        "language_aware_stats": {
            "chunks_with_language_info": 0,
            "avg_chinese_ratio": 0,
            "avg_english_ratio": 0
        }
    }
    
    total_chinese_ratio = 0
    total_english_ratio = 0
    language_info_count = 0
    total_keywords = 0
    
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    record = json.loads(line)
                    stats["total_chunks"] += 1
                    
                    # èªè¨€åˆ†å¸ƒ
                    detected_lang = record.get("detected_language", "unknown")
                    stats["language_distribution"][detected_lang] = \
                        stats["language_distribution"].get(detected_lang, 0) + 1
                    
                    # é—œéµå­—èªè¨€åˆ†å¸ƒ
                    kw_lang = record.get("keyword_language", "unknown")
                    stats["keyword_language_distribution"][kw_lang] = \
                        stats["keyword_language_distribution"].get(kw_lang, 0) + 1
                    
                    # ç”Ÿæˆç­–ç•¥åˆ†å¸ƒ
                    strategy = record.get("generation_strategy", "unknown")
                    stats["generation_strategy_distribution"][strategy] = \
                        stats["generation_strategy_distribution"].get(strategy, 0) + 1
                    
                    # å“è³ªåˆ†å¸ƒ
                    quality = record.get("_quality", "unknown")
                    if quality in stats["quality_distribution"]:
                        stats["quality_distribution"][quality] += 1
                    
                    # å…§å®¹é¡å‹åˆ†å¸ƒ
                    content_type = record.get("content_type", "unknown")
                    stats["content_type_distribution"][content_type] = \
                        stats["content_type_distribution"].get(content_type, 0) + 1
                    
                    # éŒ¯èª¤çµ±è¨ˆ
                    if record.get("has_error", False):
                        stats["error_stats"]["total_errors"] += 1
                    
                    # é—œéµå­—çµ±è¨ˆ
                    kw_count = record.get("keywords_count", 0)
                    total_keywords += kw_count
                    
                    # èªè¨€æ„ŸçŸ¥çµ±è¨ˆ
                    if record.get("language_aware_processing", False):
                        language_info_count += 1
                        lang_stats = record.get("language_stats", {})
                        total_chinese_ratio += lang_stats.get("chinese", 0)
                        total_english_ratio += lang_stats.get("english", 0)
                
                except json.JSONDecodeError:
                    continue
        
        # è¨ˆç®—å¹³å‡å€¼å’Œæ¯”ç‡
        if stats["total_chunks"] > 0:
            stats["error_stats"]["error_rate"] = \
                stats["error_stats"]["total_errors"] / stats["total_chunks"] * 100
            stats["keyword_stats"]["total_keywords"] = total_keywords
            stats["keyword_stats"]["avg_keywords_per_chunk"] = \
                total_keywords / stats["total_chunks"]
            
            stats["language_aware_stats"]["chunks_with_language_info"] = language_info_count
            if language_info_count > 0:
                stats["language_aware_stats"]["avg_chinese_ratio"] = \
                    total_chinese_ratio / language_info_count
                stats["language_aware_stats"]["avg_english_ratio"] = \
                    total_english_ratio / language_info_count
        
        # ä¿å­˜çµ±è¨ˆå ±å‘Š
        stats_file = output_file.replace('.jsonl', '_language_aware_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°é—œéµçµ±è¨ˆ
        print(f"\nğŸ“Š èªè¨€æ„ŸçŸ¥é—œéµå­—ç”Ÿæˆçµ±è¨ˆ:")
        print(f"   è™•ç†chunks: {stats['total_chunks']} å€‹")
        print(f"   æˆåŠŸç‡: {100 - stats['error_stats']['error_rate']:.1f}%")
        print(f"   å¹³å‡æ¯chunké—œéµå­—: {stats['keyword_stats']['avg_keywords_per_chunk']:.1f} å€‹")
        
        print(f"   èªè¨€æª¢æ¸¬åˆ†å¸ƒ:")
        for lang, count in stats["language_distribution"].items():
            if count > 0:
                pct = count / stats["total_chunks"] * 100
                print(f"     {lang}: {count} ({pct:.1f}%)")
        
        print(f"   é—œéµå­—èªè¨€åˆ†å¸ƒ:")
        for lang, count in stats["keyword_language_distribution"].items():
            if count > 0:
                pct = count / stats["total_chunks"] * 100
                print(f"     {lang}: {count} ({pct:.1f}%)")
        
        print(f"   ç”Ÿæˆç­–ç•¥åˆ†å¸ƒ:")
        for strategy, count in stats["generation_strategy_distribution"].items():
            if count > 0:
                pct = count / stats["total_chunks"] * 100
                print(f"     {strategy}: {count} ({pct:.1f}%)")
        
        if language_info_count > 0:
            avg_zh = stats["language_aware_stats"]["avg_chinese_ratio"]
            avg_en = stats["language_aware_stats"]["avg_english_ratio"]
            print(f"   å¹³å‡èªè¨€æ¯”ä¾‹: ä¸­æ–‡ {avg_zh:.1f}%, è‹±æ–‡ {avg_en:.1f}%")
        
        print(f"   çµ±è¨ˆå ±å‘Šå·²ä¿å­˜: {stats_file}")
    
    except Exception as e:
        print(f"[WARN] çµ±è¨ˆå ±å‘Šç”Ÿæˆå¤±æ•—: {e}")


if __name__ == "__main__":
    main()