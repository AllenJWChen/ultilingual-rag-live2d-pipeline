# -*- coding: utf-8 -*-
from __future__ import annotations

"""
æ”¹é€²ç‰ˆKeyword LLM runner - æ”¯æ´å„ªåŒ–chunksæ ¼å¼å’Œæ›´å¥½çš„é—œéµå­—ç”Ÿæˆ

ä¸»è¦æ”¹é€²ï¼š
1. æ”¯æ´å„ªåŒ–ç‰ˆchunksæ ¼å¼ï¼ˆåŒ…å«quality_scoreï¼‰
2. æ›´æ™ºèƒ½çš„é—œéµå­—ç”Ÿæˆç­–ç•¥
3. æ‰¹é‡è™•ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥
4. æ›´å¥½çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
"""

import os
import json
import re
import time
import argparse
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from tqdm.auto import tqdm
except Exception:
    tqdm = None
TQDM_AVAILABLE = tqdm is not None

# project-local
from .clients import generate_keywords, generate_keywords_batch


# ---------- æ”¹é€²ç‰ˆchunkè¼‰å…¥ ----------
def _load_from_jsonl(path: str) -> List[Dict]:
    """æ”¯æ´å„ªåŒ–ç‰ˆchunksæ ¼å¼çš„è¼‰å…¥"""
    items: List[Dict] = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line_no, line in enumerate(f, 1):
            s = line.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception as e:
                print(f"[WARN] Line {line_no}: JSONè§£æå¤±æ•— - {e}")
                continue
                
            text = (obj.get("text") or "").strip()
            if not text:
                continue
                
            # æ”¯æ´å„ªåŒ–ç‰ˆæœ¬çš„é¡å¤–å­—æ®µ
            chunk_data = {
                "text": text,
                "source": obj.get("source", obj.get("file", "unknown")),
                "page": int(obj.get("page", 0) or 0),
            }
            
            # ä¿ç•™å„ªåŒ–ç‰ˆæœ¬çš„å“è³ªä¿¡æ¯
            if "quality_score" in obj:
                chunk_data["quality_score"] = obj["quality_score"]
            if "length" in obj:  
                chunk_data["length"] = obj["length"]
            if "_quality" in obj:  # å…¼å®¹æ ¼å¼
                chunk_data["_quality"] = obj["_quality"]
                
            items.append(chunk_data)
    return items


def _load_from_txt(path: str) -> List[Dict]:
    """ä¿æŒåŸæœ‰çš„txtæ ¼å¼æ”¯æ´"""
    tmp: List[Dict] = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            # JSONL try
            if s.startswith("{") and s.endswith("}"):
                try:
                    obj = json.loads(s)
                    text = (obj.get("text") or "").strip()
                    if not text:
                        continue
                    tmp.append({
                        "text": text,
                        "source": obj.get("source", obj.get("file", "unknown")),
                        "page": int(obj.get("page", 0) or 0),
                    })
                    continue
                except Exception:
                    pass
            # TSV: src \t page \t text
            if "\t" in s:
                parts = s.split("\t", 2)
                if len(parts) == 3:
                    src, pg, txt = parts
                    try:
                        pg_i = int(pg)
                    except Exception:
                        pg_i = 0
                    if txt.strip():
                        tmp.append({"text": txt.strip(), "source": src or "unknown", "page": pg_i})
                        continue
            # PIPE: src ||| page ||| text
            if "|||" in s:
                parts = s.split("|||", 2)
                if len(parts) == 3:
                    src, pg, txt = [p.strip() for p in parts]
                    try:
                        pg_i = int(pg)
                    except Exception:
                        pg_i = 0
                    if txt:
                        tmp.append({"text": txt, "source": src or "unknown", "page": pg_i})
                        continue

    if len(tmp) >= 5:
        return tmp

    # BLOCK format: [<source> | page N] then multi-line content
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        blob = f.read()
    text_all = blob.replace("\\n", "\n")
    header_re = re.compile(r'\[(.+?)\s*\|\s*page\s+(\d+)\]', re.IGNORECASE)

    items: List[Dict] = []
    matches = list(header_re.finditer(text_all))
    for i, m in enumerate(matches):
        src = m.group(1).strip()
        try:
            pg = int(m.group(2))
        except Exception:
            pg = 0
        start = m.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text_all)
        body = text_all[start:end].strip()
        if body:
            items.append({"text": body, "source": src, "page": pg})
    return items


def load_chunks(index_dir: str, input_file: str = None) -> List[Dict]:
    """
    æ”¹é€²ç‰ˆchunkè¼‰å…¥ - æ”¯æ´æŒ‡å®šç‰¹å®šæ–‡ä»¶
    """
    if input_file:
        # å¦‚æœæŒ‡å®šäº†å…·é«”æ–‡ä»¶
        file_path = input_file if os.path.isabs(input_file) else os.path.join(index_dir, input_file)
        if os.path.exists(file_path):
            if file_path.endswith('.jsonl'):
                return _load_from_jsonl(file_path)
            else:
                return _load_from_txt(file_path)
    
    # åŸæœ‰é‚è¼¯ï¼šè‡ªå‹•æŸ¥æ‰¾
    jsonl_path = os.path.join(index_dir, "chunks_optimized.jsonl")
    txt_path = os.path.join(index_dir, "chunks.txt")
    
    if os.path.exists(jsonl_path):
        rows = _load_from_jsonl(jsonl_path)
        if rows:
            return rows
    if os.path.exists(txt_path):
        rows = _load_from_txt(txt_path)
        if rows:
            return rows
    return []


# ---------- æ”¹é€²ç‰ˆworker ----------
def _trim(s: str, max_chars: int) -> str:
    if max_chars <= 0 or len(s) <= max_chars:
        return s
    return s[:max_chars]


def _detect_content_type(text: str) -> str:
    """
    è‡ªå‹•æª¢æ¸¬å…§å®¹é¡å‹ä»¥æ”¹é€²é—œéµå­—ç”Ÿæˆ
    """
    text_lower = text.lower()
    
    # æŠ€è¡“æ–‡ä»¶æŒ‡æ¨™
    tech_indicators = ["display", "led", "oled", "cpu", "gpu", "algorithm", "protocol", "specification"]
    # å•†æ¥­æ–‡ä»¶æŒ‡æ¨™  
    business_indicators = ["market", "revenue", "business", "strategy", "competition", "growth"]
    # å­¸è¡“æ–‡ä»¶æŒ‡æ¨™
    academic_indicators = ["research", "study", "analysis", "methodology", "experiment", "conclusion"]
    
    tech_score = sum(1 for indicator in tech_indicators if indicator in text_lower)
    business_score = sum(1 for indicator in business_indicators if indicator in text_lower)
    academic_score = sum(1 for indicator in academic_indicators if indicator in text_lower)
    
    max_score = max(tech_score, business_score, academic_score)
    if max_score == 0:
        return "general"
    elif tech_score == max_score:
        return "technical"
    elif business_score == max_score:
        return "business"
    else:
        return "academic"


def _one_job(idx: int, rec: Dict, kw_lang: str, max_chars: int,
             retries: int = 2, backoff: float = 1.2) -> Tuple[int, Dict]:
    """
    æ”¹é€²ç‰ˆå–®å€‹ä»»å‹™è™•ç† - æ›´æ™ºèƒ½çš„é—œéµå­—ç”Ÿæˆ
    è¼¸å‡ºåŒ…å«å®Œæ•´chunkså…§å®¹ï¼Œä¾¿æ–¼å¾ŒçºŒå•é¡Œç”Ÿæˆ
    """
    full_text = (rec.get("text") or "").strip()
    text_for_llm = _trim(full_text, max_chars)

    source = rec.get("source", rec.get("file", "unknown"))
    try:
        page = int(rec.get("page", 0) or 0)
    except Exception:
        page = 0

    def _make_record(kws: list, error: str = None, content_type: str = "unknown") -> Dict:
        # åŒ…å«å®Œæ•´chunksè³‡è¨Šçš„è¼¸å‡ºæ ¼å¼
        out = {
            "chunk_id": idx,
            "source": source,
            "page": page,
            "text": full_text,  # ğŸ¯ å®Œæ•´æ–‡å­—å…§å®¹
            "keywords": kws,
            "preview": full_text[:150],  # é©ç•¶é•·åº¦çš„é è¦½
            "content_type": content_type,
            "text_length": len(full_text),  # æ–‡å­—é•·åº¦çµ±è¨ˆ
        }
        
        # ä¿ç•™å„ªåŒ–ç‰ˆæœ¬çš„é¡å¤–ä¿¡æ¯
        if "quality_score" in rec:
            out["quality_score"] = rec["quality_score"]
        if "_quality" in rec:
            out["_quality"] = rec["_quality"]
        if "length" in rec:
            out["original_length"] = rec["length"]
            
        # æ·»åŠ keywordsçµ±è¨ˆ
        out["keywords_count"] = len(kws)
        
        if error:
            out["error"] = error
            out["has_error"] = True
        else:
            out["has_error"] = False
            
        return out

    # æª¢æ¸¬å…§å®¹é¡å‹
    content_type = _detect_content_type(full_text)
    
    last_err = None
    for attempt in range(retries + 1):
        try:
            # ä½¿ç”¨æ”¹é€²çš„é—œéµå­—ç”Ÿæˆ
            kws = generate_keywords(text_for_llm, n=3, lang=kw_lang, content_type=content_type)
            
            # ç¢ºä¿æœ‰æ•ˆæ€§
            kws = [k for k in kws if isinstance(k, str) and k.strip()]
            if not kws:
                kws = [f"fallback_kw_{i+1}" for i in range(3)]
            elif len(kws) < 3:
                kws.extend([f"supplementary_{i+1}" for i in range(3 - len(kws))])
            
            return idx, _make_record(kws[:3], content_type=content_type)
            
        except Exception as e:
            last_err = e
            if attempt < retries:
                time.sleep((attempt + 1) * backoff)
            else:
                fallback_kws = [f"error_fallback_{i+1}" for i in range(3)]
                return idx, _make_record(
                    fallback_kws,
                    error=str(last_err),
                    content_type=content_type
                )

    return idx, _make_record(
        [f"unknown_error_{i+1}" for i in range(3)],
        error="unknown",
        content_type=content_type
    )


def _batch_process_with_context(chunks: List[Dict], kw_lang: str, max_chars: int) -> List[Dict]:
    """
    æ‰¹é‡è™•ç† - è€ƒæ…®ä¸Šä¸‹æ–‡ç›¸é—œæ€§ï¼Œè¼¸å‡ºå®Œæ•´chunkså…§å®¹
    """
    print(f"[INFO] ä½¿ç”¨æ‰¹é‡ä¸Šä¸‹æ–‡è™•ç†æ¨¡å¼")
    
    # æº–å‚™æ•¸æ“š
    processed_chunks = []
    
    # æ·»åŠ æ•¸æ“šæº–å‚™é€²åº¦æ¢
    if TQDM_AVAILABLE:
        chunk_iter = tqdm(enumerate(chunks), total=len(chunks), desc="æº–å‚™æ•¸æ“š", unit="chunk")
    else:
        chunk_iter = enumerate(chunks)
        print(f"[INFO] æº–å‚™ {len(chunks)} å€‹chunksçš„æ•¸æ“š...")
        
    for i, chunk in chunk_iter:
        chunk_copy = chunk.copy()
        chunk_copy["text"] = _trim(chunk["text"], max_chars)
        processed_chunks.append(chunk_copy)
    
    # æ‰¹é‡ç”Ÿæˆé—œéµå­—
    print(f"[INFO] é–‹å§‹æ‰¹é‡é—œéµå­—ç”Ÿæˆ...")
    results = generate_keywords_batch(processed_chunks, n=3, lang=kw_lang)
    
    # è½‰æ›ç‚ºè¼¸å‡ºæ ¼å¼ - åŒ…å«å®Œæ•´å…§å®¹
    output_records = []
    
    # æ·»åŠ çµæœè™•ç†é€²åº¦æ¢
    if TQDM_AVAILABLE:
        result_iter = tqdm(enumerate(results), total=len(results), 
                          desc="è™•ç†çµæœ", unit="chunk")
    else:
        result_iter = enumerate(results)
        print(f"[INFO] è™•ç† {len(results)} å€‹çµæœ...")
        
    for i, result in result_iter:
        source = result.get("source", "unknown")
        try:
            page = int(result.get("page", 0) or 0)
        except Exception:
            page = 0
            
        full_text = chunks[i]["text"]  # ä½¿ç”¨åŸå§‹å®Œæ•´æ–‡æœ¬
        keywords = result.get("keywords", [f"batch_kw_{j+1}" for j in range(3)])
        
        out_record = {
            "chunk_id": i,
            "source": source,
            "page": page,
            "text": full_text,  # ğŸ¯ å®Œæ•´æ–‡å­—å…§å®¹
            "keywords": keywords,
            "preview": full_text[:150],
            "content_type": _detect_content_type(full_text),
            "text_length": len(full_text),
            "keywords_count": len(keywords),
            "has_error": False,
        }
        
        # ä¿ç•™é¡å¤–ä¿¡æ¯
        if "quality_score" in result:
            out_record["quality_score"] = result["quality_score"]
        if "_quality" in result:
            out_record["_quality"] = result["_quality"]
        if "length" in result:
            out_record["original_length"] = result["length"]
            
        output_records.append(out_record)
    
    print(f"[INFO] æ‰¹é‡è™•ç†å®Œæˆï¼Œç”Ÿæˆ {len(output_records)} æ¢è¨˜éŒ„")
    return output_records


def main():
    parser = argparse.ArgumentParser(description="æ”¹é€²ç‰ˆé—œéµå­—ç”Ÿæˆå·¥å…·")
    parser.add_argument("--index", default="indices", help="ç´¢å¼•ç›®éŒ„")
    parser.add_argument("--input-file", help="æŒ‡å®šç‰¹å®šçš„chunksæ–‡ä»¶ (å¯é¸)")
    parser.add_argument("--out", default="outputs/data/keywords_optimized.jsonl", help="è¼¸å‡ºæ–‡ä»¶")
    parser.add_argument("--langs", default="zh,en", help="èªè¨€æç¤ºï¼Œç¬¬ä¸€å€‹ä½œç‚ºä¸»è¦èªè¨€")
    parser.add_argument("--max-chunks", type=int, default=0, help="é™åˆ¶è™•ç†chunksæ•¸é‡ (0=å…¨éƒ¨)")
    parser.add_argument("--workers", type=int, default=32, help="ä¸¦è¡Œworkeræ•¸é‡ (é è¨­32ï¼Œæœ€ä½³æ•ˆèƒ½)")
    parser.add_argument("--max-chars", type=int, default=1400, help="ç™¼é€çµ¦LLMå‰çš„æ–‡æœ¬æˆªæ–·é•·åº¦")
    parser.add_argument("--batch-mode", action="store_true", help="ä½¿ç”¨æ‰¹é‡ä¸Šä¸‹æ–‡è™•ç†æ¨¡å¼")
    parser.add_argument("--fast", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼š32 workers + æœ€ä½³åŒ–è¨­å®š")
    
    args = parser.parse_args()

    # å¿«é€Ÿæ¨¡å¼è¨­å®š
    if args.fast:
        args.workers = 32
        args.batch_mode = False  # é«˜ä½µç™¼æ™‚ä¸ç”¨batchæ¨¡å¼
        print(f"[FAST] å•Ÿç”¨å¿«é€Ÿæ¨¡å¼ï¼š32 workersï¼Œæœ€ä½³åŒ–ä½µç™¼è™•ç†")

    langs = [s.strip() for s in args.langs.split(",") if s.strip()]
    kw_lang = langs[0] if langs else "auto"

    os.makedirs(os.path.dirname(args.out), exist_ok=True)

    # è¼‰å…¥chunks
    chunks = load_chunks(args.index, args.input_file)
    if args.max_chunks and args.max_chunks > 0:
        chunks = chunks[:args.max_chunks]
    
    print(f"[INFO] å¾ '{args.index}' è¼‰å…¥äº† {len(chunks)} å€‹chunks")
    if args.input_file:
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šæ–‡ä»¶: {args.input_file}")
    
    # é¡¯ç¤ºchunksçµ±è¨ˆ
    quality_chunks = len([c for c in chunks if c.get("quality_score", 0) > 0])
    if quality_chunks > 0:
        avg_quality = sum(c.get("quality_score", 0) for c in chunks) / quality_chunks
        print(f"[INFO] ç™¼ç¾ {quality_chunks} å€‹å¸¶å“è³ªè©•åˆ†çš„chunksï¼Œå¹³å‡å“è³ª: {avg_quality:.3f}")

    print(f"[INFO] ä½¿ç”¨ {args.workers} å€‹ä¸¦è¡Œworkersé€²è¡Œé—œéµå­—ç”Ÿæˆ")

    # ç‚ºé«˜ä½µç™¼å„ªåŒ–ç’°å¢ƒè®Šæ•¸
    if args.workers >= 16:
        os.environ.setdefault("OLLAMA_NUM_PARALLEL", str(args.workers))
        print(f"[INFO] å·²è¨­å®š OLLAMA_NUM_PARALLEL={args.workers} æœ€ä½³åŒ–ä¸¦ç™¼è™•ç†")
    
    # æª¢æŸ¥tqdmå¯ç”¨æ€§ä¸¦æä¾›å›é€€æ–¹æ¡ˆ
    use_progress_bar = TQDM_AVAILABLE
    if not use_progress_bar:
        print(f"[WARN] tqdmä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨æ–‡å­—é€²åº¦é¡¯ç¤º")
    else:
        print(f"[INFO] ä½¿ç”¨é€²åº¦æ¢é¡¯ç¤ºè™•ç†é€²åº¦")
    
    with open(args.out, "w", encoding="utf-8") as fout:
        if args.batch_mode:
            # æ‰¹é‡ä¸Šä¸‹æ–‡æ¨¡å¼
            print(f"[INFO] ä½¿ç”¨æ‰¹é‡ä¸Šä¸‹æ–‡è™•ç†æ¨¡å¼")
            results = _batch_process_with_context(chunks, kw_lang, args.max_chars)
            for record in results:
                fout.write(json.dumps(record, ensure_ascii=False) + "\n")
            
        elif args.workers == 1:
            # é †åºè™•ç† - æ”¹é€²ç‰ˆé€²åº¦æ¢
            start_time = time.time()
            error_count_live = 0
            
            if TQDM_AVAILABLE:
                # ç°¡åŒ–çš„é€²åº¦æ¢
                with tqdm(enumerate(chunks), 
                         total=len(chunks), 
                         desc="é—œéµå­—ç”Ÿæˆ-é †åº",
                         unit="chunk") as progress:
                    
                    for i, rec in progress:
                        _, outrec = _one_job(i, rec, kw_lang=kw_lang, max_chars=args.max_chars)
                        fout.write(json.dumps(outrec, ensure_ascii=False) + "\n")
                        
                        # æ›´æ–°éŒ¯èª¤çµ±è¨ˆ
                        if outrec.get("has_error", False):
                            error_count_live += 1
                        
                        # æ¯10å€‹æ›´æ–°ä¸€æ¬¡æè¿°
                        if (i + 1) % 10 == 0:
                            success_count = (i + 1) - error_count_live
                            progress.set_description(f"é—œéµå­—ç”Ÿæˆ-é †åº [âœ“{success_count} âœ—{error_count_live}]")
            else:
                # ç„¡é€²åº¦æ¢ç‰ˆæœ¬
                for i, rec in enumerate(chunks):
                    _, outrec = _one_job(i, rec, kw_lang=kw_lang, max_chars=args.max_chars)
                    fout.write(json.dumps(outrec, ensure_ascii=False) + "\n")
                    
                    if outrec.get("has_error", False):
                        error_count_live += 1
                        
                    # æ¯50å€‹é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                    if (i + 1) % 50 == 0:
                        progress_pct = ((i + 1) / len(chunks)) * 100
                        success_count = (i + 1) - error_count_live
                        print(f"[PROGRESS] {progress_pct:.1f}% ({i+1}/{len(chunks)}) "
                              f"æˆåŠŸ:{success_count} éŒ¯èª¤:{error_count_live}")
            
            elapsed = time.time() - start_time
            chunks_per_sec = len(chunks) / elapsed if elapsed > 0 else 0
            print(f"[PERF] é †åºè™•ç†å®Œæˆï¼è€—æ™‚: {elapsed:.2f}s, é€Ÿåº¦: {chunks_per_sec:.1f} chunks/s")
                
        else:
            # é«˜æ•ˆä¸¦è¡Œè™•ç† - 32 workersæœ€ä½³åŒ–
            start_time = time.time()
            with ThreadPoolExecutor(max_workers=args.workers) as ex:
                futures = []
                for i, rec in enumerate(chunks):
                    futures.append(ex.submit(_one_job, i, rec, kw_lang, args.max_chars))
                
                results = []
                processed_count = 0
                error_count_live = 0
                
                if TQDM_AVAILABLE:
                    # ä½¿ç”¨ç°¡åŒ–çš„ tqdm é¿å…æ ¼å¼å•é¡Œ
                    print(f"[INFO] é–‹å§‹ä¸¦è¡Œè™•ç† {len(chunks)} å€‹chunks (æœ‰é€²åº¦æ¢)...")
                    
                    with tqdm(total=len(futures), 
                             desc=f"é—œéµå­—ç”Ÿæˆ-{args.workers}w", 
                             unit="chunk") as pbar:
                        
                        for fut in as_completed(futures):
                            _, outrec = fut.result()
                            results.append(outrec)
                            processed_count += 1
                            
                            # æ›´æ–°éŒ¯èª¤çµ±è¨ˆ
                            if outrec.get("has_error", False):
                                error_count_live += 1
                            
                            # æ¯10å€‹æ›´æ–°ä¸€æ¬¡æè¿°
                            if processed_count % 10 == 0:
                                success_count = processed_count - error_count_live
                                pbar.set_description(f"é—œéµå­—ç”Ÿæˆ-{args.workers}w [âœ“{success_count} âœ—{error_count_live}]")
                            
                            pbar.update(1)
                else:
                    # ç„¡é€²åº¦æ¢ç‰ˆæœ¬ - æä¾›æ¸…æ™°çš„æ–‡å­—é€²åº¦
                    print(f"[INFO] é–‹å§‹ä¸¦è¡Œè™•ç† {len(chunks)} å€‹chunks...")
                    last_update = 0
                    
                    for fut in as_completed(futures):
                        _, outrec = fut.result()
                        results.append(outrec)
                        processed_count += 1
                        
                        if outrec.get("has_error", False):
                            error_count_live += 1
                        
                        # æ¯è™•ç†å®Œ50å€‹æˆ–æ¯5ç§’é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                        current_time = time.time()
                        if processed_count % 50 == 0 or (current_time - last_update) > 5:
                            success_count = processed_count - error_count_live
                            progress_pct = (processed_count / len(chunks)) * 100
                            elapsed = current_time - start_time
                            rate = processed_count / elapsed if elapsed > 0 else 0
                            
                            print(f"[PROGRESS] {progress_pct:5.1f}% ({processed_count:3d}/{len(chunks)}) "
                                  f"[{rate:4.1f} chunk/s] æˆåŠŸ:{success_count:3d} éŒ¯èª¤:{error_count_live:2d}")
                            last_update = current_time
                
                # æŒ‰chunk_idé †åºæ’åºè¼¸å‡º
                results.sort(key=lambda x: x.get('chunk_id', 0))
                for record in results:
                    fout.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            elapsed = time.time() - start_time
            chunks_per_sec = len(chunks) / elapsed if elapsed > 0 else 0
            print(f"[PERF] è™•ç†å®Œæˆï¼è€—æ™‚: {elapsed:.2f}s, é€Ÿåº¦: {chunks_per_sec:.1f} chunks/s")
            print(f"[PERF] å¹³å‡æ¯workerè™•ç†: {len(chunks)/args.workers:.1f} chunks")

    print(f"[OK] å·²å¯«å…¥ {len(chunks)} æ¢å®Œæ•´è¨˜éŒ„åˆ° {args.out}")
    
    # é¡¯ç¤ºè™•ç†çµ±è¨ˆ - åŒ…å«æ–‡å­—å…§å®¹çµ±è¨ˆ
    content_types = {}
    error_count = 0
    keyword_quality_stats = {"good": 0, "fallback": 0, "error": 0}
    text_length_stats = []
    total_keywords = 0
    
    with open(args.out, "r", encoding="utf-8") as f:
        for line in f:
            try:
                record = json.loads(line)
                content_type = record.get("content_type", "unknown")
                content_types[content_type] = content_types.get(content_type, 0) + 1
                
                # çµ±è¨ˆæ–‡å­—é•·åº¦
                text_length = record.get("text_length", 0)
                if text_length > 0:
                    text_length_stats.append(text_length)
                
                # çµ±è¨ˆé—œéµå­—æ•¸é‡
                keywords_count = record.get("keywords_count", 0)
                total_keywords += keywords_count
                
                if record.get("has_error", False):
                    error_count += 1
                    keyword_quality_stats["error"] += 1
                else:
                    keywords = record.get("keywords", [])
                    # æª¢æŸ¥é—œéµå­—å“è³ª
                    if any("fallback" in kw or "error" in kw or "supplementary" in kw for kw in keywords):
                        keyword_quality_stats["fallback"] += 1
                    else:
                        keyword_quality_stats["good"] += 1
            except:
                pass
    
    # æ–‡å­—é•·åº¦çµ±è¨ˆ
    if text_length_stats:
        avg_length = sum(text_length_stats) / len(text_length_stats)
        min_length = min(text_length_stats)
        max_length = max(text_length_stats)
        print(f"[STATS] æ–‡å­—é•·åº¦: å¹³å‡={avg_length:.0f}, ç¯„åœ={min_length}-{max_length} å­—å…ƒ")
    
    print(f"[STATS] å…§å®¹é¡å‹åˆ†å¸ƒ: {content_types}")
    print(f"[STATS] é—œéµå­—å“è³ª: å„ªç§€({keyword_quality_stats['good']}) | "
          f"å¾Œå‚™({keyword_quality_stats['fallback']}) | éŒ¯èª¤({keyword_quality_stats['error']})")
    print(f"[STATS] ç¸½é—œéµå­—æ•¸: {total_keywords} å€‹ (å¹³å‡ {total_keywords/len(chunks):.1f} å€‹/chunk)")
    
    if error_count > 0:
        print(f"[WARN] è™•ç†éç¨‹ä¸­æœ‰ {error_count} å€‹éŒ¯èª¤")
    
    # æˆåŠŸç‡çµ±è¨ˆ
    success_rate = (len(chunks) - error_count) / len(chunks) * 100 if len(chunks) > 0 else 0
    print(f"[SUMMARY] æˆåŠŸè™•ç†ç‡: {success_rate:.1f}% ({len(chunks)-error_count}/{len(chunks)})")
    
    # è¼¸å‡ºæª”æ¡ˆæ ¼å¼èªªæ˜
    print(f"\nğŸ“‹ è¼¸å‡ºæª”æ¡ˆæ ¼å¼èªªæ˜:")
    print(f"  âœ… åŒ…å«å®Œæ•´chunksæ–‡å­—å…§å®¹ (textå­—æ®µ)")
    print(f"  âœ… åŒ…å«ç”Ÿæˆçš„é—œéµå­— (keywordså­—æ®µ)")  
    print(f"  âœ… åŒ…å«ä¾†æºå’Œé é¢ä¿¡æ¯ (source, pageå­—æ®µ)")
    print(f"  âœ… åŒ…å«å…§å®¹çµ±è¨ˆä¿¡æ¯ (text_length, keywords_countå­—æ®µ)")
    print(f"  âœ… å¯ç›´æ¥ç”¨æ–¼å¾ŒçºŒå•é¡Œç”Ÿæˆæµç¨‹")
    
    return success_rate > 90  # è¿”å›æ˜¯å¦æˆåŠŸ


if __name__ == "__main__":
    main()