# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆèªè¨€æ„ŸçŸ¥é—œéµå­—æç¤ºè©æ¨¡çµ„ (prompts.py)
ğŸ†• ä¸»è¦æ–°å¢ï¼šå®Œæ•´æ—¥æ–‡é—œéµå­—ç”Ÿæˆæ”¯æ´
ğŸ”§ ä¸»è¦ä¿®å¾©ï¼šæ”¹å–„è‹±æ–‡é—œéµå­—æç¤ºè©ç²¾ç¢ºåº¦

æ”¯æ´èªè¨€ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€æ··åˆèªè¨€
"""

import re
from typing import List, Optional
from textwrap import dedent


def build_keywords_prompt(text: str, n: int = 3, lang: str = "en") -> str:
    """
    æ”¹é€²ç‰ˆé—œéµå­—ç”Ÿæˆæç¤ºè© - æ”¯æ´ä¸­è‹±æ—¥ä¸‰èª
    
    Args:
        text: è¦åˆ†æçš„æ–‡å­—
        n: éœ€è¦çš„é—œéµå­—æ•¸é‡
        lang: ç›®æ¨™èªè¨€ ("zh", "en", "ja", "mixed", "auto")
    """
    
    # æ ¹æ“šlangåƒæ•¸é¸æ“‡åˆé©çš„æç¤ºè©
    if lang.lower() in ["zh", "chinese"]:
        return build_chinese_keywords_prompt(text, n)
    elif lang.lower() in ["en", "english"]:
        return build_english_keywords_prompt(text, n)
    elif lang.lower() in ["ja", "japanese"]:  # ğŸ†• æ—¥æ–‡æ”¯æ´
        return build_japanese_keywords_prompt(text, n)
    elif lang.lower() in ["mixed", "bilingual", "multilingual"]:
        return build_mixed_keywords_prompt(text, n)
    else:
        # autoæ¨¡å¼ï¼šæ ¹æ“šæ–‡æœ¬å…§å®¹è‡ªå‹•é¸æ“‡
        return build_auto_keywords_prompt(text, n)


def build_chinese_keywords_prompt(text: str, n: int = 3) -> str:
    """ä¸­æ–‡é—œéµå­—ç”Ÿæˆæç¤ºè©"""
    return dedent(f"""
    ä½ æ˜¯å°ˆæ¥­çš„ä¸­æ–‡é—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ä¸­æ–‡å…§å®¹æå– {n} å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚

    âš ï¸ åš´æ ¼è¦æ±‚ï¼š
    1. é—œéµå­—å¿…é ˆ**ç›´æ¥å‡ºç¾**åœ¨æ–‡æœ¬ä¸­æˆ–èˆ‡å…§å®¹**å¯†åˆ‡ç›¸é—œ**
    2. ä½¿ç”¨æº–ç¢ºçš„ä¸­æ–‡è©å½™ï¼ˆ1-4å€‹å­—ç‚ºä½³ï¼‰
    3. å„ªå…ˆç´šï¼šå°ˆæœ‰åè© > æŠ€è¡“è¡“èª > æ ¸å¿ƒæ¦‚å¿µ
    4. **çµ•å°ç¦æ­¢**ï¼šä¸ç›¸é—œçš„è©å½™ï¼ˆä¾‹å¦‚ï¼šå…§å®¹è¬›Darwinï¼Œä¸è¦ç”Ÿæˆ"åŠå°é«”"ï¼‰
    5. **çµ•å°ç¦æ­¢**ï¼šéæ–¼æ³›ç”¨çš„è©ï¼ˆå¦‚ï¼šæŠ€è¡“ã€ç™¼å±•ã€å¸‚å ´ã€ç³»çµ±ã€ç”¢å“ï¼‰
    6. **çµ•å°ç¦æ­¢**ï¼šå¹´ä»½ã€æ—¥æœŸæˆ–æ™‚é–“ç›¸é—œè©å½™
    7. è¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯JSONæ•¸çµ„ï¼š["è©1", "è©2", "è©3"]

    âœ… è‰¯å¥½ç¤ºä¾‹ï¼š
    - å…§å®¹æåŠ"å°ç©é›»" â†’ é—œéµå­—åŒ…å«["å°ç©é›»"]
    - å…§å®¹æåŠ"ç‰©ç¨®æ¼”åŒ–" â†’ é—œéµå­—åŒ…å«["æ¼”åŒ–", "é”çˆ¾æ–‡"]
    - å…§å®¹æåŠ"åŠå°é«”ç”¢æ¥­" â†’ é—œéµå­—åŒ…å«["åŠå°é«”"]

    âŒ éŒ¯èª¤ç¤ºä¾‹ï¼š
    - å…§å®¹è¬›é”çˆ¾æ–‡ â†’ éŒ¯èª¤ç”Ÿæˆ["åŠå°é«”"]ï¼ˆå®Œå…¨ç„¡é—œï¼‰
    - å…§å®¹è¬›ç”Ÿç‰©å­¸ â†’ éŒ¯èª¤ç”Ÿæˆ["äººå·¥æ™ºæ…§", "æŠ€è¡“"]ï¼ˆä¸ç›¸é—œï¼‰

    æ–‡æœ¬å…§å®¹ï¼š
    {text}

    è«‹ä»”ç´°é–±è®€ä¸Šè¿°å…§å®¹ï¼Œæå–**ç¢ºå¯¦å‡ºç¾**åœ¨æ–‡æœ¬ä¸­çš„é—œéµå­—ï¼š
    """).strip()


def build_english_keywords_prompt(text: str, n: int = 3) -> str:
    """è‹±æ–‡é—œéµå­—ç”Ÿæˆæç¤ºè© - ä¿®å¾©ç‰ˆ"""
    return dedent(f"""
    You are a professional English keyword extraction expert. Extract exactly {n} most important keywords from the following English content.

    âš ï¸ CRITICAL REQUIREMENTS:
    1. Keywords MUST be **directly present** or **clearly related** to the actual content
    2. Use precise English terms (1-4 words maximum)
    3. Priority: Specific proper nouns > Technical terms > Core concepts
    4. **ABSOLUTELY FORBIDDEN**: Irrelevant terms (e.g., if content is about Darwin, DON'T generate "semiconductor")
    5. **ABSOLUTELY FORBIDDEN**: Generic words (technology, development, market, system, product, important, content)
    6. **ABSOLUTELY FORBIDDEN**: Years, dates, or temporal references
    7. **REQUIRED**: Output ONLY JSON array format: ["term1", "term2", "term3"]

    âœ… EXCELLENT Examples:
    - Content mentions "TSMC" â†’ keywords include ["TSMC"]
    - Content mentions "species evolution" â†’ keywords include ["evolution", "Darwin"]  
    - Content mentions "semiconductor industry" â†’ keywords include ["semiconductor", "industry"]

    âŒ FORBIDDEN Examples:
    - Content about Darwin â†’ WRONG to generate ["semiconductor"] (completely unrelated)
    - Content about biology â†’ WRONG to generate ["AI", "technology"] (irrelevant)

    Text content:
    {text}

    Extract keywords that **actually appear** in the text. Output JSON array only:
    """).strip()


def build_japanese_keywords_prompt(text: str, n: int = 3) -> str:
    """ğŸ†• æ—¥æ–‡é—œéµå­—ç”Ÿæˆæç¤ºè©"""
    return dedent(f"""
    ã‚ãªãŸã¯å°‚é–€çš„ãªæ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ—¥æœ¬èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æœ€ã‚‚é‡è¦ãª {n} å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    âš ï¸ å³æ ¼ãªè¦æ±‚ï¼š
    1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æ–‡ç« ä¸­ã«**ç›´æ¥ç¾ã‚Œã‚‹**ã‹ã€å†…å®¹ã¨**å¯†æ¥ã«é–¢é€£**ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    2. æ­£ç¢ºãªæ—¥æœ¬èªç”¨èªã‚’ä½¿ç”¨ï¼ˆ1-6æ–‡å­—ãŒæœ€é©ï¼‰
    3. å„ªå…ˆé †ä½ï¼šå›ºæœ‰åè© > æŠ€è¡“ç”¨èª > æ ¸å¿ƒæ¦‚å¿µ
    4. **çµ¶å¯¾ç¦æ­¢**ï¼šç„¡é–¢ä¿‚ãªç”¨èªï¼ˆä¾‹ï¼šãƒ€ãƒ¼ã‚¦ã‚£ãƒ³ã®å†…å®¹ã§ã€ŒåŠå°ä½“ã€ã‚’ç”Ÿæˆã—ãªã„ï¼‰
    5. **çµ¶å¯¾ç¦æ­¢**ï¼šæ±ç”¨çš„ã™ãã‚‹èªï¼ˆæŠ€è¡“ã€ç™ºå±•ã€å¸‚å ´ã€ã‚·ã‚¹ãƒ†ãƒ ã€è£½å“ï¼‰
    6. **çµ¶å¯¾ç¦æ­¢**ï¼šå¹´ã€æ—¥ä»˜ã€æ™‚é–“é–¢é€£ã®èªå½™
    7. å‡ºåŠ›å½¢å¼ã¯JSONé…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š["ç”¨èª1", "ç”¨èª2", "ç”¨èª3"]

    âœ… è‰¯ã„ä¾‹ï¼š
    - å†…å®¹ã«ã€ŒTSMCã€ãŒå«ã¾ã‚Œã‚‹ â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«["TSMC"]ã‚’å«ã‚€
    - å†…å®¹ã«ã€Œç¨®ã®é€²åŒ–ã€ãŒå«ã¾ã‚Œã‚‹ â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«["é€²åŒ–", "ãƒ€ãƒ¼ã‚¦ã‚£ãƒ³"]ã‚’å«ã‚€
    - å†…å®¹ã«ã€ŒåŠå°ä½“ç”£æ¥­ã€ãŒå«ã¾ã‚Œã‚‹ â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«["åŠå°ä½“"]ã‚’å«ã‚€

    âŒ é–“é•ã£ãŸä¾‹ï¼š
    - ãƒ€ãƒ¼ã‚¦ã‚£ãƒ³ã®å†…å®¹ â†’ é–“é•ã£ã¦["åŠå°ä½“"]ã‚’ç”Ÿæˆï¼ˆå…¨ãç„¡é–¢ä¿‚ï¼‰
    - ç”Ÿç‰©å­¦ã®å†…å®¹ â†’ é–“é•ã£ã¦["AI", "æŠ€è¡“"]ã‚’ç”Ÿæˆï¼ˆé–¢ä¿‚ãªã—ï¼‰

    ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ï¼š
    {text}

    ä¸Šè¨˜ã®å†…å®¹ã‚’æ³¨æ„æ·±ãèª­ã¿ã€**å®Ÿéš›ã«ç¾ã‚Œã‚‹**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
    """).strip()


def build_mixed_keywords_prompt(text: str, n: int = 3) -> str:
    """ä¸­è‹±æ—¥æ··åˆå†…å®¹é—œéµå­—ç”Ÿæˆæç¤ºè©"""
    return dedent(f"""
    ã‚ãªãŸã¯å¤šè¨€èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¸­è‹±æ—¥æ··åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æœ€ã‚‚é‡è¦ãª {n} å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    You are a professional multilingual keyword extraction expert. Extract {n} most important keywords from the following mixed Chinese-English-Japanese content.
    ä½ æ˜¯å°ˆæ¥­çš„å¤šèªè¨€é—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ä¸­è‹±æ—¥æ··åˆå…§å®¹æå– {n} å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚

    âš ï¸ åš´æ ¼è¦æ±‚ / STRICT REQUIREMENTS / å³æ ¼ãªè¦æ±‚ï¼š
    1. é—œéµå­—å¿…é ˆ**ç›´æ¥å‡ºç¾**åœ¨æ–‡æœ¬ä¸­æˆ–èˆ‡å…§å®¹**å¯†åˆ‡ç›¸é—œ** / Keywords MUST be **directly present** or **clearly related** / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯**ç›´æ¥ç¾ã‚Œã‚‹**ã‹**å¯†æ¥ã«é–¢é€£**ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    2. å¯ä½¿ç”¨ä¸­æ–‡ã€è‹±æ–‡æˆ–æ—¥æ–‡è©å½™ï¼Œé¸æ“‡æœ€æº–ç¢ºçš„èªè¨€ / Use Chinese, English, or Japanese terms, whichever is most precise / ä¸­å›½èªã€è‹±èªã€æ—¥æœ¬èªã®ç”¨èªã‚’ä½¿ç”¨ã—ã€æœ€ã‚‚æ­£ç¢ºãªè¨€èªã‚’é¸æŠ
    3. å„ªå…ˆç´šï¼šå°ˆæœ‰åè© > æŠ€è¡“è¡“èª > æ ¸å¿ƒæ¦‚å¿µ / Priority: Proper nouns > Technical terms > Core concepts / å„ªå…ˆé †ä½ï¼šå›ºæœ‰åè© > æŠ€è¡“ç”¨èª > æ ¸å¿ƒæ¦‚å¿µ
    4. **çµ•å°ç¦æ­¢**ä¸ç›¸é—œè©å½™ / **ABSOLUTELY FORBIDDEN**: Irrelevant terms / **çµ¶å¯¾ç¦æ­¢**ï¼šç„¡é–¢ä¿‚ãªç”¨èª
    5. **çµ•å°ç¦æ­¢**éæ–¼æ³›ç”¨çš„è© / **ABSOLUTELY FORBIDDEN**: Generic words / **çµ¶å¯¾ç¦æ­¢**ï¼šæ±ç”¨çš„ã™ãã‚‹èª
    6. è¼¸å‡ºæ ¼å¼ï¼šJSONæ•¸çµ„ / Output format: JSON array / å‡ºåŠ›å½¢å¼ï¼šJSONé…åˆ— ["è©1/term1/ç”¨èª1", "è©2/term2/ç”¨èª2", "è©3/term3/ç”¨èª3"]

    âœ… è‰¯å¥½ç¤ºä¾‹ / GOOD Examples / è‰¯ã„ä¾‹ï¼š
    - å…§å®¹åŒ…å«"TSMCå°ç©é›»" â†’ ["TSMC", "å°ç©é›»", "åŠå°ä½“"]
    - Content contains "äººå·¥æ™ºæ…§AI" â†’ ["äººå·¥æ™ºæ…§", "AI", "machine learning"]
    - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«"Displayãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤CES"ãŒå«ã¾ã‚Œã‚‹ â†’ ["Display", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤", "CES"]

    âŒ éŒ¯èª¤ç¤ºä¾‹ / BAD Examples / é–“é•ã£ãŸä¾‹ï¼š
    - å…§å®¹è¬›åŠå°é«”ï¼Œç”Ÿæˆ["ç”Ÿç‰©å­¦"] / Content about semiconductors, generating ["biology"] / åŠå°ä½“ã®å†…å®¹ã§["ç”Ÿç‰©å­¦"]ã‚’ç”Ÿæˆ

    æ–‡æœ¬å…§å®¹ / Text content / ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ï¼š
    {text}

    è«‹æå–**ç¢ºå¯¦ç›¸é—œ**çš„é—œéµå­— / Extract **actually relevant** keywords / **å®Ÿéš›ã«é–¢é€£ã™ã‚‹**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼š
    """).strip()


def build_auto_keywords_prompt(text: str, n: int = 3) -> str:
    """è‡ªå‹•æª¢æ¸¬èªè¨€ä¸¦ç”Ÿæˆé—œéµå­—çš„æç¤ºè© - æ”¯æ´æ—¥æ–‡"""
    # æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))  # å¹³å‡å+ç‰‡å‡å
    total_chars = chinese_chars + english_chars + japanese_chars
    
    if total_chars == 0:
        # ç„¡æ³•æª¢æ¸¬ï¼Œä½¿ç”¨è‹±æ–‡æç¤ºè©
        return build_english_keywords_prompt(text, n)
    
    chinese_ratio = chinese_chars / total_chars
    english_ratio = english_chars / total_chars
    japanese_ratio = japanese_chars / total_chars
    
    # æ—¥æ–‡å„ªå…ˆæª¢æ¸¬
    if japanese_ratio > 0.2:
        return build_japanese_keywords_prompt(text, n)
    elif chinese_ratio > 0.6:
        return build_chinese_keywords_prompt(text, n)
    elif english_ratio > 0.6:
        return build_english_keywords_prompt(text, n)
    else:
        return build_mixed_keywords_prompt(text, n)


def build_contextual_keywords_prompt(text: str, context_keywords: List[str] = None, 
                                   n: int = 3, lang: str = "en") -> str:
    """
    ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—œéµå­—ç”Ÿæˆ - æ”¯æ´ä¸­è‹±æ—¥ä¸‰èª
    
    Args:
        text: è¦åˆ†æçš„æ–‡å­—
        context_keywords: å·²æœ‰çš„é—œéµå­—åˆ—è¡¨ï¼ˆä¾†è‡ªç›¸é„°chunksæˆ–ç›¸åŒæ–‡ä»¶ï¼‰
        n: éœ€è¦çš„é—œéµå­—æ•¸é‡
        lang: èªè¨€
    """
    
    context_info = ""
    if context_keywords:
        context_list = ", ".join(context_keywords[:10])  # é¿å…promptéé•·
        if lang.lower() in ["zh", "chinese"]:
            context_info = f"\nå·²çŸ¥ç›¸é—œé—œéµå­—ï¼š{context_list}\nè«‹ç”Ÿæˆ**äº’è£œä¸”ä¸é‡è¤‡**çš„é—œéµå­—ã€‚\n"
        elif lang.lower() in ["ja", "japanese"]:
            context_info = f"\næ—¢çŸ¥ã®é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š{context_list}\n**è£œå®Œçš„ã§é‡è¤‡ã—ãªã„**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
        else:
            context_info = f"\nExisting related keywords: {context_list}\nGenerate **complementary and non-duplicate** keywords.\n"
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # åœ¨è¦æ±‚éƒ¨åˆ†æ’å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯
    if lang.lower() in ["zh", "chinese"]:
        context_section = f"{context_info}ç‰¹åˆ¥æ³¨æ„ï¼šèˆ‡å·²çŸ¥é—œéµå­—å½¢æˆä¸»é¡Œå®Œæ•´æ€§ï¼Œä½†ä¸è¦é‡è¤‡ã€‚"
    elif lang.lower() in ["ja", "japanese"]:
        context_section = f"{context_info}ç‰¹åˆ¥æ³¨æ„ï¼šæ—¢çŸ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ä¸»é¡Œã®æ•´åˆæ€§ã‚’ä¿ã¡ãªãŒã‚‰ã€é‡è¤‡ã‚’é¿ã‘ã‚‹ã€‚"
    else:
        context_section = f"{context_info}Focus on: Terms that complement existing keywords but provide new information."
    
    return base_prompt.replace("è¦æ±‚ï¼š", f"è¦æ±‚ï¼š\n{context_section}\nåŸè¦æ±‚ï¼š") if "è¦æ±‚ï¼š" in base_prompt else \
           base_prompt.replace("å³æ ¼ãªè¦æ±‚ï¼š", f"å³æ ¼ãªè¦æ±‚ï¼š\n{context_section}\nå…ƒã®è¦æ±‚ï¼š") if "å³æ ¼ãªè¦æ±‚ï¼š" in base_prompt else \
           base_prompt.replace("Requirements:", f"Requirements:\n{context_section}\nOriginal requirements:")


def build_adaptive_keywords_prompt(text: str, content_type: str = "auto", 
                                 n: int = 3, lang: str = "en") -> str:
    """
    è‡ªé©æ‡‰é—œéµå­—ç”Ÿæˆ - æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´ç­–ç•¥ï¼Œæ”¯æ´æ—¥æ–‡
    
    Args:
        content_type: "technical", "business", "academic", "auto"
    """
    
    # è‡ªå‹•æª¢æ¸¬å…§å®¹é¡å‹
    if content_type == "auto":
        text_lower = text.lower()
        japanese_text = text  # ä¿æŒåŸæ–‡ç”¨æ–¼æ—¥æ–‡æª¢æ¸¬
        
        tech_indicators = ["patent", "algorithm", "specification", "protocol", "display", "semiconductor", "ç‰¹è¨±", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ä»•æ§˜", "ãƒ—ãƒ­ãƒˆã‚³ãƒ«", "ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤", "åŠå°ä½“"]
        business_indicators = ["market", "revenue", "business", "strategy", "company", "investment", "å¸‚å ´", "åç›Š", "ãƒ“ã‚¸ãƒã‚¹", "æˆ¦ç•¥", "ä¼šç¤¾", "æŠ•è³‡"]
        academic_indicators = ["research", "study", "analysis", "methodology", "experiment", "ç ”ç©¶", "åˆ†æ", "æ–¹æ³•è«–", "å®Ÿé¨“", "è«–æ–‡"]
        
        if any(indicator in text_lower or indicator in japanese_text for indicator in tech_indicators):
            content_type = "technical"
        elif any(indicator in text_lower or indicator in japanese_text for indicator in business_indicators):
            content_type = "business"  
        elif any(indicator in text_lower or indicator in japanese_text for indicator in academic_indicators):
            content_type = "academic"
        else:
            content_type = "general"
    
    # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´æç¤ºè©
    specialized_instructions = {
        "technical": {
            "zh": "å°ˆæ³¨æ–¼æŠ€è¡“è¦æ ¼ã€ç”¢å“å‹è™Ÿã€è£½ç¨‹åƒæ•¸ã€å”è­°æ¨™æº–ã€å°ˆåˆ©æŠ€è¡“",
            "en": "Focus on technical specs, product models, process parameters, protocol standards, patent technologies",
            "ja": "æŠ€è¡“ä»•æ§˜ã€è£½å“ãƒ¢ãƒ‡ãƒ«ã€ãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ—ãƒ­ãƒˆã‚³ãƒ«æ¨™æº–ã€ç‰¹è¨±æŠ€è¡“ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "business": {
            "zh": "å°ˆæ³¨æ–¼å…¬å¸åç¨±ã€å•†æ¥­æ¨¡å¼ã€å¸‚å ´ç­–ç•¥ã€ç”¢æ¥­è¶¨å‹¢ã€æŠ•è³‡ç›¸é—œ",
            "en": "Focus on company names, business models, market strategies, industry trends, investment aspects",
            "ja": "ä¼šç¤¾åã€ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã€å¸‚å ´æˆ¦ç•¥ã€æ¥­ç•Œãƒˆãƒ¬ãƒ³ãƒ‰ã€æŠ•è³‡é–¢é€£ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "academic": {
            "zh": "å°ˆæ³¨æ–¼ç ”ç©¶æ–¹æ³•ã€ç†è«–æ¦‚å¿µã€å¯¦é©—çµæœã€å­¸è¡“è¡“èªã€ä½œè€…å§“å",
            "en": "Focus on research methods, theoretical concepts, experimental results, academic terms, author names",
            "ja": "ç ”ç©¶æ–¹æ³•ã€ç†è«–æ¦‚å¿µã€å®Ÿé¨“çµæœã€å­¦è¡“ç”¨èªã€è‘—è€…åã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "general": {
            "zh": "å°ˆæ³¨æ–¼æ ¸å¿ƒä¸»é¡Œã€é—œéµæ¦‚å¿µã€é‡è¦å¯¦é«”ã€ä¸»è¦äººç‰©",
            "en": "Focus on core topics, key concepts, important entities, main figures",
            "ja": "æ ¸å¿ƒä¸»é¡Œã€é‡è¦æ¦‚å¿µã€é‡è¦ãªå®Ÿä½“ã€ä¸»è¦äººç‰©ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        }
    }
    
    lang_key = "ja" if lang.lower() in ["ja", "japanese"] else ("zh" if lang.lower() in ["zh", "chinese"] else "en")
    additional_instruction = specialized_instructions[content_type][lang_key]
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # æ’å…¥å°ˆé–€åŒ–æŒ‡ä»¤
    if lang.lower() in ["zh", "chinese"]:
        specialized_section = f"å…§å®¹é¡å‹ï¼š{content_type.upper()}\nç‰¹åŒ–è¦æ±‚ï¼š{additional_instruction}\n\n"
        return base_prompt.replace("ä½ æ˜¯ä¸€å€‹", f"{specialized_section}ä½ æ˜¯ä¸€å€‹")
    elif lang.lower() in ["ja", "japanese"]:
        specialized_section = f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ï¼š{content_type.upper()}\nç‰¹åŒ–è¦æ±‚ï¼š{additional_instruction}\n\n"
        return base_prompt.replace("ã‚ãªãŸã¯", f"{specialized_section}ã‚ãªãŸã¯")
    else:
        specialized_section = f"Content type: {content_type.upper()}\nSpecialized focus: {additional_instruction}\n\n"
        return base_prompt.replace("You are a", f"{specialized_section}You are a")


def build_quality_enhanced_keywords_prompt(text: str, quality_score: float = 0.5, 
                                         n: int = 3, lang: str = "en") -> str:
    """
    åŸºæ–¼å“è³ªåˆ†æ•¸çš„å¢å¼·æç¤ºè© - æ”¯æ´æ—¥æ–‡
    
    Args:
        quality_score: chunkå“è³ªåˆ†æ•¸ (0-1)
    """
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    if quality_score > 0.8:
        # é«˜å“è³ªchunkï¼Œè¦æ±‚æ›´å¤šé—œéµå­—å’Œæ›´é«˜ç²¾åº¦
        if lang.lower() in ["zh", "chinese"]:
            enhancement = "\nâ­ é«˜å“è³ªå…§å®¹æª¢æ¸¬ï¼šè«‹æå–æ›´å¤šå°ˆæ¥­è¡“èªå’Œæ ¸å¿ƒæ¦‚å¿µï¼Œç¢ºä¿é—œéµå­—çš„å°ˆæ¥­æ€§å’Œæº–ç¢ºæ€§ã€‚"
        elif lang.lower() in ["ja", "japanese"]:
            enhancement = "\nâ­ é«˜å“è³ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºï¼šã‚ˆã‚Šå¤šãã®å°‚é–€ç”¨èªã¨æ ¸å¿ƒæ¦‚å¿µã‚’æŠ½å‡ºã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å°‚é–€æ€§ã¨æ­£ç¢ºæ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚"
        else:
            enhancement = "\nâ­ High-quality content detected: Extract more professional terms and core concepts, ensure keyword precision and accuracy."
        
        # å¢åŠ é—œéµå­—æ•¸é‡
        n_enhanced = min(n + 1, 6)
        base_prompt = base_prompt.replace(f"{n} å€‹", f"{n_enhanced} å€‹").replace(f"extract {n}", f"extract {n_enhanced}").replace(f"{n} å€‹ã®", f"{n_enhanced} å€‹ã®")
        
    elif quality_score < 0.6:
        # ä½å“è³ªchunkï¼Œå°ˆæ³¨æ–¼åŸºæœ¬æ¦‚å¿µ
        if lang.lower() in ["zh", "chinese"]:
            enhancement = "\nâš ï¸ å…§å®¹å“è³ªè¼ƒä½ï¼šè«‹å°ˆæ³¨æ–¼æœ€åŸºæœ¬å’Œæœ€æ˜ç¢ºçš„æ¦‚å¿µï¼Œé¿å…è¤‡é›œæˆ–æ¨¡ç³Šçš„è¡“èªã€‚"
        elif lang.lower() in ["ja", "japanese"]:
            enhancement = "\nâš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªãŒä½ã„ï¼šæœ€ã‚‚åŸºæœ¬çš„ã§æ˜ç¢ºãªæ¦‚å¿µã«ç„¦ç‚¹ã‚’å½“ã¦ã€è¤‡é›‘ã¾ãŸã¯æ›–æ˜§ãªç”¨èªã‚’é¿ã‘ã¦ãã ã•ã„ã€‚"
        else:
            enhancement = "\nâš ï¸ Lower quality content: Focus on most basic and clear concepts, avoid complex or ambiguous terms."
    else:
        enhancement = ""
    
    return base_prompt + enhancement


def build_domain_specific_keywords_prompt(text: str, domain: str = "general", 
                                        n: int = 3, lang: str = "en") -> str:
    """
    é ˜åŸŸç‰¹å®šçš„é—œéµå­—æç¤ºè© - æ”¯æ´æ—¥æ–‡
    
    Args:
        domain: "medical", "legal", "technology", "finance", "academic", "general"
    """
    
    domain_instructions = {
        "medical": {
            "zh": "é†«å­¸é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼ç–¾ç—…åç¨±ã€è—¥ç‰©åç¨±ã€è§£å‰–è¡“èªã€æ²»ç™‚æ–¹æ³•ã€é†«å­¸æ¦‚å¿µ",
            "en": "Medical domain: Focus on disease names, drug names, anatomical terms, treatment methods, medical concepts",
            "ja": "åŒ»å­¦é ˜åŸŸå°‚ç”¨ï¼šç–¾æ‚£åã€è–¬å‰¤åã€è§£å‰–å­¦ç”¨èªã€æ²»ç™‚æ–¹æ³•ã€åŒ»å­¦æ¦‚å¿µã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "legal": {
            "zh": "æ³•å¾‹é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼æ³•å¾‹æ¢æ–‡ã€æ¡ˆä¾‹åç¨±ã€æ³•å¾‹æ¦‚å¿µã€ç¨‹åºè¡“èªã€æ³•é™¢åˆ¤æ±º",
            "en": "Legal domain: Focus on legal provisions, case names, legal concepts, procedural terms, court decisions",
            "ja": "æ³•å¾‹é ˜åŸŸå°‚ç”¨ï¼šæ³•å¾‹æ¡æ–‡ã€åˆ¤ä¾‹åã€æ³•çš„æ¦‚å¿µã€æ‰‹ç¶šãç”¨èªã€è£åˆ¤æ‰€åˆ¤æ±ºã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "technology": {
            "zh": "ç§‘æŠ€é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼æŠ€è¡“åç¨±ã€ç”¢å“å‹è™Ÿã€æŠ€è¡“æ¨™æº–ã€å…¬å¸åç¨±ã€å‰µæ–°æ¦‚å¿µ",
            "en": "Technology domain: Focus on technical names, product models, technical standards, company names, innovation concepts",
            "ja": "æŠ€è¡“é ˜åŸŸå°‚ç”¨ï¼šæŠ€è¡“åç§°ã€è£½å“ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“æ¨™æº–ã€ä¼šç¤¾åã€é©æ–°æ¦‚å¿µã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "finance": {
            "zh": "é‡‘èé ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼é‡‘èå·¥å…·ã€å¸‚å ´è¡“èªã€å…¬å¸åç¨±ã€æŠ•è³‡æ¦‚å¿µã€ç¶“æ¿ŸæŒ‡æ¨™",
            "en": "Finance domain: Focus on financial instruments, market terms, company names, investment concepts, economic indicators",
            "ja": "é‡‘èé ˜åŸŸå°‚ç”¨ï¼šé‡‘èå•†å“ã€å¸‚å ´ç”¨èªã€ä¼šç¤¾åã€æŠ•è³‡æ¦‚å¿µã€çµŒæ¸ˆæŒ‡æ¨™ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "academic": {
            "zh": "å­¸è¡“é ˜åŸŸå°ˆç”¨ï¼šå°ˆæ³¨æ–¼ç ”ç©¶ä¸»é¡Œã€ä½œè€…å§“åã€ç†è«–åç¨±ã€æ–¹æ³•è«–ã€å¯¦é©—çµæœ",
            "en": "Academic domain: Focus on research topics, author names, theory names, methodologies, experimental results",
            "ja": "å­¦è¡“é ˜åŸŸå°‚ç”¨ï¼šç ”ç©¶ä¸»é¡Œã€è‘—è€…åã€ç†è«–åã€æ–¹æ³•è«–ã€å®Ÿé¨“çµæœã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        },
        "general": {
            "zh": "é€šç”¨é ˜åŸŸï¼šå°ˆæ³¨æ–¼æ ¸å¿ƒæ¦‚å¿µã€é‡è¦å¯¦é«”ã€é—œéµè¡“èª",
            "en": "General domain: Focus on core concepts, important entities, key terms",
            "ja": "ä¸€èˆ¬é ˜åŸŸï¼šæ ¸å¿ƒæ¦‚å¿µã€é‡è¦ãªå®Ÿä½“ã€é‡è¦ç”¨èªã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹"
        }
    }
    
    lang_key = "ja" if lang.lower() in ["ja", "japanese"] else ("zh" if lang.lower() in ["zh", "chinese"] else "en")
    domain_instruction = domain_instructions.get(domain, domain_instructions["general"])[lang_key]
    
    base_prompt = build_keywords_prompt(text, n, lang)
    
    # æ’å…¥é ˜åŸŸç‰¹å®šæŒ‡ä»¤
    if lang.lower() in ["zh", "chinese"]:
        domain_section = f"ğŸ¯ {domain_instruction}\n\n"
        return base_prompt.replace("ä½ æ˜¯ä¸€å€‹", f"{domain_section}ä½ æ˜¯ä¸€å€‹")
    elif lang.lower() in ["ja", "japanese"]:
        domain_section = f"ğŸ¯ {domain_instruction}\n\n"
        return base_prompt.replace("ã‚ãªãŸã¯", f"{domain_section}ã‚ãªãŸã¯")
    else:
        domain_section = f"ğŸ¯ {domain_instruction}\n\n"
        return base_prompt.replace("You are a", f"{domain_section}You are a")


# ========== æ‰¹é‡è™•ç†ç›¸é—œæç¤ºè© ==========

def build_batch_keywords_prompt(chunks: List[dict], n: int = 3, lang: str = "en") -> str:
    """
    æ‰¹é‡é—œéµå­—ç”Ÿæˆçš„æç¤ºè© - æ”¯æ´æ—¥æ–‡
    å°ˆç‚ºæ‰¹é‡è™•ç†è¨­è¨ˆï¼Œè€ƒæ…®chunksé–“çš„é—œè¯æ€§
    """
    
    if lang.lower() in ["zh", "chinese"]:
        prompt_header = f"ä½ æ˜¯å°ˆæ¥­çš„æ‰¹é‡é—œéµå­—æå–å°ˆå®¶ã€‚è«‹ç‚ºä»¥ä¸‹ {len(chunks)} å€‹æ–‡æœ¬ç‰‡æ®µåˆ†åˆ¥ç”Ÿæˆ {n} å€‹é—œéµå­—ã€‚"
        requirements = """
âš ï¸ æ‰¹é‡è™•ç†è¦æ±‚ï¼š
1. æ¯å€‹ç‰‡æ®µçš„é—œéµå­—å¿…é ˆ**ç¨ç«‹ä¸”ç›¸é—œ**
2. è€ƒæ…®ç‰‡æ®µé–“çš„ä¸»é¡Œé€£è²«æ€§ï¼Œä½†é¿å…é‡è¤‡
3. ä½¿ç”¨æº–ç¢ºçš„ä¸­æ–‡è©å½™
4. è¼¸å‡ºæ ¼å¼ï¼š[["ç‰‡æ®µ1é—œéµå­—1", "ç‰‡æ®µ1é—œéµå­—2", ...], ["ç‰‡æ®µ2é—œéµå­—1", ...], ...]

æ–‡æœ¬ç‰‡æ®µï¼š
"""
        footer = "\nè«‹ç‚ºæ¯å€‹ç‰‡æ®µç”Ÿæˆé—œéµå­—ï¼š"
    elif lang.lower() in ["ja", "japanese"]:
        prompt_header = f"ã‚ãªãŸã¯å°‚é–€çš„ãªä¸€æ‹¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã® {len(chunks)} å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã«å¯¾ã—ã¦ã€ãã‚Œãã‚Œ {n} å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
        requirements = """
âš ï¸ ä¸€æ‹¬å‡¦ç†è¦æ±‚ï¼š
1. å„æ–­ç‰‡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯**ç‹¬ç«‹ã—ã¦é–¢é€£æ€§**ãŒã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
2. æ–­ç‰‡é–“ã®ä¸»é¡Œã®ä¸€è²«æ€§ã‚’è€ƒæ…®ã—ã¤ã¤ã€é‡è¤‡ã‚’é¿ã‘ã¦ãã ã•ã„
3. æ­£ç¢ºãªæ—¥æœ¬èªç”¨èªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
4. å‡ºåŠ›å½¢å¼ï¼š[["æ–­ç‰‡1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "æ–­ç‰‡1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", ...], ["æ–­ç‰‡2ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", ...], ...]

ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ï¼š
"""
        footer = "\nå„æ–­ç‰‡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š"
    else:
        prompt_header = f"You are a professional batch keyword extraction expert. Please generate {n} keywords for each of the following {len(chunks)} text chunks."
        requirements = """
âš ï¸ Batch processing requirements:
1. Keywords for each chunk must be **independent and relevant**
2. Consider thematic coherence between chunks but avoid duplication
3. Use precise terms
4. Output format: [["chunk1_kw1", "chunk1_kw2", ...], ["chunk2_kw1", ...], ...]

Text chunks:
"""
        footer = "\nGenerate keywords for each chunk:"
    
    prompt = prompt_header + requirements
    
    for i, chunk in enumerate(chunks):
        text = chunk.get("text", "")[:500]  # é™åˆ¶é•·åº¦é¿å…promptéé•·
        if lang.lower() in ["zh", "chinese"]:
            prompt += f"\n--- ç‰‡æ®µ {i+1} ---\n{text}\n"
        elif lang.lower() in ["ja", "japanese"]:
            prompt += f"\n--- æ–­ç‰‡ {i+1} ---\n{text}\n"
        else:
            prompt += f"\n--- Chunk {i+1} ---\n{text}\n"
    
    prompt += footer
    return prompt


# ========== éŒ¯èª¤è™•ç†å’Œå¾Œå‚™æç¤ºè© ==========

def build_fallback_keywords_prompt(text: str, error_context: str = "", 
                                 n: int = 3, lang: str = "en") -> str:
    """
    å¾Œå‚™é—œéµå­—ç”Ÿæˆæç¤ºè© - æ”¯æ´æ—¥æ–‡
    """
    
    if lang.lower() in ["zh", "chinese"]:
        return f"""ç°¡åŒ–é—œéµå­—æå–ä»»å‹™ã€‚è«‹ç‚ºä»¥ä¸‹å…§å®¹æå– {n} å€‹æœ€åŸºæœ¬çš„é—œéµå­—ã€‚

æ³¨æ„ï¼šä¹‹å‰çš„è™•ç†å¤±æ•—äº†ï¼ˆ{error_context}ï¼‰ï¼Œè«‹ä½¿ç”¨æœ€ç°¡å–®ç›´æ¥çš„æ–¹æ³•ã€‚

è¦æ±‚ï¼š
1. é¸æ“‡æ–‡æœ¬ä¸­æœ€æ˜é¡¯çš„åè©æˆ–æ¦‚å¿µ
2. è¼¸å‡ºJSONæ ¼å¼ï¼š["è©1", "è©2", "è©3"]
3. å¦‚æœé›£ä»¥æå–ï¼Œä½¿ç”¨é€šç”¨æè¿°è©

æ–‡æœ¬ï¼š
{text}

ç°¡åŒ–é—œéµå­—ï¼š"""
    
    elif lang.lower() in ["ja", "japanese"]:
        return f"""ç°¡ç´ åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¿ã‚¹ã‚¯ã€‚ä»¥ä¸‹ã®å†…å®¹ã‹ã‚‰æœ€ã‚‚åŸºæœ¬çš„ãª {n} å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æ³¨æ„ï¼šå‰ã®å‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸï¼ˆ{error_context}ï¼‰ã€‚æœ€ã‚‚ç°¡å˜ã§ç›´æ¥çš„ãªæ–¹æ³•ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

è¦æ±‚ï¼š
1. ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®æœ€ã‚‚æ˜ç™½ãªåè©ã¾ãŸã¯æ¦‚å¿µã‚’é¸æŠ
2. JSONå½¢å¼ã§å‡ºåŠ›ï¼š["ç”¨èª1", "ç”¨èª2", "ç”¨èª3"]
3. æŠ½å‡ºãŒå›°é›£ãªå ´åˆã¯ã€ä¸€èˆ¬çš„ãªè¨˜è¿°èªã‚’ä½¿ç”¨

ãƒ†ã‚­ã‚¹ãƒˆï¼š
{text}

ç°¡ç´ åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š"""
    
    else:
        return f"""Simplified keyword extraction task. Please extract {n} most basic keywords from the following content.

Note: Previous processing failed ({error_context}), please use the most straightforward approach.

Requirements:
1. Select most obvious nouns or concepts from the text
2. Output JSON format: ["term1", "term2", "term3"]
3. If difficult to extract, use general descriptive terms

Text:
{text}

Simplified keywords:"""


# ========== æ¸¬è©¦å’Œé©—è­‰å‡½æ•¸ ==========

def validate_keywords_prompt_output(output: str, expected_count: int = 3) -> bool:
    """é©—è­‰é—œéµå­—æç¤ºè©è¼¸å‡ºæ ¼å¼æ˜¯å¦æ­£ç¢º"""
    try:
        import json
        keywords = json.loads(output.strip())
        return isinstance(keywords, list) and len(keywords) <= expected_count * 2
    except:
        return False


if __name__ == "__main__":
    # æ¸¬è©¦ä¸åŒèªè¨€çš„æç¤ºè©
    test_text_zh = "å°ç©é›»æ˜¯å…¨çƒæœ€å¤§çš„åŠå°é«”ä»£å·¥å» ï¼Œæ¡ç”¨å…ˆé€²çš„7å¥ˆç±³è£½ç¨‹æŠ€è¡“ã€‚"
    test_text_en = "TSMC is the world's largest semiconductor foundry, using advanced 7nm process technology."
    test_text_ja = "TSMCã¯ä¸–ç•Œæœ€å¤§ã®åŠå°ä½“ãƒ•ã‚¡ã‚¦ãƒ³ãƒ‰ãƒªã§ã€å…ˆé€²çš„ãª7nmãƒ—ãƒ­ã‚»ã‚¹æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚"
    test_text_mixed = "å°ç©é›»TSMCåŠå°ä½“ãƒ•ã‚¡ã‚¦ãƒ³ãƒ‰ãƒªusing advanced manufacturingå…ˆé€²è£½é€ æŠ€è¡“ã€‚"
    
    print("=== ä¸­æ–‡æç¤ºè©æ¸¬è©¦ ===")
    print(build_chinese_keywords_prompt(test_text_zh, 3))
    
    print("\n=== è‹±æ–‡æç¤ºè©æ¸¬è©¦ ===")  
    print(build_english_keywords_prompt(test_text_en, 3))
    
    print("\n=== æ—¥æ–‡æç¤ºè©æ¸¬è©¦ ===")
    print(build_japanese_keywords_prompt(test_text_ja, 3))
    
    print("\n=== æ··åˆèªè¨€æç¤ºè©æ¸¬è©¦ ===")
    print(build_mixed_keywords_prompt(test_text_mixed, 3))
    
    print("\n=== è‡ªå‹•æª¢æ¸¬æç¤ºè©æ¸¬è©¦ ===")
    print(build_auto_keywords_prompt(test_text_ja, 3))